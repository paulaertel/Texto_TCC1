\documentclass[12pt,a4paper]{scrartcl}
    % \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)

\usepackage{iftex}
\ifPDFTeX
	\usepackage[brazil]{babel}
	\usepackage[T1]{fontenc}
	\usepackage[utf8]{inputenc}
\else
	\usepackage{fontspec}
	\usepackage{polyglossia}
	\setdefaultlanguage{brazil}
\fi
\usepackage[onehalfspacing]{setspace}
\usepackage{graphicx}
\usepackage{hyperref}

\usepackage{amssymb,amsfonts,amsmath,amsthm}
\usepackage{mathtools}
\usepackage{latexsym}
\usepackage[brazilian]{cleveref}

\usepackage{float}
\usepackage[dvipsnames,usenames]{xcolor}
\usepackage[breakable]{tcolorbox}
\usepackage{xargs}

\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\newcommandx{\unsure}[2][1=]{\todo[linecolor=red,backgroundcolor=red!25,bordercolor=red,#1]{#2}}
\newcommandx{\change}[2][1=]{\todo[linecolor=blue,backgroundcolor=blue!25,bordercolor=blue,#1]{#2}}
\newcommandx{\info}[2][1=]{\todo[linecolor=OliveGreen,backgroundcolor=OliveGreen!25,bordercolor=OliveGreen,#1]{#2}}
\newcommandx{\improvement}[2][1=]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,#1]{#2}}
\newcommandx{\thiswillnotshow}[2][1=]{\todo[disable,#1]{#2}}
\usepackage{indentfirst}
\usepackage{dsfont}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pdflscape}

\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator*{\argmax}{arg\, max}
\DeclareMathOperator*{\argmin}{arg\, min}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\ones}{ones}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\posto}{posto}
\DeclareMathOperator{\sgn}{sgn}

\def\Xset{\mathcal{X}}
\def\Yset{\mathcal{Y}}
\def\Hset{\mathcal{H}}
\def\RR{\mathds{R}}
\def\xbar{\bar{x}}
\def\wbar{\bar{w}}
\def\bbar{\bar{b}}


\newtheorem{teorema}{Teorema}%ambientes em itálico
\newtheorem{prop}{Proposição}
\newtheorem{teo}{Teorema}
\newtheorem{lema}{Lema}


\theoremstyle{definition}%ambientes normais
\newtheorem{exem}{Exemplo}
\newtheorem{defi}{Definição}
\newtheorem{obs}{Observação}
\newtheorem{fato}{Fato}	

\crefname{exem}{exemplo}{exemplos}
\Crefname{exem}{Exemplo}{Exemplos}
\crefname{teo}{teorema}{teoremas}
\Crefname{teo}{Teorema}{Teoremas}
\crefname{lema}{lema}{lemas}
\Crefname{lema}{Lema}{Lemas}
\crefname{defi}{definição}{definições}
\Crefname{defi}{Definição}{Definições}
\crefname{obs}{observação}{observações}
\Crefname{obs}{Observação}{Observações}
\Crefname{fato}{fato}{fatos}
\Crefname{fato}{Fato}{Fatos}

\usepackage{csquotes}
\usepackage[
backend=biber,
style=numeric-comp, noerroretextools=true, sorting=nty,
]{biblatex}
\addbibresource{referencias.bib}

 \let\etoolboxforlistloop\forlistloop % save the good meaning of \forlistloop
 \usepackage{autonum}
 \let\forlistloop\etoolboxforlistloop % restore the good meaning of \forlistloop


%\usepackage[left,mathlines]{lineno}
%\usepackage{etoolbox} %% <- for \pretocmd, \apptocmd and \patchcmd

%% Patch 'normal' math environment: (currently unused, but good to have)
% \newcommand*\linenomathpatch[1]{%
%   \expandafter\pretocmd\csname #1\endcsname {\linenomath}{}{}%
%   \expandafter\pretocmd\csname #1*\endcsname{\linenomath}{}{}%
%   \expandafter\apptocmd\csname end#1\endcsname {\endlinenomath}{}{}%
%   \expandafter\apptocmd\csname end#1*\endcsname{\endlinenomath}{}{}%
% }
%% Patch AMS math environment:
%5\newcommand*\linenomathpatchAMS[1]{%
%  \expandafter\pretocmd\csname %#1\endcsname {\linenomathAMS}{}{}%
%  \expandafter\pretocmd\csname %#1*\endcsname{\linenomathAMS}{}{}%
%  \expandafter\apptocmd\csname %end#1\endcsname {\endlinenomath}{}{}%
 % \expandafter\apptocmd\csname %end#1*\endcsname{\endlinenomath}{}{}%
%}

%% Definition of \linenomathAMS depends on whether the mathlines option is provided
%\expandafter\ifx\linenomath\linenomathWithnumbers
%  \let\linenomathAMS\linenomathWithnumbers
  %% The following line gets rid of an extra line numbers at the bottom:
%  \patchcmd\linenomathAMS{\advance\postdisplaypenalty\linenopenalty}{}{}{}
%\else
%  \let\linenomathAMS\linenomathNonumbers
%\fi

% \linenomathpatch{equation} %% <- unnecessary, equation is already patched
%\linenomathpatchAMS{gather}
%\linenomathpatchAMS{multline}
%\linenomathpatchAMS{align}
%\linenomathpatchAMS{alignat}
%\linenomathpatchAMS{flalign}

%\linenumbers
  \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    % \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}

    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
     \AtBeginEnvironment{tcolorbox}{\footnotesize}

\begin{document}

\title{Otimização e Análise Teórica das Máquinas de Vetores Suporte Aplicadas à Classificação de Dados\\ {\small Relatório final de Iniciação Científica}} 
\author{ \normalfont Paula Cristina Rohr Ertel\thanks{Acadêmica do curso de Licenciatura em Matemática/UFSC-Blumenau. Bolsista PIBIC/CNPq.} \\ \small Orientador: Luiz Rafael dos Santos \\ \small Universidade Federal de Santa Catarina - Campus Blumenau}
%\date{\small 18 de Novembro de 2019}
\subtitle{Relatório final de Iniciação Científica}
\maketitle

\begin{abstract}

\begin{center}
\textbf{Resumo}
\end{center}

Este relatório de pesquisa resume as atividades e resultados obtidos pelo bolsista durante sua iniciação científica entre Agosto de 2020 e Julho de 2021.
Em problemas que exigem a análise de uma grande quantidade de dados para classificá-los, um processo manual torna-se inviável, motivando o desenvolvimento de técnicas computacionais capazes de reconhecer padrões para desempenhar tal tarefa. Assim, o objetivo central deste projeto foi desenvolver um estudo teórico, do ponto de vista da Otimização, de uma técnica de aprendizagem de máquina supervisionada aplicada à classificação binária de dados: as Máquinas de Vetores Suporte (SVM). Para tanto, tendo em vista que a formulação matemática da técnica SVM consiste num problema de programação quadrática convexa com restrições lineares, abordamos aspectos da teoria de otimização, com e sem restrições, assim como apresentamos definições e resultados de otimização convexa, que fornecem propriedades importantes relacionadas aos problemas de otimização, como a garantia de existência de soluções. Desenvolvemos com detalhes a modelagem matemática da técnica SVM com margem rígida, demonstrando que o problema de otimização decorrente admite solução e ela é única, e sua generalização para um dos casos em que os dados não são linearmente separáveis, denominada SVM com margem flexível. Por fim, utilizando a linguagem de programação Julia, realizamos uma implementação computacional da técnica SVM para classificar o conjunto de dados Flor Íris em relação às suas espécies e, posteriormente, para classificar um conjunto de dados sobre células de câncer de mama em tumor maligno ou benigno. Através desses experimentos numéricos foi possível analisar a eficiência da técnica SVM. Em particular, no caso em que aplicamos SVM com margem flexível, tal eficiência está relacionada com a escolha de um parâmetro de penalização adequado.

\textbf{Palavras-chave:} Aprendizagem de Máquina, Classificação, Máquinas de Vetores Suporte, Otimização com restrições, Otimização convexa.
\end{abstract}

\section{Introdução}

A análise inteligente de dados tem se tornado cada vez mais importante para auxiliar na tomada de decisões em diversos campos da ciência. A Aprendizagem de Máquina (AM, do inglês \textit{Machine Learning}) é um campo da inteligência computacional que estuda o uso de técnicas computacionais para automaticamente detectar padrões em dados e utilizá-los para fazer predições e tomar decisões. Ela possui contribuições em diversos campos da ciência e também está presente em diferentes situações do cotidiano como na recomendação de filmes e séries em serviços de Streaming, nos mecanismos de pesquisa do Google e na detecção de spam em e-mails, por exemplo. 

A Aprendizagem de Máquina também está diretamente relacionada à Otimização, que contribui para a formulação matemática e para a implementação computacional de muitos de seus algoritmos. As técnicas de Aprendizagem de Máquina podem ser divididas em dois tipos principais, a aprendizagem supervisionada, em que através de um conjunto de dados, cujas saídas são previamente conhecidas, o algoritmo detecta padrões e produz um modelo capaz de deduzir as saídas corretas para novos dados, e a aprendizagem não-supervisionada, empregada em problemas que não possuem dados previamente rotulados. Nosso objetivo neste trabalho é realizar um estudo teórico, do ponto de vista da Otimização, de uma técnica de aprendizagem supervisionada: as Máquinas de Vetores Suporte (SVM, do inglês \textit{Support Vector Machine}).

A técnica SVM é fundamentada na Teoria de Aprendizagem Estatística e foi desenvolvida por Vladimir Vapnik, Bernhard Boser, Isabelle Guyon e Corrina Cortes \cite{Vladimir1992,Vladimir1995}. Conforme destacado por \textcite{Evelin2017}, as SVM são amplamente empregadas em problemas de regressão e de classificação, possuem um embasamento teórico bem consolidado e apresentam uma boa capacidade de generalização. São indicadas nos casos em que ocorrem dados de dimensões elevadas e com altos níveis de ruídos. 

Neste trabalho será abordada a modelagem matemática da técnica SVM aplicada ao problema de classificação, que resulta no seguinte problema de programação quadrática convexa e com restrições lineares
\[ \label{problema_geral_SVM_introducao}
\begin{aligned}
\min_{w,b} & \quad f(w) \\
\text{s.a.} &  \quad g(w,b) \leq 0, \end{aligned}
\]
em que as funções $f:\RR^{n+1} \rightarrow \RR$ e $g:\RR^{n+1} \rightarrow \RR^{m}$ são continuamente diferenciáveis. A resolução do problema \eqref{problema_geral_SVM_introducao} fornece uma solução $(w,b)$, a partir da qual obtém-se um classificador linear. Geometricamente, dado um conjunto de dados, a técnica SVM realiza a separação deste conjunto em diferentes classes através de um hiperplano definido pela equação $w^{T}x + b = 0$ \cite{Evelin2017}, que é construído a partir da solução do problema \eqref{problema_geral_SVM_introducao}.

Dependendo do conjunto de dados e da complexidade do problema, a técnica SVM apresenta três diferentes formulações: SVM com margem rígida (\Cref{fig:hiperplano_margemrigida}), SVM com margem flexível (\Cref{fig:hiperplano_margemflexivel}) e SVM não linear (\Cref{fig:SVM_naolinear}). 

\begin{figure}[!ht] 
\centering
\begin{subfigure}[h]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{hiperplano_SVM_linear}
\caption{Margem Rígida. \label{fig:hiperplano_margemrigida}}
\end{subfigure}
\begin{subfigure}[!ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hiperplano_SVM_flexivel}
	\caption{Margem Flexível. \label{fig:hiperplano_margemflexivel}}
\end{subfigure}
\begin{subfigure}[!ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hiperplano_SVM_nao_linear}
	\caption{Não Linear. \label{fig:SVM_naolinear}}
\end{subfigure}
\caption{Conjunto de dados para classificação. \label{fig:distribuicao_dados_e_classificacao}}
\end{figure}

O problema de classificação representado na \Cref{fig:hiperplano_margemrigida} é um exemplo que pode ser modelado pelo problema \eqref{problema_geral_SVM_introducao}. Neste caso os dados são linearmente separáveis, sendo possível encontrar um hiperplano que os classifique corretamente. As \Cref{fig:hiperplano_margemflexivel,fig:SVM_naolinear}, no entanto, apresentam problemas cujos dados não são linearmente separáveis. Na \Cref{fig:hiperplano_margemflexivel} temos um exemplo de SVM de margem flexível, em que promovendo um relaxamento das restrições através de variáveis de folga $\xi_{i}$ associadas a cada atributo $x^{i}$ ainda é possível obter um hiperplano que classifique os dados. Já no caso da \Cref{fig:SVM_naolinear} é preciso aplicar a SVM não linear, na qual o hiperplano ótimo é obtido através de um mapeamento dos dados para um espaço de dimensão elevada \cite{Evelin2017}. Neste trabalho abordaremos somente os casos de SVM com margem rígida e margem flexível, pois para a modelagem da técnica SVM não linear são necessários alguns conceitos matemáticos mais complexos que não serão abordados neste trabalho e portanto, ela se constitui numa proposta de estudo a ser desenvolvida em projetos futuros.

Como o problema de classificação \eqref{problema_geral_SVM_introducao} que desejamos formular trata-se de um problema de programação quadrática convexa com restrições lineares, a \Cref{chap:conceitos_de_otimizacao} apresenta os principais conceitos e resultados de otimização irrestrita e com restrições lineares de igualdade e desigualdade. As principais referências consultadas para elaboração deste capítulo foram \textcite{Ana1994,Ademir2013,Izmailov2014ac,luenberger2008linear}.

Na \Cref{chap:otimizacao_convexa} são discutidos os conceitos de conjunto convexo e função convexa com o intuito de analisar a convexidade dos problemas de SVM. Para construção deste capítulo as principais bibliografias utilizadas foram \textcite{Ademir2013,Izmailov2014ac,luenberger2008linear,bertsekas2016nonlinear}.

A modelagem matemática do problema de classificação através da técnica SVM, tanto para o caso de margem rígida quanto de margem flexível, é realizada na \Cref{chap:maquinas_vetores_suporte}. Para tanto, foram utilizadas como principais referências \textcite{Evelin2017,Faisal2019}.

Por fim, a técnica SVM possui inúmeras aplicações como, por exemplo, no reconhecimento facial, leitura de placas automotivas e detecção de spam. Assim, na \Cref{chap:experimentos_numericos} realizamos experimentos numéricos com a técnica SVM para a classificação de dados. Num primeiro momento aplicamos a técnica SVM para classificar o conjunto de dados Iris, e posteriormente, para a classificação de amostras de células de câncer de mama em tumor maligno ou benigno. 






%-------------------------------------------------------------------------------------------------------------------------

\newpage
\section{Conceitos de Otimização} \label{chap:conceitos_de_otimizacao}

Neste capítulo pretendemos discutir alguns conceitos e resultados para o problema 
\[ \label{problema_geral_otimizacao}
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad x \in \Omega ,
\end{aligned}
\]
em que $f: \RR^{n} \rightarrow \RR$ é chamada \emph{função objetivo}, $\Omega \subset \RR^{n}$ é chamado \emph{conjunto factível} do problema \eqref{problema_geral_otimizacao} e os pontos de $\Omega$ são chamados de \emph{pontos factíveis}. 

Como o problema \eqref{problema_geral_SVM_introducao} que surge da formulação matemática da técnica SVM para classificação trata-se de um problema de otimização que possui restrições lineares apenas, toda a teoria de condições de otimalidade somente será estudada para problemas desse tipo: com restrições lineares. Para o desenvolvimento da teoria de otimização irrestrita, as principais referências utilizadas foram \textcite{Izmailov2014ac,Ademir2013}. Já os conceitos e resultados da teoria de otimização com restrições foram desenvolvidos, principalmente, com base em \textcite{Ana1994,luenberger2008linear}.  

Inicialmente, vamos caracterizar os pontos que são solução do problema \eqref{problema_geral_otimizacao}.

\begin{defi} \label{defi:minimizador_local_global}
Dizemos que um ponto $x^{*} \in \Omega$ é
\begin{enumerate}
	\item[(a)] \emph{minimizador local} de $f$ em $\Omega$ se, e somente se, existe $\varepsilon >0$ tal que $f(x^{*}) \leq f(x)$ para todo $x \in B(x^{*}, \varepsilon) \cap \Omega$.

	\item[(b)] \emph{minimizador global} de $f$ em $\Omega$ se, e somente se, $f(x^{*}) \leq f(x)$ para todo $x \in \Omega$
\end{enumerate} 
\end{defi}

Quando as desigualdades na \Cref{defi:minimizador_local_global} forem estritas para $x \neq x^{*}$, diremos que $x^{*}$ é minimizador estrito. Essas definições são representadas na \Cref{fig:grafico_minimo_local_global}.

\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.86\textwidth]{grafico_minimo_local_global}
	\caption{ Mínimo local e mínimo global em uma dimensão. \label{fig:grafico_minimo_local_global}}
\end{figure}

Pela \Cref{defi:minimizador_local_global}, todo minimizador global é também minimizador local, porém a recíproca não é verdadeira. É interessante salientar que em muitas circunstâncias iremos nos contentar com um ponto de mínimo local pois, de modo geral, condições globais e soluções globais só podem ser encontradas se o problema possuir certas propriedades que garantem, essencialmente, que qualquer mínimo local é global. Uma destas propriedades é a convexidade, a qual abordaremos mais adiante (veja \Cref{chap:otimizacao_convexa}).

\begin{obs}
Todo problema de maximização
\[
\begin{aligned}
\max_{x} & \quad f(x) \\
\text{s.a} & \quad x \in \Omega \end{aligned}
 \]
pode ser transformado em um problema de minimização equivalente
\[
\begin{aligned}
\min_{x} & \quad -f(x) \\
\text{s.a} & \quad x \in \Omega .\end{aligned}
\]
Em particular, as soluções locais e globais de ambos os problemas são as mesmas, com sinais opostos para os valores ótimos.
\end{obs} %acrescentar imagem izmailoz&solodov

%Se a função objetivo $f$ for linear, \eqref{problema_geral_otimizacao} é chamado de problema de \emph{programação linear}, e se $f$ for uma função quadrática, \eqref{problema_geral_otimizacao} é chamado de problema de \emph{programação quadrática}. Ademais, quando $\Omega$ é um conjunto convexo e $f$ é uma função convexa, dizemos que \eqref{problema_geral_otimizacao} é um problema de \emph{programação convexa}. 


Ademais, quando o conjunto factível $\Omega = \RR^{n}$ dizemos que o problema \eqref{problema_geral_otimizacao} é irrestrito. No caso em que $\Omega$ é definido por um sistema de igualdades ou desigualdades como
\[
\Omega = \{ x\in \RR^{n} \mid h(x)=0, \ g(x) \leq 0 \},
\]
falamos em otimização com restrições.

Frequentemente, a formulação de problemas mais complexos envolve restrições à função objetivo. No entanto, veremos mais adiante que muitos desses problemas podem ser convertidos em problemas irrestritos, utilizando as restrições para estabelecer relações entre as variáveis. Em vista disso, abordaremos primeiramente a teoria de otimização para o caso irrestrito para posteriormente obter as condições de otimalidade para problemas com restrições de igualdade e desigualdade, haja vista que o problema de classificação, o qual estamos interessados em resolver, possui tal formato.

\begin{defi} \label{defi:valor_otimo}
Dizemos que $\bar{v} \in \RR \cup \{ -\infty \}$ definido por
\[
\bar{v} = \inf_{x \in \Omega} f(x)
\]
é o \emph{valor ótimo} do problema \eqref{problema_geral_otimizacao}.
\end{defi}

No estudo do problema de otimização irrestrita uma das principais questões que surge diz respeito a existência da solução. Observe que se na \Cref{defi:valor_otimo} temos $\bar{v} = - \infty$, o problema \eqref{problema_geral_otimizacao} não admite solução global, pois neste caso $f$ é ilimitada inferiormente no conjunto factível. Outro caso em que também não existe minimizador global ocorre quando $\bar{v}$ não é atingido em nenhum ponto factível. Vejamos um exemplo.

\begin{exem}
Seja $f: \RR \rightarrow \RR$, definida por $f(x) = e^{x}$, $\Omega = \RR$. Note que $\bar{v} = \inf_{x \in \RR} e^{x} = 0$, contudo, não existe $x \in \RR$ tal que $e^{x} = 0$. 
De modo análogo, considere $f$ definida como anteriormente e $\Omega = (0,1]$. Temos que $\bar{v} = \inf_{x \in (0,1]} e^{x} = 1$ e novamente não existe $x \in (0,1]$ tal que $e^{x}=1$. Observe que a função $f$ é contínua em $\Omega$, porém no primeiro caso $\Omega$ não é limitado e no segundo, $\Omega$ não é fechado. 
Considere agora $\Omega = [0,1]$, $f(x)=e^{x}$ para $x \in (0,1]$ e $f(0)=0$. Novamente, $\bar{v} = \inf_{x \in (0,1]} e^{x} = 1$, porém não existe $x\in \Omega$ tal que $f(x)=1$. Neste exemplo, $\Omega$ é compacto mas $f$ não é contínua.
\end{exem}

Assim, a partir desses exemplos é possível perceber que a existência de soluções está relacionada à continuidade da função objetivo e à compacidade do conjunto factível. O principal resultado que garante a existência de soluções globais é o Teorema de Weierstrass.

\begin{teo}(\textbf{Weierstrass}) \label{teo:weierstrass}
Sejam $f: \RR^{n} \rightarrow \RR$ uma função contínua e $\Omega \in \RR^{n}$ um conjunto compacto não vazio. Então existe minimizador global de $f$ em $\Omega$.
\end{teo}
\begin{proof}
A imagem de um conjunto compacto por uma função contínua é compacta. Assim, $f(\Omega)$ é compacto, em particular, como $f(\Omega) \in \RR$, este conjunto é limitado inferiormente, ou seja, existe $\bar{v} \in \RR$ tal que
\[
\bar{v} = \inf_{x\in \Omega} f(x).
\]
Pela definição de ínfimo, temos que para todo $k\in \mathds{N}$ existe $x^{k} \in \Omega$ tal que 
\[
\bar{v} \leq f(x^{k}) \leq \bar{v} + \dfrac{1}{k} .
\]
Passando ao limite quando $k \rightarrow \infty $, e usando o Teorema do Sanduíche concluímos que
\[\label{eq:teo_weierstrass}
\displaystyle\lim_{k \rightarrow \infty} f(x^{k}) = \bar{v}.
\]
Como $\{ x^{k} \} \in \Omega$ e $\Omega$ é compacto, temos que $\{ x^{k} \}$ é uma sequência limitada. Logo, ela possui uma subsequência convergente em $\Omega$, isto é, existe uma subsequência $\{ x^{k_{j}} \}$ que converge para um ponto $\bar{x} \in \Omega$,
\[
\displaystyle\lim_{j \rightarrow \infty} x^{k_{j}} = \bar{x} \in \Omega .
\]
Como $f$ é contínua, temos que
\[
\displaystyle\lim_{j \rightarrow \infty} f(x^{k_{j}}) = f(\bar{x}).
\]
Usando \eqref{eq:teo_weierstrass}, temos que $f(\bar{x}) = \bar{v}$ e portanto, $f$ assume valor mínimo no ponto $\bar{x} \in \Omega$. Em outras palavras, $\bar{x}$ é um minimizador global de $f$ em $\Omega$.
\end{proof}


\subsection{Condições de Otimalidade para Problemas sem Restrições} \label{subsection:irrestrito}

Considere o problema \eqref{problema_geral_otimizacao} irrestrito , isto é, com $\Omega = \RR^{n}$,
\[ \label{problema_otimizacao_irrestrita}
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad x \in \RR^{n} ,
\end{aligned}
\]
em que $f:\RR^{n} \rightarrow \RR$. Nesta seção serão determinadas as condições que um ponto $x^{*} \in \RR^{n}$ deve satisfazer quando é minimizador local do problema \eqref{problema_otimizacao_irrestrita}. Condições desse tipo são chamadas de \emph{condições necessárias de otimalidade}. Determinaremos também as condições que garantem que um ponto dado é minimizador local do problema, que são denominadas \emph{condições suficientes de otimalidade}. 

Para os resultados que vem a seguir vamos utilizar os \Cref{fato:Taylor_primeira_ordem,fato:Taylor_segunda_ordem}, que fornecem a Fórmula de Taylor de primeira e segunda ordem. As demonstrações destes fatos podem ser encontradas em \textcite[p.194]{Elon2019}.


\begin{fato}(\textbf{Taylor de Primeira Ordem}) \label{fato:Taylor_primeira_ordem}
Seja $f:\RR^{n} \rightarrow \RR$ uma função diferenciável e $\xbar \in \RR^{n}$. Então podemos escrever
\[
f(x) = f(\xbar) + \nabla f(\xbar)^{T}(x-\xbar) + o(x),
\]
com $\displaystyle\lim_{x \rightarrow \xbar}\dfrac{o(x)}{\Vert x-\xbar \Vert} = 0$.
\end{fato}

\begin{fato}(\textbf{Taylor de Segunda Ordem}) \label{fato:Taylor_segunda_ordem}
Seja $f:\RR^{n} \rightarrow \RR$ uma função duas vezes diferenciável e $\xbar \in \RR^{n}$. Então
\[
f(x) = f(\xbar) + \nabla f(\xbar)^{T}(x-\xbar) + \dfrac{1}{2}(x-\xbar)^{T}\nabla^{2} f(\xbar)(x-\xbar) + o(x),
\] 
com $\displaystyle\lim_{x \rightarrow \xbar}\dfrac{o(x)}{\Vert x - \xbar \Vert^{2}} = 0$. 
\end{fato}

Agora, nos teoremas que seguem, mostraremos as condições de otimalidade de primeira e de segunda ordem para o problema irrestrito.

\begin{teo}(\textbf{Condição necessária de primeira ordem}) \label{teo:condicao_necessaria_1_ordem}
Seja $f:\RR^{n} \rightarrow \RR$ diferenciável no ponto $x^{*} \in \RR^{n}$. Se $x^{*}$ é um minimizador local de $f$, então
\[ \label{eq:condicao_necessaria_1_ordem}
\nabla f(x^{*}) = 0.
\]
\end{teo}
\begin{proof}
Considere $d \in \RR^{n}$ arbitrário, porém fixo. Pela definição de minimizador local, existe $\varepsilon > 0$ tal que 
\[ \label{eq:1_condicao_1_ordem_irrestrito}
f(x^{*}) \leq f(x^{*}+td),
\]
para todo $t\in (0,\varepsilon ]$. Pela diferenciabilidade de $f$ em $x^{*}$, aplicando o \Cref{fato:Taylor_primeira_ordem}, podemos escrever 
\[
f(x^{*}+td) = f(x^{*}) + t\nabla f(x^{*})^{T}d + o(t),
\] 
com $\displaystyle\lim_{t\rightarrow 0} \dfrac{o(t)}{t} =0$. Utilizando \eqref{eq:1_condicao_1_ordem_irrestrito}, temos
\[
0 \leq t\nabla f(x^{*})^{T}d + o(t),
\]
e dividindo por $t>0$, 
\[
0 \leq \nabla f(x^{*})^{T}d + \dfrac{o(t)}{t} .
\]
Passando o limite quando $t\rightarrow 0$, obtemos
\[
0 \leq \nabla f(x^{*})^{T}d .
\]
Supondo que $\nabla f(x^{*})$ fosse não-nulo, como $d \in \RR^{n}$ é arbitrário, poderíamos escolher $d=-\nabla f(x^{*})$, resultando em
\[
0 \leq \nabla f(x^{*})^{T}d = - \Vert \nabla f(x^{*}) \Vert^{2} ,
\]
o que é uma contradição. Portanto, $\nabla f(x^{*}) =0$.
\end{proof}


\begin{defi}\label{defi:ponto_critico}
Um ponto $x^{*} \in \RR^{n}$ que cumpre a condição \eqref{eq:condicao_necessaria_1_ordem} é dito \emph{ponto crítico} ou \emph{estacionário} da função $f$. 
\end{defi}


%retirado do texto:a matriz Hessiana de $f$ no ponto $x^{*}$ é semidefinida positiva, isto é,
\begin{teo}(\textbf{Condição necessária de segunda ordem}) \label{teo:condicao_necessaria_2_ordem}
Seja $f:\RR^{n} \rightarrow \RR$ duas vezes diferenciável no ponto $x^{*} \in \RR^{n}$. Se $x^{*}$ é um minimizador local de $f$, então 
\[
d^{T} \nabla^{2} f(x^{*})d \geq 0,
\]
para todo $d \in \RR^{n}$.
\end{teo}
\begin{proof}
Seja novamente $d\in \RR^{n}$ arbitrário, porém fixo. Se $x^{*}$ é minimizador local de $f$, então para todo $t>0$ suficientemente pequeno, temos que
\[\label{eq:1_condicao_2_ordem_irrestrito}
0 \leq f(x^{*}+td) - f(x^{*}).
\]
Aplicando o \Cref{fato:Taylor_segunda_ordem},
\[
f(x^{*}+td) = f(x^{*}) + t\nabla f(x^{*})^{T}d + \dfrac{t^{2}}{2} d^{T}\nabla^{2} f(x^{*})d + o(t^{2}),
\]
com $\displaystyle\lim_{t\rightarrow 0} \dfrac{o(t^{2})}{t^{2}} =0$, e  \eqref{eq:1_condicao_2_ordem_irrestrito} implica em
\[ \label{eq:2_condicao_2_ordem_irrestrito}
0 \leq \dfrac{t^{2}}{2} d^{T}\nabla^{2} f(x^{*})d + o(t^{2}),
\]
 pois como $x^{*}$ é minimizador local, o \Cref{teo:condicao_necessaria_1_ordem} garante que $\nabla f(x^{*}) =0$. Dividindo ambos os lados da desigualdade \eqref{eq:2_condicao_2_ordem_irrestrito} por $t^{2}$ e tomando o limite quando $t\rightarrow 0$, obtemos
\[
d^{T} \nabla^{2} f(x^{*})d \geq 0,
\]
para todo $d \in \RR^{n}$.
\end{proof}

\begin{defi} \label{defi:definida_positiva}
Seja $A \in \RR^{n\times n}$ uma matriz simétrica. Dizemos que $A$ é \emph{definida positiva} quando $x^{T}Ax >0$, para todo $x\in \RR^{n}$, $x \neq 0$. Tal propriedade é denotada por $A \succ 0$. Se $x^{T}Ax \geq 0$, para todo $x\in \RR^{n}$, $A$ é dita \emph{semidefinida positiva}, fato este denotado por $A \succcurlyeq 0$.
\end{defi}
\begin{obs}
A definição geral de positividade de uma matriz não exige que ela seja simétrica. Entretanto, neste trabalho sempre que dissermos que a matriz é definida ou semidefinida positiva assumimos implicitamente que a matriz é simétrica.
\end{obs}

Assim, de acordo com a \Cref{defi:definida_positiva}, o \Cref{teo:condicao_necessaria_2_ordem} nos diz que se um ponto $x^{*}$ é minimizador local de $f$, então a matriz Hessiana $\nabla^{2} f(x^{*})$ é semidefinida positiva.

É possível obter, a partir da condição necessária de segunda ordem, a condição suficiente de segunda ordem, isto é, condição que implica que $x^{*}$ é minimizador local estrito, bastando para isso pedir a definição positiva da Hessiana.

\begin{teo}(\textbf{Condição suficiente de segunda ordem}) \label{teo:condicao_suficiente_irrestrito}
Seja $f:\RR^{n} \rightarrow \RR$ duas vezes diferenciável no ponto $x^{*} \in \RR^{n}$. Se $x^{*}$ é um ponto estacionário da função $f$ e $\nabla^{2} f(x^{*})$ é definida positiva, então $x^{*}$ é minimizador local estrito de $f$.
\end{teo}
\begin{proof}
Seja $B \coloneqq \{ h\in \RR^{n} \mid \Vert h \Vert =1 \}$ e consideremos a função $\phi : B \rightarrow \RR$ dada por
\[
\phi (h) = h^{T} \nabla^{2} f(x^{*})h.
\]
A função $\phi$ é contínua e $B$ é um conjunto compacto, portanto, pelo \Cref{teo:weierstrass}, $\phi$ atinge um valor máximo e um valor mínimo em $B$. Considere $\phi_{\min} \coloneqq \phi(h^{*})$ o valor mínimo de $\phi$ em $B$. Como $\nabla^{2} f(x^{*}) \succ 0$, então $\phi(h) >0$, para todo $h\in B$ e, em particular,
\[
\phi(h) \geq \phi_{\min} >0.
\]
Agora, consideremos $d\in \RR^{n}$, arbitrário não-nulo. Como $\dfrac{d}{\Vert d \Vert} \in B$, temos que
\[
{\dfrac{d}{\Vert d \Vert}}^{T} \nabla^{2} f(x^{*})\dfrac{d}{\Vert d \Vert} \geq a
\]
implica em
\[ \label{eq:1_condicao_suficiente_irrestrito}
d^{T}\nabla^{2} f(x^{*})d \geq a\vert d\Vert^{2} .
\]
Aplicando o \Cref{fato:Taylor_segunda_ordem} em torno de $x^{*}$, temos
\[\label{eq:2_condicao_suficiente_irrestrito}
f(x^{*}+d) - f(x^{*}) = \nabla f(x^{*})^{T}d + \dfrac{1}{2} d^{T}\nabla^{2} f(x^{*})d + o(\Vert d \Vert^{2}).
\]
Como, por hipótese, $\nabla f(x^{*}) = 0$, \eqref{eq:1_condicao_suficiente_irrestrito} e \eqref{eq:2_condicao_suficiente_irrestrito} implicam que
\[ 
f(x^{*}+d) - f(x^{*}) = \dfrac{1}{2} d^{T}\nabla^{2} f(x^{*})d + o(\Vert d \Vert^{2}) ,
\]
e assim, 
\[ \label{eq:3_condicao_suficiente_irrestrito}
f(x^{*}+d) - f(x^{*}) \geq \dfrac{\phi_{\min}}{2}\Vert d \Vert^{2} + o(\Vert d \Vert^{2}) .
\]
Então, para todo $d$ tal que $\Vert d \Vert$ é suficientemente pequena, o primeiro termo do membro direito da desigualdade \eqref{eq:3_condicao_suficiente_irrestrito} define o sinal deste lado. Além disso,
\[
\dfrac{\phi_{\min}}{2}\Vert d \Vert^{2} >0.
\]
Portanto, para $\Vert d \Vert$ suficientemente pequeno não-nulo, temos que 
\[
f(x^{*} + d) - f(x^{*}) >0,
\]
e assim,
\[
f(x^{*}) < f(x^{*} + d).
\]
Portanto, para todo $x = x^{*} + d$, com $d$ suficientemente pequeno não-nulo, temos que $f(x^{*}) < f(x)$. Logo, $x^{*}$ é um minimizador local estrito de $f$.
\end{proof}

Vejamos um exemplo em que a condição suficiente de segunda ordem nos permite determinar uma solução para um problema de minimização irrestrito.

\begin{exem}
Considere o problema irrestrito
\[\begin{aligned} \label{exem:verificar_condicao_suficiente_irrestrito}
\min_{x} & \quad f(x) = x_{1}^{2} - x_{1} x_{2} + 2x_{2}^{2} - 2x_{1} + \dfrac{2}{3}x_{2} + e^{x_{1}+x_{2}}  \\
\text{s.a} & \quad x\in \RR^{2} .
\end{aligned} \]
\end{exem}
Para um ponto $x^{*}$ ser minimizador local estrito de \eqref{exem:verificar_condicao_suficiente_irrestrito} ele deve satisfazer as condições suficientes de segunda ordem do \Cref{teo:condicao_suficiente_irrestrito}, que são
\[ \label{eq:1_exem_verificar_condicao_suficiente_irrestrito}
\nabla f(x^{*}) = \begin{bmatrix*}
2x_{1}^{*} - x_{2}^{*} - 2 + e^{x_{1}^{*} +x_{2}^{*} } \\ -x_{1}^{*} + 4x_{2}^{*} + \dfrac{2}{3} + e^{x_{1}^{*} +x_{2}^{*} } 
\end{bmatrix*} =
\begin{bmatrix*}
0 \\ 0
\end{bmatrix*} ,
\]
e
\[ \label{eq:2_exem_verificar_condicao_suficiente_irrestrito}
\nabla^{2} f(x^{*}) = \begin{bmatrix*}
2 + e^{x_{1}^{*} +x_{2}^{*} } & -1 + e^{x_{1}^{*} +x_{2}^{*} } \\ -1 + e^{x_{1}^{*} +x_{2}^{*} } & 4 + e^{x_{1}^{*} +x_{2}^{*} }
\end{bmatrix*} \succ 0.
\]
Com efeito, o ponto $\bar{x} = (1/3, -1/3 )$ satisfaz \eqref{eq:1_exem_verificar_condicao_suficiente_irrestrito}, pois
\[
\nabla f(\bar{x}) = \begin{bmatrix*} 0 \\ 0 \end{bmatrix*} .
\]
Além disso, $\nabla^{2} f(\bar{x})$ é definida positiva, pois
\[
\nabla^{2} f(\bar{x}) = \begin{bmatrix*} 3 & 0 \\ 0 & 5 \end{bmatrix*},
\]
e, para todo $x \in \RR^{2}\backslash\{ 0\}$, temos que
\[
\begin{bmatrix*} x_{1} & x_{2} \end{bmatrix*}^{T} \begin{bmatrix*} 3 & 0 \\ 0 & 5 \end{bmatrix*} \begin{bmatrix*} x_{1} \\ x_{2} \end{bmatrix*} = 3x_{1}^{2} + 5x_{2}^{2} >0.
\]
Portanto, $\bar{x} = (1/3, -1/3 )$ é minimizador local estrito do problema \eqref{exem:verificar_condicao_suficiente_irrestrito}.


Tendo em vista que o problema de classificação \eqref{eq:problemageralSVM} que estamos interessados em resolver apresenta restrições e seu conjunto factível é poliedral, abordaremos nas seções seguintes o problema de minimização com restrições nesse contexto. Para tanto, iniciamos analisando o problema de minimização com restrições lineares de igualdade.


%-----------------------------------------------------------------------------------------------------------------------------
\subsection{Minimização com Restrições Lineares de Igualdade} \label{section:restricoes_igualdade}

Nosso objetivo nesta seção será analisar o seguinte problema de minimização com restrições lineares de igualdade
\[ \label{problema_geral_minimizacao_igualdade}
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad Ax=b,
\end{aligned}
\]
em que $A \in \RR^{m\times n}$, $1 \leq m <n$ e $\posto (A)=m$. Assim como no caso irrestrito, estamos interessados em obter as condições que caracterizam as soluções do problema \eqref{problema_geral_minimizacao_igualdade}.

O conjunto
\[
\Omega \coloneqq  \{ x \in \RR^{n} \mid Ax=b \}
\]
é chamado \emph{conjunto de factibilidade} de \eqref{problema_geral_minimizacao_igualdade}. Este conjunto é a variedade afim de soluções do sistema linear
\[ \label{eq:restricao_de_igualdade}
Ax=b.
\]
Note que $\Omega$ é uma reta se $m=n-1$, um plano se $m=n-2$ e uma variedade de dimensão $n-m$ para $m$ genérico. No caso em que $n>2$ e $m=1$ dizemos que $\Omega$ é um hiperplano. 

Para obter as condições de otimalidade para um ponto de mínimo $x^{*}$ do problema \eqref{problema_geral_minimizacao_igualdade}, a ideia central é considerar o movimento para longe do ponto $x^{*}$ em uma determinada direção, desde que possamos continuar, pelo menos inicialmente, no conjunto $\Omega$. Desse modo, dado $x \in \Omega$ dizemos que um vetor $d$ é uma \emph{direção factível} de $x$ se existe $\bar{\alpha} >0$ tal que $x + \alpha d \in \Omega$, para todo $\alpha$ com $0 \leq \alpha \leq \bar{\alpha}$. Em vista disso, o primeiro passo será determinar o conjunto de direções factíveis em $\Omega$. 

Para tanto, associado a $\Omega$ temos o conjunto de soluções do sistema homogêneo $Ax=0$, que é chamado de Núcleo de $A$ e denotado por $\mathcal{N}(A)$. Este conjunto é um subespaço de $\RR^{n}$ de dimensão $n-m$, pois $\posto (A)=m$. É interessante observar que $\mathcal{N}(A)$ é um subespaço paralelo à variedade afim $\Omega$ e passa pela origem. Esta noção é representada na \Cref{fig:nucleoA_paralelo_Omega}.

\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.60\textwidth]{nucleoA_paralelo_Omega}
	\caption{ Núcleo de uma matriz. \\ Fonte: \textcite{Ana1994} \label{fig:nucleoA_paralelo_Omega}}
\end{figure}


Como $\posto (A)=m$, as $m$ linhas de $A$ formam um conjunto de vetores linearmente independentes que geram o subespaço chamado imagem de $A^{T}$ de dimensão $m$, que é denotado por $Im(A^{T})$. Dessa forma, como é possível observar na \Cref{fig:nucleoA_paralelo_Omega}, as linhas de $A$ são ortogonais ao $\mathcal{N}(A)$, ou, em outras palavras, $\mathcal{N}(A)$ é o complemento ortogonal de $Im(A^{T})$. Vamos demonstrar este resultado. 

\begin{prop} \label{prop:complemento_ortogonal_nucleo}
Seja $A \in \RR^{m\times n}$. $\mathcal{N}(A) = Im(A^{T})^{\bot}$.
\end{prop}
\begin{proof}
Um vetor $v \in \mathcal{N}(A)$ se, e somente se, $Av=0$. Mas isso ocorre se, e somente se, $Av$ é ortogonal a todo vetor $u\in \RR^{m}$, isto é, $u^{T}Av=0$, para todo $u \in \RR^{m}$. Assim, $(u^{T}Av)^{T} = 0^{T}$ implica que $v^{T}(A^{T}u) = 0$. Agora, variando $u \in \RR^{m}$, temos que $A^{T}u$ fornece o conjunto $Im(A^{T})$ e desse modo, $v^{T}(A^{T}v) = 0$, para todo $u \in \RR^{m}$, se, e somente se, $v \in Im(A^{T})^{\bot}$. Portanto, $v\in \mathcal{N}(A)$ se, e somente se, $v\in Im(A^{T})^{\bot}$.
\end{proof}

Assim, $\mathcal{N}(A)$ e $Im(A^{T})$ são subespaços vetoriais ortogonais e verificam
\[ \label{eq:nucleo_imagem_subespacos_geram_Rn} \mathcal{N}(A) \cap Im(A^{T}) = \{ 0 \} \quad \text{e} \quad \RR^{n} = \mathcal{N}(A) \oplus Im(A^{T}) .\]
\begin{obs} 
A demonstração de \eqref{eq:nucleo_imagem_subespacos_geram_Rn} pode ser encontrada em \textcite{Elon2014}.
\end{obs}

Em vista disso, o teorema a seguir nos dá uma caracterização para o conjunto das direções factíveis em $\Omega$
\begin{teo} \label{teo:direcao_factivel_igualdade}
Seja $\tilde{x} \in \Omega$. Um vetor $d \in \RR^{n}$ é uma direção factível a partir de $\tilde{x}$ se, e somente se, $d \in \mathcal{N}(A)$.
\end{teo}
\begin{proof}
Seja $d \in \RR^{n}$ uma direção factível em $\tilde{x}$, então $x \coloneqq \tilde{x} + \alpha d \in \Omega$ , isto é, $Ax=b$. Assim, 
\[
A(\tilde{x} + \alpha d) = b,
\] 
e como $A\tilde{x} = b$ e $\alpha >0$, temos que 
\[
Ad = 0,
\] 
e portanto, $d \in \mathcal{N}(A)$.

A recíproca também é verdadeira, pois se $d \in \mathcal{N}(A)$ e $\tilde{x} \in \Omega$, então $x= \tilde{x} + \alpha d \in \Omega$, pois
\[
A(\tilde{x} + \alpha d) = A\tilde{x} + \alpha Ad = A\tilde{x} = b ,
\]
e portanto, qualquer $d \in \mathcal{N}(A)$ é uma direção factível a partir de $\tilde{x}$. 
\end{proof}

Dessa forma, concluímos que $\mathcal{N}(A)$ é o conjunto de direções factíveis em $\Omega$, ou seja, qualquer $d \in \mathcal{N}(A)$ é uma direção no espaço na qual podemos nos deslocar a partir de uma solução factível sem correr o risco de abandonar a região de factibilidade.

A partir disso, é possível construir uma parametrização que caracterize o conjunto factível. Se $\{ z_{1}, z_{2}, \ldots , z_{n-m} \}$ é uma base de $\mathcal{N}(A)$ e $Z$ a matriz de dimensão $n\times (n-m)$ cujas colunas são os vetores $z_{i}$, então para todo $d \in \mathcal{N}(A)$, existe $\boldsymbol{\gamma} \in \RR^{n-m}$ tal que $d=Z\boldsymbol{\gamma}$, ou seja, $d$ é escrito como combinação linear dos vetores da base do núcleo de A. Assim, se $\tilde{x}$ é uma solução de \eqref{eq:restricao_de_igualdade}, podemos caracterizar o conjunto factível da seguinte forma
\[ \label{parametrizacao_de_Omega}
\Omega = \{ x \in \RR^{n} \mid x=\tilde{x} + Z\boldsymbol{\gamma} , \ \boldsymbol{\gamma} \in \RR^{n-m} \ \text{e} \ A\tilde{x} =b \} .
\]

\subsubsection{Condições Necessárias de Primeira Ordem} \label{subsection:condicoes_1ordem_igualdade}

Como vimos anteriormente, de modo geral, se um ponto é solução de um problema de otimização então deve satisfazer determinadas propriedades, que são chamadas de condições de otimalidade. Nesta seção abordaremos a condição necessária de primeira ordem para o problema de minimização com restrições de igualdade. Para obter esta condição utilizaremos a parametrização do conjunto factível proposta em \eqref{parametrizacao_de_Omega}, transferindo as restrições de \eqref{problema_geral_minimizacao_igualdade} para sua função objetivo. Com isso obtemos um novo problema de minimização irrestrita, para o qual as condições necessárias de primeira e segunda ordem já são conhecidas.

Assim sendo, a caracterização de $\Omega$ dada em \eqref{parametrizacao_de_Omega} permite definir a seguinte função $\varphi : \RR^{n-m} \rightarrow \RR$ dada por
\[ \label{funcao_phi_restricoes_igualdade}
\varphi (\boldsymbol{\gamma}) = f(\tilde{x} + Z\boldsymbol{\gamma}),
\]
e a partir dela podemos considerar o seguinte problema de minimização sem restrições
\[ \label{problema_minimizacao_irrestrita_phi}
\min_{\boldsymbol{\gamma}} \varphi (\boldsymbol{\gamma}) .
\]

Vejamos um exemplo de como determinar uma parametrização para o conjunto factível e a partir dela obter a função $\varphi$.

\begin{exem}
Considere o problema 
\[ \label{exem:parametrizacao_omega}
\begin{aligned}
\min_{x} & \quad x_{1}^{2} + 2x_{2}^{2} - 2x_{1} - 2x_{1}x_{2} \\
\text{s.a} & \quad 2x_{1} + x_{2} = 1 .
\end{aligned}
\]
Podemos reescrever a restrição na forma matricial
\[
\begin{bmatrix*} 2 & 1 \end{bmatrix*} \begin{bmatrix*} x_{1} \\ x_{2} \end{bmatrix*} = \begin{bmatrix*} 1 \end{bmatrix*} .
\]
Assim, denotando $A= \begin{bmatrix*} 2 & 1 \end{bmatrix*}$, é preciso primeiramente determinar uma base para $\mathcal{N}(A)$, pois ele é o conjunto das direções factíveis. Para tanto, basta resolver o sistema $Ad =0$ e a partir disso obtemos 
\[ \label{eq:1_exem_parametrizacao_omega}
Z = \begin{bmatrix*} 1 \\ -2 \end{bmatrix*} ,
\]
em que a coluna da matriz $Z$ é uma base do $\mathcal{N}(A)$. Por conseguinte, é necessário determinar um ponto $\tilde{x}$ factível. Então, resolvendo
\[
2x_{1} + x_{2} = 1 ,
\]
temos que 
\[
x_{2} = 1 - 2x_{1} ,
\]
e escolhendo $x_{1} =1$ obtemos $x_{2} = -1$, isto é, $\tilde{x} = (1,-1)$. Assim, podemos reescrever o conjunto factível $\Omega$ como proposto em \eqref{parametrizacao_de_Omega}:
\[
\Omega = \left\{ x\in \RR^{2} \mid x = \begin{bmatrix*} 1 \\ -1 \end{bmatrix*} + \gamma \begin{bmatrix*} 1 \\ -2 \end{bmatrix*} , \ \gamma \in \RR \right\} .
\]
A partir dessa caracterização de $\Omega$ definimos a função $\varphi : \RR \rightarrow \RR$ dada por 
\[
\varphi(\gamma) = f(\tilde{x} + Z\gamma) = 13\gamma^{2} + 14\gamma +3 ,
\]
e o problema \eqref{exem:parametrizacao_omega} pode ser reformulado para um problema de minimização sem restrições dado por
\[ \label{exem:parametrizacao_omega_funcao_phi}
\min_{\boldsymbol{\gamma}} \varphi (\boldsymbol{\gamma}) = 13\gamma^{2} + 14\gamma +3 .
\]
Dessa forma, para determinar uma solução do problema \eqref{exem:parametrizacao_omega_funcao_phi} utilizamos a condição suficiente de segunda ordem para problemas irrestritos dada pelo \Cref{teo:condicao_suficiente_irrestrito}. Ou seja, dado $\gamma^{*} \in \RR$, devemos ter
\[
\nabla \varphi(\gamma^{*}) = \begin{bmatrix*} 26 \gamma^{*} +14 \end{bmatrix*} = 0 ,
\]
e resolvendo para $\gamma^{*}$, obtemos
\[
\gamma^{*} = - \dfrac{7}{13} .
\]
Além disso, como
\[
\nabla^{2} \varphi(\gamma^{*}) = \begin{bmatrix*} 26 \end{bmatrix*} \succ 0 ,
\]
pois para todo $x\in \RR$, com $x\neq 0$, temos que $x^{T}\nabla^{2} \varphi(\gamma^{*})x = 26x^{2} \geq 0$, concluímos que $\gamma^{*} =  -\dfrac{7}{13}$ é minimizador local de \eqref{exem:parametrizacao_omega_funcao_phi}. E definindo $x^{*} \coloneqq  \tilde{x} + Z\boldsymbol{\gamma}^{*}$ temos que 
\[
x^{*} = \begin{bmatrix*} 1 \\ -1 \end{bmatrix*}  -\dfrac{7}{13}\begin{bmatrix*} 1 \\ -2 \end{bmatrix*}  = \begin{bmatrix*} \frac{6}{13} \\ \frac{1}{13} \end{bmatrix*} ,
\]
é minimizador local do problema \eqref{exem:parametrizacao_omega}.
\end{exem}

Com efeito, para determinar a solução do problema \eqref{exem:parametrizacao_omega} utilizamos o fato de haver uma equivalência entre sua solução e a solução do problema irrestrito \eqref{exem:parametrizacao_omega_funcao_phi}. Essa equivalência entre as soluções é garantida pela proposição a seguir.

\begin{prop} \label{prop:equivalencia_problema_restricoes_igualdade_irrestrito}
O vetor $\boldsymbol{\gamma}^{*}$ é um minimizador local (global) de $\varphi$ em $\RR^{n-m}$ se, e somente se, $x^{*} \coloneqq  \tilde{x} + Z\boldsymbol{\gamma}^{*}$ é um minimizador local (global) de \eqref{problema_geral_minimizacao_igualdade}.
\end{prop}
\begin{proof}
Seja $\boldsymbol{\gamma}^{*}$ um minimizador local (global) de $\varphi$ em $\RR^{n-m}$. Então, existe $\varepsilon >0$ tal que
\[
\varphi (\boldsymbol{\gamma}^{*}) \leq \varphi (\boldsymbol{\gamma}) ,
\]
para todo $\boldsymbol{\gamma} \in B( \boldsymbol{\gamma}^{*} , \delta ) \cap \RR^{n-m}$, em que $\delta \coloneqq \dfrac{\varepsilon}{\Vert Z \Vert}$. Logo, definindo $x^{*}\coloneqq \tilde{x} + Z\boldsymbol{\gamma}^{*} \in \Omega$ temos, para $\boldsymbol{\gamma} \in B( \boldsymbol{\gamma}^{*} , \delta ) \cap \RR^{n-m}$, que
\[
 f(x^{*}) = f(\tilde{x} + Z\boldsymbol{\gamma}^{*} ) = \varphi (\boldsymbol{\gamma}^{*}) \leq \varphi (\boldsymbol{\gamma}) = f(\tilde{x} + Z\boldsymbol{\gamma}) = f(x) ,
\]
em que $x = \tilde{x} + Z\boldsymbol{\gamma} \in \Omega$. Note que
\[
\begin{aligned}
\Vert x-x^{*} \Vert &= \Vert (\tilde{x} + Z\boldsymbol{\gamma}) - (\tilde{x} + Z\boldsymbol{\gamma}^{*}) \Vert \\
&= \Vert Z(\boldsymbol{\gamma} - \boldsymbol{\gamma}^{*} )\Vert \\
&\leq \Vert Z\Vert \Vert \boldsymbol{\gamma} - \boldsymbol{\gamma}^{*} \Vert \label{eq:teste} \\
&< \Vert Z\Vert \dfrac{\varepsilon}{\Vert Z \Vert} = \varepsilon , 
\end{aligned}
\]
em que na primeira desigualdade usamos a consistência da norma-2. Portanto, $x\in B(x^{*}, \varepsilon ) \cap \Omega$, e assim,  $x^{*}$ é um minimizador local (global) de \eqref{problema_geral_minimizacao_igualdade}.

Reciprocamente, suponhamos que $\boldsymbol{\gamma}^{*} \in \RR^{n-m}$ não é mínimo local (global) de $\varphi$. Então, dado $\varepsilon >0$ existe $\boldsymbol{\bar{\gamma}} \in B(\boldsymbol{\gamma}^{*} , \delta )$, com $\delta \coloneqq \dfrac{\varepsilon}{\Vert Z\Vert}$, tal que 
\[ \label{eq:1_prop:equivalencia_problema_restricoes_igualdade_irrestrito}
\varphi(\boldsymbol{\bar{\gamma}}) < \varphi({\boldsymbol{\gamma}^{*}}) .
\]
Neste caso, $f(\xbar) < f(x^{*})$, em que $x^{*} \coloneqq \tilde{x} + Z\boldsymbol{\gamma}^{*}, \xbar \coloneqq \tilde{x} + Z\boldsymbol{\bar{\gamma}} \in \Omega $. Além disso,
\[
\begin{aligned}
\Vert \xbar -x^{*} \Vert &= \Vert (\tilde{x} + Z\boldsymbol{\bar{\gamma}}) - (\tilde{x} + Z\boldsymbol{\gamma}^{*}) \Vert \\
&= \Vert Z(\boldsymbol{\bar{\gamma}} - \boldsymbol{\gamma}^{*} )\Vert \\
&\leq \Vert Z\Vert \Vert \boldsymbol{\bar{\gamma}} - \boldsymbol{\gamma}^{*} \Vert \\
&< \Vert Z\Vert \delta \\
&= \Vert Z\Vert \dfrac{\varepsilon}{\Vert Z \Vert} = \varepsilon , 
\end{aligned}
\]
isto é, $\xbar \in B(x^{*} , \varepsilon )$. Portanto, $x^{*}$ não é minimizador local (global) de \eqref{problema_geral_minimizacao_igualdade}.
\end{proof}

Agora, como \eqref{problema_minimizacao_irrestrita_phi} é um problema de minimização irrestrito, o \Cref{teo:condicao_necessaria_1_ordem} nos diz que a condição necessária de primeira ordem para algum $\boldsymbol{\gamma}^{*} \in \RR^{n-m}$ é
\[ \label{condicao_1_ordem_phi}
\nabla \varphi(\boldsymbol{\gamma}^{*}) = 0.
\]

Por \eqref{funcao_phi_restricoes_igualdade} e definindo a função $g: \RR^{n-m} \rightarrow \RR^{n}$, com $g(\boldsymbol{\gamma}) = \tilde{x} + Z\boldsymbol{\gamma}$, temos que $\varphi(\boldsymbol{\gamma}) = f(g(\boldsymbol{\gamma}))$. Aplicando a regra da cadeia para calcular sua derivada, obtemos
\[
\nabla \varphi (\boldsymbol{\gamma})^{T} = \nabla f(g(\boldsymbol{\gamma})) J_{g}(\boldsymbol{\gamma}) ,
\]
em que $J_{g}(\boldsymbol{\gamma})$ é a matriz Jacobiana de $g$ em $\boldsymbol{\gamma}$. Note que $J_{g}(\boldsymbol{\gamma}) = Z$, portanto
\[
\nabla \varphi(\boldsymbol{\gamma}) = Z^{T}\nabla f(g(\boldsymbol{\gamma})) .
\]

Desse modo, da condição \eqref{condicao_1_ordem_phi}, resulta que
\[
0 = \nabla \varphi (\boldsymbol{\gamma}^{*}) = Z^{T} \nabla f(\tilde{x} + Z\boldsymbol{\gamma}^{*}) = Z^{T} \nabla f(x^{*}).
\]

Consequentemente, uma condição necessária de primeira ordem para que $x^{*}$ seja minimizador local de \eqref{problema_geral_minimizacao_igualdade} é que
\[ \label{condicao_1_ordem_f}
Z^{T} \nabla f(x^{*}) = 0,
\]
ou seja, que $(z_{i})^{T} \nabla f(x^{*}) =0$, para todo $i=1, \ldots , n-m$. Como $\{ z_{1}, \ldots , z_{n-m} \}$ é uma base de $\mathcal{N}(A)$, tal condição implica que $\nabla f(x^{*})$ seja ortogonal a $\mathcal{N}(A)$. Logo, pelas considerações feitas anteriormente (\Cref{prop:complemento_ortogonal_nucleo}), temos que $\nabla f(x^{*}) \in Im(A^{T})$, em outras palavras, $\nabla f(x^{*})$ deve ser uma combinação linear das linhas de $A$. Portanto, existe $\boldsymbol{\lambda}^{*} \in \RR^{m}$ tal que
\[ \label{condicao_1_ordem_gradiente}
\nabla f(x^{*}) = A^{T} \boldsymbol{\lambda}^{*} .
\] 

Observe que as condições propostas em \eqref{condicao_1_ordem_f} e \eqref{condicao_1_ordem_gradiente} são equivalentes. Com efeito, se \eqref{condicao_1_ordem_f} se verifica, isso implica que $\nabla f(x^{*}) \in \mathcal{N}(A)^{\bot} = Im(A^{T})$ e portanto \eqref{condicao_1_ordem_gradiente} é verdadeiro. Reciprocamente, se \eqref{condicao_1_ordem_gradiente} ocorre, temos que, pela \Cref{prop:complemento_ortogonal_nucleo}, $Z^{T}\nabla f(x^{*}) = Z^{T}(A^{T} \boldsymbol{\lambda}^{*}) = 0$.

Assim, em vista do que segue acima temos o seguinte teorema.
\begin{teo} \label{teo:multiplicadores_de_lagrange} (\textbf{Multiplicadores de Lagrange})
Se $x^{*}$ é um minimizador local de \eqref{problema_geral_minimizacao_igualdade}, então existe $\boldsymbol{\lambda}^{*} \in \RR^{m}$ tal que $(x^{*}, \boldsymbol{\lambda}^{*})$ é solução do seguinte sistema de $(n+m)$ equações
\[ \label{problema_restricoes_igualdade_dual}
\begin{aligned}
& \nabla f(x^{*}) = A^{T} \boldsymbol{\lambda}^{*} \\
& Ax^{*} = b .
\end{aligned}
\]
\end{teo}

O vetor $\boldsymbol{\lambda}^{*} \in \RR^{m}$ é chamado vetor de \emph{multiplicadores de Lagrange} associado a $x^{*}$.

A solução de \eqref{problema_geral_minimizacao_igualdade} é necessariamente solução de \eqref{problema_restricoes_igualdade_dual}, porém para que a recíproca seja verdadeira é necessária informação de segunda ordem.

Vejamos a seguir um exemplo em que as condições de otimalidade de primeira ordem nos permitem determinar a expressão geral para uma solução de um problema.

\begin{exem} \label{exem:exemplo_minimizar_norma}
Queremos encontrar uma solução do sistema linear $Ax =b$, em que $m<n$, (portanto um sistema linear com infinitas soluções), com a menor norma possível. Podemos descrever matematicamente este problema da seguinte forma
\[\begin{aligned} \label{exem:minimizar_norma}
\min_{x} & \quad \dfrac{1}{2}\Vert x \Vert^{2} \\
\text{s.a} & \quad Ax=b,
\end{aligned} \]
com $x \in \RR^{n}$, $A \in \RR^{m\times n}$, $b\in \RR^{m}$, $m<n$ e $\posto (A)=m$. Com efeito, seja $\tilde{x}$ solução de \eqref{exem:minimizar_norma}. Então, $\tilde{x}$ também minimiza $\Vert x \Vert^{2}$ que por sua vez minimiza $\Vert x \Vert$, pois os problemas são equivalentes. Além disso, veremos que a solução $\tilde{x}$ desse problema pode ser escrita como $\tilde{x} = A^{\dag}b$, em que $A^{\dag} \in \RR^{n\times m}$ e $AA^{\dag} =I$.
\end{exem}
Assim, inicialmente $\tilde{x}$ deve satisfazer a restrição de igualdade, isto é,
\[ \label{exem:minimizar_norma_verifica_restricao}
A\tilde{x} = b .
\]
Além disso, se $\tilde{x}$ é solução então,
\[
\nabla f(\tilde{x}) =A^{T} \tilde{\boldsymbol{\lambda}},
\]
e como $\nabla f(\tilde{x}) = \tilde{x}$, obtemos 
\[ \label{exem:minimizar_norma_gradiente}
\tilde{x} = A^{T} \tilde{\boldsymbol{\lambda}} .
\]
Substituindo \eqref{exem:minimizar_norma_gradiente} em \eqref{exem:minimizar_norma_verifica_restricao}, temos
\[
AA^{T} \tilde{\boldsymbol{\lambda}} = b,
\]
e como por hipótese $A$ é posto completo, temos que $AA^{T}$ é não singular. Desse modo,
\[
(AA^{T})^{-1} AA^{T} \tilde{\boldsymbol{\lambda}} = (AA^{T})^{-1} b,
\]
o que implica em 
\[ \label{exem:minimizar_norma_lambdatil}
\tilde{\boldsymbol{\lambda}} = (AA^{T})^{-1} b.
\]
Agora, substituindo \eqref{exem:minimizar_norma_lambdatil} em \eqref{exem:minimizar_norma_gradiente}, concluímos que
\[
\tilde{x} = A^{T}(AA^{T})^{-1}b = A^{\dag} b,
\]
em que $A^{\dag} = A^{T}(AA^{T})^{-1} \in \RR^{n\times m}$ e $AA^{\dag} = AA^{T}(AA^{T})^{-1} = I$.

\begin{obs}
A matriz $A^{\dag}$ é também chamada de \emph{pseudo-inversa} de Moore-Penrose. Para mais informações consulte \textcite[p.423]{meyer2000matrix}.
\end{obs}


\subsubsection{Condições Necessárias e Suficientes de Segunda Ordem} \label{subsubsection:condicao_2ordem_igualdade}

A condição necessária de segunda ordem para uma solução $\boldsymbol{\gamma}^{*}$ do problema \eqref{problema_minimizacao_irrestrita_phi} é dada pelo \Cref{teo:condicao_necessaria_2_ordem} e toma a seguinte forma
\[ \label{hessiana_phi}
\nabla^{2} \varphi (\boldsymbol{\gamma}^{*}) \succcurlyeq 0.
\]

Derivando $\nabla \varphi (\boldsymbol{\gamma}) = Z^{T}\nabla f(\tilde{x} + Z\boldsymbol{\gamma})$ através da regra da cadeia, obtemos
\[
\nabla^{2} \varphi(\boldsymbol{\gamma}) = Z^{T} \nabla^{2} f(\tilde{x} + Z\boldsymbol{\gamma})Z .
\]
Assim, por \eqref{hessiana_phi} temos que a condição necessária de segunda ordem para que $x^{*}$ seja minimizador local de \eqref{problema_geral_minimizacao_igualdade} é
\[
Z^{T} \nabla^{2} f(x^{*})Z \succcurlyeq 0.
\]

Ademais, observe que $Z^{T}\nabla^{2} f(x^{*})Z$ é uma matriz de ordem $(n-m)\times (n-m)$, e o fato de ser semidefinida positiva significa que
\[
y^{T}\nabla^{2} f(x^{*})y \geq 0 \ \text{para todo} \ y \in \mathcal{N}(A) .
\]
Isso significa que $\nabla^{2} f(x^{*})$ é semidefinida positiva para todo $y\in \mathcal{N}(A)$. Desse modo, com base no que foi desenvolvido acima temos o seguinte teorema da condição necessária de segunda ordem.

\begin{teo}(\textbf{Condição necessária de segunda ordem}) \label{teo:condicao_necessaria_2ordem_igualdade}
Se $x^{*} \in \RR^{n}$ é minimizador local de \eqref{problema_geral_minimizacao_igualdade}, então  
\begin{enumerate}
	\item[(i)] existe $\boldsymbol{\lambda}^{*} \in \RR^{m}$ tal que $\nabla f(x^{*}) = A^{T} \boldsymbol{\lambda}^{*}$;
	\item[(ii)] para todo $y \in \mathcal{N}(A)$ temos que $y^{T}\nabla^{2} f(x^{*})y \geq 0$.
\end{enumerate}
\end{teo}

Analogamente, aplicando o \Cref{teo:condicao_suficiente_irrestrito} para o problema \eqref{problema_minimizacao_irrestrita_phi} podemos determinar as condições suficientes de segunda ordem para o problema \eqref{problema_geral_minimizacao_igualdade}. De fato, pelo \Cref{teo:condicao_suficiente_irrestrito}, se $\boldsymbol{\gamma}^{*} \in \RR^{n-m}$ satisfaz
\[ \label{eq:1_condicao_suficiente_igualdade}
\nabla \varphi(\boldsymbol{\gamma}^{*}) =0
\] 
e
\[ \label{eq:2_condicao_suficiente_igualdade}
\nabla^{2} \varphi(\boldsymbol{\gamma}^{*}) \succ 0,
\]
então $\boldsymbol{\gamma}^{*}$ é minimizador local de \eqref{problema_minimizacao_irrestrita_phi}. Então, com base no que foi desenvolvido até o momento, temos que de \eqref{eq:1_condicao_suficiente_igualdade} decorre que existe $\boldsymbol{\lambda}^{*} \in \RR^{m}$ tal que
\[ \label{eq:3_condicao_suficiente_igualdade}
\nabla f(x^{*}) = A^{T} \boldsymbol{\lambda}^{*} ,
\]
e de \eqref{eq:2_condicao_suficiente_igualdade} resulta que para todo $y \in \mathcal{N}(A)$, com $y \neq 0$, 
\[ \label{eq:4_condicao_suficiente_igualdade}
y^{T}\nabla^{2} f(x^{*})y > 0.
\]
Portanto, se $x^{*} \in \Omega$ satisfaz \eqref{eq:3_condicao_suficiente_igualdade} e \eqref{eq:4_condicao_suficiente_igualdade}, então ele é um minimizador local de \eqref{problema_geral_minimizacao_igualdade}. Assim, usando os mesmos argumentos dos \Cref{teo:multiplicadores_de_lagrange,teo:condicao_necessaria_2ordem_igualdade}, segue o \Cref{teo:condicao_suficiente_2ordem_igualdade}. 

\begin{teo}(\textbf{Condição suficiente de segunda ordem}) \label{teo:condicao_suficiente_2ordem_igualdade}
Se ${x}^{*} \in \RR^{n}$ verifica $A{x}^{*} = b$ e 
\begin{enumerate}
	\item[(i)] existe $\boldsymbol{\lambda}^{*} \in \RR^{m}$ tal que $\nabla f({x}^{*}) = A^{T}\boldsymbol{\lambda}^{*}$;
	\item[(ii)] para todo $y \in \mathcal{N}(A)$, com $y\neq 0$, temos que $y^{T}\nabla^{2} f(x^{*})y > 0$; 
\end{enumerate}
então ${x}^{*}$ é um minimizador local de \eqref{problema_geral_minimizacao_igualdade}.
\end{teo}
%\begin{proof}
%Para provar que $\tilde{x}$ é minimizador local de $f$ vamos verificar se ele satisfaz as condições suficientes de segunda ordem. Por hipótese temos que $\tilde{x} \in \RR^{n}$ satisfaz a restrição $A\tilde{x} = b$. Por conseguinte, existe $\boldsymbol{\lambda} \in \RR^{m}$ tal que $\nabla f(\tilde{x}) = A^{T}\boldsymbol{\lambda}$. Assim, seja Z a matriz cujas colunas são os vetores de uma base do $\mathcal{N}(A)$. Sabemos que as linhas de $A$ são ortogonais a $\mathcal{N}(A)$ e portanto,
%\[
%Z^{T}\nabla f(\tilde{x}) = Z^{T}A^{T}\boldsymbol{\lambda} = 0.
%\]
%Além disso, como $\nabla^{2} f(\tilde{x})$ é definida positiva e a matriz $Z$ é posto completo, temos que 
%\[
%Z^{T}\nabla^{2} f(\tilde{x}) Z \succ 0.
%\]
%Portanto, $\tilde{x}$ é minimizador local de $f$.
%\end{proof}

Vejamos um exemplo em que as condições suficientes nos permitem determinar uma solução para o problema de minimizar uma função quadrática sujeita a restrições de igualdade.

\begin{exem}
Considere o problema
\[ \begin{aligned} \label{exem:minimizar_quadratica_problema}
\min_{x} & \quad \dfrac{1}{2}x^{T}Qx + p^{T}x + q \\
\text{s.a} & \quad Ax = b,
\end{aligned} \]
em que $Q \in \RR^{n\times n}$ é simétrica, $ x,p \in \RR^{n}$, $q \in \RR$, $A \in \RR^{m\times n}$ e $b \in \RR^{m}$. Seja $Z$ uma base do $\mathcal{N}(A)$ e suponha que $Z^{T}QZ$ é definida positiva. Seja $x^{0}$ tal que $Ax^{0}=b$. Então a solução $\tilde{x}$ é dada por
\[ \label{exem:minimizar_quadratica_solucao}
\tilde{x} = x^{0} - Z(Z^{T}QZ)^{-1} Z^{T}(Qx^{0} + p).
\]
\end{exem}
Para provar que \eqref{exem:minimizar_quadratica_solucao} é solução é preciso verificar se $\tilde{x}$ cumpre as condições suficientes de segunda ordem. Para tanto, devemos ter:
\begin{enumerate}
	\item[(i)] $\tilde{x}$ cumpre a restrição de igualdade. De fato,
	\[ \begin{aligned}
	A\tilde{x} &= A(x^{0} - Z(Z^{T}QZ)^{-1} Z^{T}(Qx^{0} + p)) \\
	&= Ax^{0} - AZ(Z^{T}QZ)^{-1} Z^{T}(Qx^{0} + p) \\
	&= b,
	\end{aligned} \]
	pois $AZ=0$ pelo fato de cada coluna de $Z$ pertencer ao $\mathcal{N}(A)$ e portanto, serem ortogonais as linhas de $A$.

	\item[(ii)] É preciso verificar se $Z^{T}\nabla f(\tilde{x}) = 0$. Como $\nabla f(\tilde{x}) = Q\tilde{x} + p$, temos que
	\[ \begin{aligned}
	Z^{T}(Q\tilde{x} +p) &= Z^{T} Q (x^{0} - Z(Z^{T}QZ)^{-1}Z^{T}(Qx^{0} +p)) + Z^{T}p \\
	&= Z^{T} Qx^{0} - Z^{T}QZ(Z^{T}QZ)^{-1}Z^{T}(Qx^{0} +p) + Z^{T}p \\
	&= Z^{T} Qx^{0} - Z^{T} Qx^{0} - Z^{T} p + Z^{T} p \\
	&= 0.
	\end{aligned} \]

	\item[(iii)] Por fim, $Z^{T}\nabla^{2} f(\tilde{x})Z$ deve ser definida positiva. Assim, como $\nabla^{2} f(\tilde{x}) = Q$, temos que 
	\[
	Z^{T}QZ \succ 0.
	\]
\end{enumerate}
Portanto, $\tilde{x}$ é solução do problema \eqref{exem:minimizar_quadratica_problema}.

\begin{obs}
Veremos mais a frente que no caso em que $Q$ é uma matriz semidefinida positiva, os problemas quadráticos são convexos.
\end{obs}


%-------------------------------------------------------------------------------------------------------------------
\subsection{Minimização com Restrições Lineares de Desigualdade} \label{section:restricoes_desigualdade}

Vamos considerar agora problemas da forma
\[ \label{problema_geral_minimizacao_desigualdade}
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad Wx \leq c,
\end{aligned}
\]
em que $x \in \RR^{n}$ e $W \in \RR^{m\times n}$. Como na seção anterior, estamos interessados nesse primeiro momento em analisar a região de factibilidade e determinar as direções factíveis, que são aquelas em que há espaço para se movimentar dentro da região $\Omega$. 

Denotando cada linha da matriz $W$ da forma $w_{i}^{T} = (w_{i1} , w_{i2} , \ldots , w_{in})$, podemos caracterizar o conjunto factível da seguinte forma
\[
\Omega = \{ x\in \RR^{n} \mid w_{i}^{T}x \leq c_{i} \ \text{para todo} \ i = 1,2, \ldots , m \} .
\]

Cada uma das $m$ desigualdades $w_{i}^{T}x \leq c_{i}$ define em $\RR^{n}$ um \emph{semiespaço}. O hiperplano divisor é dado por $\Hset (w_{i}, c_{i}) = \{ x \mid w_{i}^{T}x = c_{i} \}$ e o semiespaço definido é aquele que está do lado contrário à direção apontada pelo vetor $w_{i}$. Desta forma, a região $\Omega$ consiste na intersecção dos $m$ semiespaços. Este tipo de conjunto de $\RR^{n}$ é chamado \emph{poliedro} ou conjunto \emph{poliedral}. Vamos definir formalmente este conceito.


\begin{defi} \label{defi:conjunto_poliedral}
Um \emph{poliedro} ou um conjunto \emph{poliedral} $\mathcal{P} \subset \RR^{n}$ é definido como o conjunto de soluções de um sistema finito de equações e inequações lineares:
\[
\mathcal{P} = \{ x\in \RR^{n} \mid Ax=b, \ Wx \leq c \} ,
\] 
em que $A\in \RR^{m\times n}$, $W\in \RR^{p\times n}$, $b\in \RR^{m}$ e $c\in \RR^{p}$. Neste contexto, dizemos que $h:\RR^{n} \rightarrow \RR^{m}$, dada por $h(x) = Ax-b$, e $g:\RR^{n} \rightarrow \RR^{p}$, dada por $g(x)=Wx-c$, são funções \emph{afim}.
\end{defi}

A \Cref{fig:regiao_factivel_poliedro} representa um poliedro em $\RR^{2}$.

\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.45\textwidth]{conjunto_factivel_poliedral}
	\caption{Conjunto factível poliedral. \\ Fonte: \textcite{Ana1994} \label{fig:regiao_factivel_poliedro}}
\end{figure}

Um conceito fundamental para formular as condições de otimalidade para o problema \eqref{problema_geral_minimizacao_desigualdade} e que fornece uma grande quantidade de informações é o de \emph{restrição ativa}. Portanto, consideramos, no que segue, que $\Omega$ é o conjunto factível deste problema.

\begin{defi}
Dado $\xbar \in \Omega$, dizemos que a restrição de desigualdade $w_{i}^{T}\bar{x} \leq c_{i}$ que corresponde ao índice $i\in \{ 1,2, \ldots , m\}$ é \emph{ativa} no ponto $\xbar$ quando $w_{i}^{T}\bar{x} = c_{i}$. Caso $w_{i}^{T}\bar{x} < c_{i}$ dizemos que a restrição é \emph{inativa} em $\xbar$. O conjunto dos índices das restrições de desigualdade ativas no ponto $\xbar \in \Omega$ é denotado por
\[
\mathcal{I} (\xbar) = \{ i = 1,2, \ldots , m \mid w_{i}^{T}\bar{x} = c_{i} \} .
\]
\end{defi} 


A cada $x\in \Omega$ podemos associar um número $r(x)$, com $0\leq r(x) \leq m$, que representa a quantidade de restrições ativas em $x$. Assim, na \Cref{fig:restricoes_ativas} temos que $r(x^{1}) = 1$, $r(x^{2}) = 2$, $r(x^{3}) = 1$, $r(x^{4}) = 0$ e $r(x^{5}) = 2$.

\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.45\textwidth]{restricoes_ativas}
	\caption{Restrições ativas e inativas. \label{fig:restricoes_ativas}}
\end{figure}

Para $x^{*} \in \Omega$, com $1 \leq r(x^{*}) \leq m$, vamos denotar por $\mathcal{I}(x^{*}) = \{ i_{1}, i_{2}, \ldots , i_{r(x^{*})} \}$ o conjunto dos índices das restrições ativas em $x^{*}$, e por $W_{\mathcal{I}} \in \RR^{r(x^{*})\times n}$ a submatriz de $W$ cujas linhas são as que tem índices em $\mathcal{I}(x^{*})$ e, consequentemente, 
\[
c_{\mathcal{I}} = \begin{bmatrix*} c_{i_{1}} \\ c_{i_{2}} \\ \vdots \\ c_{i_{r(x^{*})}} \end{bmatrix*} .
\]

Ademais, o conjunto das direções factíveis a partir de $x$ depende das restrições ativas nesse ponto, pois são elas que restringem o domínio de factibilidade na vizinhança de $x$, enquanto que as restrições inativas não possuem influência na vizinhança de $x$.
O \Cref{teo:direcoes_factiveis_restricao_desigualdade} nos fornece uma caracterização das direções factíveis em $x$.

\begin{teo} \label{teo:direcoes_factiveis_restricao_desigualdade}
Considere $x\in \Omega$ tal que $r(x) = p$ com $0<p \leq m$. Um vetor $d \in \RR^{n}$ é uma direção factível a partir de $x$ se, e somente se, $w_{i}^{T}d \leq 0$ para todo $i \in \mathcal{I} (x)$.
\end{teo}
\begin{proof}
Suponhamos que $d\in \RR^{n}$ é uma direção factível em $x$. Então existe $\tilde{\alpha} >0$ tal que $x+\alpha d \in \Omega$ para todo $\alpha \in (0, \tilde{\alpha} ]$, o que ocorre se, e somente se, $W(x+\alpha d) \leq c$, ou seja, $w_{i}^{T}(x+\alpha d) \leq c_{i}$, para todo $i \in \mathcal{M} \coloneqq \{ 1,2, \ldots ,m \}$. 
Em particular, se $i\in \mathcal{I} (x)$ temos que
\[
w_{i}^{T}(x+\alpha d) = c_{i} + \alpha w_{i}^{T}d \leq c_{i},
\]
e consequentemente, temos
\[
w_{i}^{T}d \leq 0.
\]

Reciprocamente, seja $d\in \RR^{n}$ e suponhamos que $w_{i}^{T}d \leq 0$ para todo $i \in \mathcal{I} (x)$. Assim, se $i \in \mathcal{I} (x)$, temos que $w_{i}^{T} x = c_{i}$, implicando que, para qualquer $\alpha >0$, $w_{i}^{T}(x+\alpha d) \leq c_{i}$.
Por outro lado, se $i \notin \mathcal{I}(x)$, temos que $w_{i}^{T} x < c_{i}$. Neste caso, é preciso analisar duas situações:
\begin{enumerate}
	\item[(i)] Se $w_{i}^{T}d \leq 0$, então $w_{i}^{T}(x + \alpha d) \leq c_{i}$, para qualquer $\alpha >0$             ;

	\item[(ii)]Se $w_{i}^{T}d >0$ podemos encontrar $\alpha_{i} >0$ de modo que $w_{i}^{T}x + w_{i}^{T}(\alpha_{i} d) = c_{i}$. Considere que como $w_{i}^{T}x < c_{i}$ existe $\beta_{i} >0$, tal que $w_{i}^{T}x + \beta_{i} = c_{i}$. Desse modo, temos que escolher $\alpha_{i}$ tal que $\beta_{i} = \alpha_{i} w_{i}^{T} d$, isto é,
	\[
	w_{i}^{T}x + \alpha_{i} w_{i}^{T}d = c_{i} ,
	\]
	para todo $i \notin \mathcal{I}(x)$. Podemos fazer isso resolvendo a equação anterior para $\alpha_{i}$, encontrando
	\[ \label{eq:1_teo_direcoes_factiveis_restricao_desigualdade}
	\alpha_{i} = \dfrac{c_{i} - w_{i}^{T}x}{w_{i}^{T}d}.
	\]
	Assim, se tomarmos o mínimo de \eqref{eq:1_teo_direcoes_factiveis_restricao_desigualdade}, para todo $i \in \mathcal{M} \backslash \mathcal{I}(x)$, isto é, se definirmos
	\[ \label{eq:2_teo_direcoes_factiveis_restricao_desigualdade}
	\tilde{\alpha} = \min_{i \in \mathcal{M} \backslash \mathcal{I}(x) } \left( \dfrac{c_{i} - w_{i}^{T}x}{w_{i}^{T}d} \right) ,
	\]
	teremos que $w_{i}^{T}(x + \alpha d) \leq c_{i}$ para todo $i \in \mathcal{M}$ e $\alpha \in (0,\tilde{\alpha} ]$. 
\end{enumerate}	
Portanto, de (i) e (ii), $d$ será uma direção factível a partir de $x$.
\end{proof}

A demonstração do \Cref{teo:direcoes_factiveis_restricao_desigualdade} nos dá, na equação \eqref{eq:2_teo_direcoes_factiveis_restricao_desigualdade}, o quanto podemos andar em uma direção factível $d$ a partir de $x \in \Omega$ e ainda continuarmos sobre o conjunto factível. Este critério é também chamado \emph{Teste da razão}.


\subsubsection{Condições Necessárias de Primeira Ordem}

No intuito de determinar as condições de otimalidade no caso de problemas com restrições de desigualdade, acabamos de desenvolver a noção de direções factíveis para avaliar a estrutura do conjunto factível na vizinhança de uma solução. Agora, é necessário definir o conceito de direção de descida.

\begin{defi} \label{defi:direcao_de_descida}
Dizemos que $d\in \RR^{n}$ é uma \emph{direção de descida} de $f:\RR^{n} \rightarrow \RR$ no ponto $\xbar \in \RR^{n}$, se existe $\varepsilon >0$ tal que
\[
f(\xbar + td) <f(\xbar)
\]
para todo $t \in (0,\varepsilon ]$.
\end{defi}
Denotamos por $\mathcal{D}_{f}(\xbar)$ o conjunto de todas as direções de descida da função $f$ no ponto $\xbar$.


\begin{lema} \label{lema:direcao_de_descida}
Seja $f:\RR^{n} \rightarrow \RR$ uma função diferenciável no ponto $\xbar \in \RR^{n}$. Então:
\begin{enumerate}
	\item[(i)] Para todo $d \in \mathcal{D}_{f}(\xbar)$, tem-se que $\nabla f(\xbar)^{T}d \leq 0$.

	\item[(ii)] Se $d\in \RR^{n}$ satisfaz $\nabla f(\xbar)^{T}d < 0$, tem-se que $d \in \mathcal{D}_{f}(\xbar)$.
\end{enumerate}	
\end{lema}
\begin{proof}
Seja $d \in \mathcal{D}_{f}(\xbar)$. Assim, para todo $t>0$ suficientemente pequeno, 
\[ \label{eq:1_lema_direcao_de_descida}
f(\xbar + td) < f(\xbar).
\]
Pelo \Cref{fato:Taylor_primeira_ordem}, temos
\[ \label{eq:2_lema_direcao_de_descida}
f(\xbar + td) - f(\xbar) = t\nabla f(\xbar)^{T} d + o(t),
\] 
com $\displaystyle\lim_{t\rightarrow 0} \dfrac{o(t)}{t} =0$, e de \eqref{eq:1_lema_direcao_de_descida} e \eqref{eq:2_lema_direcao_de_descida}, obtemos
\[ \label{eq:3_lema_direcao_de_descida}
t\nabla f(\xbar)^{T}d + o(t) < 0.
\]
Dividindo ambos os lados da desigualdade \eqref{eq:3_lema_direcao_de_descida} por $t>0$ e passando o limite quando $t \rightarrow 0^{+}$, obtemos
\[
\nabla f(\xbar)^{T}d \leq 0,
\]
provando o item $(i)$.

Suponhamos agora que $\nabla f(\xbar)^{T}d <0$. Aplicando o \Cref{fato:Taylor_primeira_ordem}, podemos escrever
\[
f(\xbar + td) - f(\xbar) = t\left( \nabla f(\xbar)^{T}d + \dfrac{o(t)}{t} \right) .
\]
Em particular, para todo $t>0$ suficientemente pequeno, temos
\[
\nabla f(\xbar)^{T}d + \dfrac{o(t)}{t} \leq \dfrac{1}{2} \nabla f(\xbar)^{T}d <0,
\]
o que implica em
\[
f(\xbar + td) - f(\xbar) <0.
\]
Portanto, $d \in \mathcal{D}_{f}(\xbar)$.
\end{proof}

Assim, a partir de um ponto $\xbar \in \Omega$, estamos interessados em saber se existem direções de descida factíveis, isto é, direções factíveis tais que
\[ \label{eq:caracterizacao_direcao_descida}
\nabla f(\xbar)^{T}d < 0,
\]
pois se existe uma direção $d$ que satisfaz \eqref{eq:caracterizacao_direcao_descida}, decorre do \Cref{lema:direcao_de_descida} e da \Cref{defi:direcao_de_descida} que $\xbar$ não é minimizador local do problema \eqref{problema_geral_minimizacao_desigualdade}.

\begin{obs}
Novamente, a análise dependerá das restrição ativas em um dado ponto $\xbar$. Em particular, se $r(\xbar) =0$, o ponto está no interior de $\Omega$ e as condições necessárias e suficientes são as mesmas do caso em que o problema é irrestrito, pois qualquer direção é factível nesse ponto.
\end{obs}

Agora, já possuímos os ferramentais necessários para determinar a condição necessária de primeira ordem a ser satisfeita por uma solução do problema \eqref{problema_geral_minimizacao_desigualdade}.



\begin{teo}(\textbf{Condição necessária de primeira ordem}) \label{teo:condicao_necessaria_1ordem_restricao_desigualdade}
Considere o problema \eqref{problema_geral_minimizacao_desigualdade} com $f \in \mathcal{C}^{1}$. Se $x^{*}$ é minimizador local de \eqref{problema_geral_minimizacao_desigualdade} e $\posto (W_{\mathcal{I}}) = r(x^{*})$, então existe $\boldsymbol{\mu} \in \RR^{r(x^{*})}$ tal que 
%\[ \label{eq:condicao_necessaria_1ordem_restricao_desigualdade_somatorio}
%\nabla f(x^{*}) + \sum_{k=1}^{r(x^{*})} \mu_{k} w_{i_{k}} = 0 \quad \text{e} \quad \mu_{k} \geq 0, \ 1\leq k\leq r(x^{*}),
%\]
%ou, equivalentemente
\[ \label{eq:condicao_necessaria_1ordem_restricao_desigualdade}
\nabla f(x^{*}) + W_{\mathcal{I}}^{T} \boldsymbol{\mu} =0 \quad e \quad \mu_{k} \geq 0, \ 1\leq k\leq r(x^{*}).
\]
\end{teo}
\begin{proof}
Suponhamos, por contradição, que \eqref{eq:condicao_necessaria_1ordem_restricao_desigualdade} não ocorre. Isso pode acontecer por dois motivos:
\begin{enumerate}
\item[(i)] $\nabla f(x^{*}) + W_{\mathcal{I}}^{T} \boldsymbol{\mu} \neq 0 \ \text{para todo} \ \boldsymbol{\mu} \in \RR^{r(x^{*})}$.

Assim, temos que $\nabla f(x^{*})$ não pode ser escrito como combinação linear das linhas de $W_{\mathcal{I}}$. Em outras palavras, $x^{*}$ não é minimizador local do seguinte problema
\[ \label{eq:1_condicao_necessaria_1ordem_restricao_desigualdade}
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad W_{\mathcal{I}}x=c_{\mathcal{I}} ,
\end{aligned}
\]
pois $x^{*}$ não satisfaz a condição necessária de primeira ordem para o problema \eqref{eq:1_condicao_necessaria_1ordem_restricao_desigualdade} estabelecida pelo \Cref{teo:multiplicadores_de_lagrange}. Logo, $x^{*}$ não pode ser minimizador local do problema \eqref{problema_geral_minimizacao_desigualdade}, o que é uma contradição.

\item[(ii)] $\nabla f(x^{*}) + W_{\mathcal{I}}^{T} \boldsymbol{\mu} = 0$, mas existe $j$ tal que $\mu_{j} <0$.

Primeiramente, se $r(x^{*}) = 1$ temos $\mathcal{I} = \{ i_{1} \}$ e então, 
\[
\nabla f(x^{*}) + \mu_{1} w_{i_{1}} =0 ,
\]
com $\mu_{1} <0$, ou equivalentemente, 
\[
\nabla f(x^{*}) = - \mu_{1} w_{i_{1}} .
\]
Assim, tomando $d=-\nabla f(x^{*}) = \mu_{1} w_{i_{1}}$, com $\mu_{1} <0$, temos que $d$ é uma direção de descida, pois
\[
\nabla f(x^{*})^{T}d = -\nabla f(x^{*})^{T} \nabla f(x^{*}) = -\Vert \nabla f(x^{*}) \Vert^{2} <0,
\]
e além disso, $d$ é uma direção factível, pois
\[
w_{i_{1}}^{T}d = \mu_{1} w_{i_{1}}^{T}w_{i_{1}} = \mu_{1} \Vert w_{i_{1}} \Vert^{2} < 0,
\]
devido o fato de $\mu_{1} <0$. Desse modo, $d$ é uma direção de descida factível no ponto $x^{*}$, e portanto $x^{*}$ não pode ser minimizador local de \eqref{problema_geral_minimizacao_desigualdade}, contradizendo a hipótese.

Agora, considere o caso em que $2\leq r(x^{*}) \leq n$. Vamos denotar por $\tilde{W_{\mathcal{I}}}$ a matriz obtida retirando a linha $w_{i_{j}}$ que corresponde ao multiplicador $\mu_{j} <0$. Tomando $d=\proj_{\mathcal{N}(\tilde{W_{\mathcal{I}}})} (-\nabla f(x^{*}))$, temos que
\[
(-\nabla f(x^{*}) -d)^{T}d =0,
\]
pois eles são ortogonais (veja \Cref{fig:projecao_gradiente_sobre_nucleoA}), o que implica em
\[ \label{eq:2_condicao_necessaria_1ordem_restricao_desigualdade}
\nabla f(x^{*})^{T}d = -d^{T}d = -\Vert d \Vert^{2} <0. \]
\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.55\textwidth]{projecao_gradiente_sobre_nucleoW}
	\caption{Projeção de $-\nabla f(x)$ sobre o $\mathcal{N}(\tilde{W_{\mathcal{I}}})$. \\ Fonte: \textcite{Ana1994}. \label{fig:projecao_gradiente_sobre_nucleoA}}
\end{figure}

Logo, $d$ é uma direção de descida. Além disso, $d$ é uma direção factível. Com efeito, temos que
\[
\nabla f(x^{*}) + \mu_{1} w_{i_{1}} + \mu_{2} w_{i_{2}} + \ldots + \mu_{j} w_{i_{j}} + \ldots + \mu_{r(x^{*})} w_{i_{r(x^{*})}} =0
\]
com $\mu_{j} <0$, e como tomamos $d \in \mathcal{N}(\tilde{W_{\mathcal{I}}})$ temos que
\[ \label{eq:3_condicao_necessaria_1ordem_restricao_desigualdade}
w_{i_{k}}^{T}d =0, 
\]
para todo $k \neq j$. Então,
\[
\nabla f(x^{*})^{T}d + \mu_{j} w_{i_{j}}^{T} d =0,
\]
e reescrevendo, obtemos
\[
\nabla f(x^{*})^{T}d = -\mu_{j} w_{i_{j}}^{T} d.
\]
Agora, utilizando \eqref{eq:2_condicao_necessaria_1ordem_restricao_desigualdade} e o fato de $\mu_{j} <0$, obtemos
\[ \label{eq:4_condicao_necessaria_1ordem_restricao_desigualdade}
w_{i_{j}}^{T} d <0.
\]
Assim, de \eqref{eq:3_condicao_necessaria_1ordem_restricao_desigualdade} e \eqref{eq:4_condicao_necessaria_1ordem_restricao_desigualdade}, concluímos que
\[
w_{i_{k}}^{T} d \leq 0,
\]
para todo $k$ tal que $1 \leq k \leq r(x^{*})$, ou seja, $d$ é uma direção factível. Portanto, $d$ é uma direção factível e de descida a partir de $x^{*}$, o que contradiz o fato de $x^{*}$ ser minimizador local de \eqref{problema_geral_minimizacao_desigualdade}.
\end{enumerate}
\end{proof}


%\begin{teo}(\textbf{Condição necessária de primeira ordem}) \label{teo:condicao_necessaria_1ordem_restricao_desigualdade}
%Consideremos o problema \eqref{problema_geral_minimizacao_desigualdade} com $f \in \mathcal{C}^{1}$ e $x^{*} \in \Omega$ tal que $1 \leq r(x^{*}) \leq n$. Seja $\mathcal{I} \subset \mathcal{M}$, $\mathcal{I} = \{ i_{1}, i_{2}, \ldots , i_{r(x^{*})} \}$ tal que $w_{i}^{T}x = c_{i}$ se, e somente se, $i\in \mathcal{I}$. Seja $W_{\mathcal{I}} \in \RR^{r(x^{*})\times n}$ a submatriz de $W$ cujas linhas são as que tem índices em $\mathcal{I}$ e 
%\[
%c_{\mathcal{I}} = \begin{bmatrix*} c_{i_{1}} \\ c_{i_{2}} \\ \vdots \\ c_{i_{r(x^{*})}} \end{bmatrix*} .
%\]
%Supomos que $\posto (W_{\mathcal{I}}) = r(x^{*})$.

%Se $x^{*}$ é minimizador local de \eqref{problema_geral_minimizacao_desigualdade}, então existe $\boldsymbol{\mu} \in \RR^{r(x^{*})}$ tal que 
%\[
%\nabla f(x^{*}) = \sum_{k=1}^{r(x^{*})} \mu_{k} w_{i_{k}} \quad \text{e} \quad \mu_{k} \leq 0, \ 1\leq k\leq r(x^{*}),
%\]
%ou, equivalentemente
%\[ \label{eq:condicao_necessaria_1ordem_restricao_desigualdade}
%\nabla f(x^{*}) = W_{\mathcal{I}}^{T} \boldsymbol{\mu}, \quad \boldsymbol{\mu} \in \RR^{r(x^{*})}
%\]
%e
%\[
%\mu_{k} \leq 0, \ 1\leq k\leq r(x^{*}).
%\]
%\end{teo}
%\begin{proof}
%Suponhamos que \eqref{eq:condicao_necessaria_1ordem_restricao_desigualdade} não ocorre. Isso pode acontecer por dois motivos:
%\begin{enumerate}
%\item[(i)] $\nabla f(x^{*}) \neq W_{\mathcal{I}}^{T} \boldsymbol{\mu} \ \text{para todo} \ \boldsymbol{\mu} \in \RR^{r(x^{*})}$.

%Em outras palavras, temos que $\nabla f(x^{*})$ não pode ser escrito como combinação linear das linhas de $W_{\mathcal{I}}$. Assim, $x^{*}$ não é minimizador local do problema com restrições de igualdade definido por 
%\[ \label{eq:1_condicao_necessaria_1ordem_restricao_desigualdade}
%\begin{aligned}
%\min_{x} & \quad f(x) \\
%\text{s.a} & \quad W_{\mathcal{I}}x=c_{\mathcal{I}} ,
%\end{aligned}
%\]
%pois $x^{*}$ não satisfaz a condição necessária de primeira ordem para o problema \eqref{eq:1_condicao_necessaria_1ordem_restricao_desigualdade}. Logo, $x^{*}$ também não pode ser minimizador local do problema \eqref{problema_geral_minimizacao_desigualdade}, o que é uma contradição.

%\item[(ii)] $\nabla f(x^{*}) = W_{\mathcal{I}}^{T} \boldsymbol{\mu}$, mas existe $j$ tal que $\mu_{j} >0$.

%Se $r(x^{*}) = 1$ e $\mathcal{I} = \{ i_{1} \}$, então $\nabla f(x^{*}) = \mu_{1} w_{i_{1}}$ e $\mu_{1} >0$. Tomando $d=-\nabla f(x^{*})$ temos que
%\[
%w_{i_{1}}^{T}d = -\mu_{1} w_{i_{1}}^{T}w_{i_{1}} = -\mu_{1} \Vert w_{i_{1}} \Vert^{2} < 0.
%\]
%Assim, $d$ é uma direção de descida factível no ponto $x^{*}$, e portanto, $x^{*}$ não pode ser minimizador local de \eqref{problema_geral_minimizacao_desigualdade}, contradizendo a hipótese.

%Agora, se $2\leq r(x^{*}) \leq n$, denotamos por $\tilde{W_{\mathcal{I}}}$ a matriz obtida retirando a linha $w_{i_{j}}$ correspondente ao multiplicador $\mu_{j} >0$. Consideramos $d=P_{\mathcal{N}(\tilde{W_{\mathcal{I}}})} (-\nabla f(x^{*}))$, em que $P_{\mathcal{N}(\tilde{W_{\mathcal{I}}})}$ é o operador projeção ortogonal sobre $\mathcal{N}(\tilde{W_{\mathcal{I}}})$. Então, temos que
%\[
%(-\nabla f(x^{*}) -d)^{T}d =0,
%\]
%implicando em
%\[ \label{eq:2_condicao_necessaria_1ordem_restricao_desigualdade}
%\nabla f(x^{*})^{T}d = -d^{T}d = -\Vert d \Vert^{2} <0,
%\]
%e portanto, $d$ é uma direção de descida. Veja \Cref{fig:projecao_gradiente_sobre_nucleoA}.

%\begin{figure}[!ht] 
%	\centering
%	\includegraphics[width=0.55\textwidth]{projecao_gradiente_sobre_nucleoW}
%	\caption{Projeção de $-\nabla f(x)$ sobre o $\mathcal{N}(\tilde{W_{\mathcal{I}}})$. \\ Fonte: \textcite{Ana1994}. \label{fig:projecao_gradiente_sobre_nucleoA}}
%\end{figure}
%Ademais, vejamos que $d$ também é uma direção factível. Com efeito,
%\[
%\nabla f(x^{*}) = \mu_{1} w_{i_{1}} + \mu_{2} w_{i_{2}} + \ldots + \mu_{j} w_{i_{j}} + \ldots + \mu_{r(x^{*})} w_{i_{r(x^{*})}}
%\]
%e, por construção, como tomamos $d \in \mathcal{N}(\tilde{W_{\mathcal{I}}})$ e $\posto (W_{\mathcal{I}}) = r(x^{*})$, temos que
%\[
%w_{i_{k}}^{T}d =0 \ \text{para todo} \ k \neq j.
%\]
%Então,
%\[
%\nabla f(x^{*})^{T}d = \mu_{j} w_{i_{j}}^{T} d.
%\]
%Usando \eqref{eq:2_condicao_necessaria_1ordem_restricao_desigualdade} e o fato de $\mu_{j} >0$, obtemos
%\[
%w_{i_{j}}^{T} d <0.
%\]
%Portanto, $w_{i_{k}}^{T} d \leq 0$ para todo $k$ tal que $1 \leq k \leq r(x^{*})$, ou seja, $d$ é uma direção factível e de descida, contradizendo o fato de $x^{*}$ ser minimizador local de \eqref{problema_geral_minimizacao_desigualdade}.
%\end{enumerate}
%\end{proof}


\subsubsection{Condições Necessárias e Suficientes de Segunda Ordem}

Desenvolveremos a seguir as condições necessárias e as condições suficientes de segunda ordem para o problema \eqref{problema_geral_minimizacao_desigualdade} com restrições de desigualdade.

\begin{teo}(\textbf{Condição necessária de segunda ordem}) \label{teo:condicao_necessaria_2ordem_desigualdade}
Considere o problema \eqref{problema_geral_minimizacao_desigualdade} com $f \in \mathcal{C}^{2}$ e $r(x^{*})$ e $\mathcal{I}(x^{*})$ definidos como anteriormente. Se $x^{*}$ é um minimizador local do problema \eqref{problema_geral_minimizacao_desigualdade},  então
\begin{enumerate}
	\item[(i)] existe $\boldsymbol{\mu} \in \RR^{r(x^{*})}$ tal que $\nabla f(x^{*}) + W_{\mathcal{I}}^{T} \boldsymbol{\mu} =0$ e $\mu_{k} \geq 0$ para todo $i_{k} \in \mathcal{I}(x^{*})$;

	\item[(ii)] para todo $y \in \mathcal{N}(W_{\mathcal{I}})$ temos que $y^{T} \nabla^{2} f(x^{*})y \geq 0$.
\end{enumerate}
\end{teo}
\begin{proof}
Perceba que se $x^{*}$ é minimizador local de \eqref{problema_geral_minimizacao_desigualdade}, então pelo \Cref{teo:condicao_necessaria_1ordem_restricao_desigualdade}, existe $\boldsymbol{\mu} \in \RR^{r(x^{*})}$ tal que 
\[
\nabla f(x^{*}) + W_{\mathcal{I}}^{T} \boldsymbol{\mu} =0 \quad \text{e} \quad \mu_{k} \geq 0,
\] 
para todo $i_k \in \mathcal{I}(x^{*})$.

Por conseguinte, se $x^{*}$ é minimizador local de \eqref{problema_geral_minimizacao_desigualdade}, em particular, ele é solução do seguinte problema com restrições de igualdade
\[ 
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad W_{\mathcal{I}}x=c_{\mathcal{I}} ,
\end{aligned}
\]
em que $W_{\mathcal{I}}$ é a submatriz de $W$ cujas linhas correspondem às restrições ativas em $x^{*}$. Então, pelo \Cref{teo:condicao_necessaria_2ordem_igualdade}, concluímos que
\[
y^{T}\nabla^{2} f(x^{*})y \geq 0, 
\]
para todo $y \in \mathcal{N}(W_{\mathcal{I}})$. 
\end{proof}


\begin{teo}(\textbf{Condição suficiente de segunda ordem}) \label{teo:condicao_suficiente_2ordem_desigualdade}
Considere o problema \eqref{problema_geral_minimizacao_desigualdade} com $f \in \mathcal{C}^{2}$ e $r(x^{*})$ e $\mathcal{I}(x^{*})$ definidos como anteriormente. Se $x^{*} \in \Omega$ satisfaz 
\begin{enumerate}
	\item[(i)] $\nabla f(x^{*}) + W_{\mathcal{I}}^{T} \boldsymbol{\mu} =0$ com $\mu_{k} \geq 0$ para todo $i_{k} \in \mathcal{I}(x^{*})$; 
	\item[(ii)] $y^{T} \nabla^{2} f(x^{*})y > 0$ para todo $y \in \mathcal{N}(W_{\mathcal{J}})$, $y\neq 0$, em que $\mathcal{J} = \{ k\in \{ 1,2, \ldots , r(x^{*}) \} \mid \mu_{k} >0 \}$;
\end{enumerate}
então $x^{*}$ é um minimizador local de \eqref{problema_geral_minimizacao_desigualdade}.
\end{teo}
\begin{proof}
Primeiramente, se $\mu_{k} >0$ para todo $i_k \in \mathcal{I}(x^{*})$, então
\[
W_{\mathcal{I}} = W_{\mathcal{J}}.
\]
Assim, como $x^{*} \in \Omega$ satisfaz
\[
\nabla f(x^{*}) + W_{\mathcal{J}}^{T} \boldsymbol{\mu} =0
\]
e
\[
y^{T} \nabla^{2} f(x^{*})y > 0,
\]
para todo $y \in \mathcal{N}(W_{\mathcal{J}})$ e $y\neq 0$, então, pelo \Cref{teo:condicao_suficiente_2ordem_igualdade}, $x^{*}$ é minimizador local do problema
\[ 
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad W_{\mathcal{J}}x=c_{\mathcal{J}} ,
\end{aligned}
\]
e consequentemente, é minimizador local do problema \eqref{problema_geral_minimizacao_desigualdade}.

Agora, suponha que existe pelo menos um $k$ tal que $\mu_{k} =0$. Neste caso, observe que
\[
W_{\mathcal{I}}^{T} \boldsymbol{\mu} = W_{\mathcal{J}}^{T} \boldsymbol{\mu}_{\mathcal{J}},
\]
em que $\boldsymbol{\mu}_{\mathcal{J}}$ é vetor formado pelas componentes $\mu_{k}$ que possuem índice em $\mathcal{J} = \{ k\in \{ 1,2, \ldots , r(x^{*}) \} \mid \mu_{k} >0 \}$. Desse modo, temos que $x^{*} \in \Omega$ satisfaz
\[
\nabla f(x^{*}) + W_{\mathcal{J}}^{T} \boldsymbol{\mu}_{\mathcal{J}} =0
\]
e
\[
y^{T} \nabla^{2} f(x^{*})y > 0,
\]
para todo $y \in \mathcal{N}(W_{\mathcal{J}})$ e $y\neq 0$. Assim, o \Cref{teo:condicao_suficiente_2ordem_igualdade} nos garante que $x^{*}$ é minimizador local do problema
\[ 
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad W_{\mathcal{J}}x=c_{\mathcal{J}} ,
\end{aligned}
\]
e portanto, $x^{*}$ é minimizador local do problema \eqref{problema_geral_minimizacao_desigualdade}.
\end{proof}

Vejamos no exemplo a seguir as condições de otimalidade de segunda ordem para um problema de minimização em que a função objetivo é uma quadrática.

\begin{exem}
Considere o problema de programação quadrática
\[ \label{exem:problema_quadratica_restricao_desigualdade}
\begin{aligned}
\min_{x} & \quad \dfrac{1}{2} x^{T}Qx + p^{T}x \\
\text{s.a} & \quad Wx \leq c,
\end{aligned}
\]
em que $Q\in \RR^{n\times n}$ é simétrica, $p\in \RR^{n}$, $W\in \RR^{m\times n}$ e $c\in \RR^{m}$.
De acordo com o \Cref{teo:condicao_necessaria_2ordem_desigualdade}, as condições necessárias de segunda ordem a serem satisfeitas por um ponto $x^{*}$ que é um minimizador local do problema \eqref{exem:problema_quadratica_restricao_desigualdade} são
\begin{enumerate}
\item[(i)] existe $\boldsymbol{\mu} \in \RR^{r(x^{*})}$ tal que
\[
(Qx^{*} +p) + W_{\mathcal{I}}^{T} \boldsymbol{\mu} = 0 \quad \text{e} \quad \mu_{k} \geq 0,
\]
para todo $i_{k} \in \mathcal{I}(x^{*})$, em que $\mathcal{I}(x^{*})$ é o conjunto dos índices das restrições ativas em $x^{*}$ já definido anteriormente, e $\nabla f(x^{*}) = Qx^{*} +p$;

\item[(ii)] para todo $y\in \mathcal{N}(W_{\mathcal{I}})$, isto é, tal que $W_{\mathcal{I}} y=0$, devemos ter
\[
y^{T}Qy \geq 0,
\]
em que $\nabla^{2} f(x^{*}) =Q$.
\end{enumerate}

Por outro lado, para que $x^{*}$ seja minimizador local do problema \eqref{exem:problema_quadratica_restricao_desigualdade}, pelo \Cref{teo:condicao_suficiente_2ordem_desigualdade} é suficiente que $x^{*}$ satisfaça a restrição de desigualdade $Wx \leq b$ e também
\begin{enumerate}
\item[(i)] existe $\boldsymbol{\mu} \in \RR^{r(x^{*})}$ tal que
\[
(Qx^{*} +p) + W_{\mathcal{I}}^{T} \boldsymbol{\mu} = 0 \quad \text{e} \quad \mu_{k} \geq 0,
\]
para todo $i_{k} \in \mathcal{I}(x^{*})$;

\item[(ii)] para todo $y\in \mathcal{N}(W_{\mathcal{J}})$, com $y\neq 0$, temos que
\[
y^{T}Qy >0,
\]
em que $\mathcal{J} = \{ k\in \{1,2, \ldots , r(x^{*}) \} \mid \mu_{k} >0 \}$.
\end{enumerate}	

Além disso, é interessante observar que se tomarmos $Q=I$ e $p=0$, o problema \eqref{exem:problema_quadratica_restricao_desigualdade} pode ser reescrito da seguinte forma
\[ 
\begin{aligned}
\min_{x} & \quad \dfrac{1}{2} \Vert x \Vert^{2} \\
\text{s.a} & \quad Wx \leq b.
\end{aligned}
\]
Tal problema consiste em encontrar uma solução do sistema linear $Wx \leq b$ com a menor norma possível, e como as condições de otimalidade para problemas com restrições de desigualdade consideram, essencialmente, as restrições que são ativas em uma solução do problema, sua solução pode ser caracterizada de modo análogo ao realizado no \Cref{exem:exemplo_minimizar_norma}.
\end{exem}



%------------------------------------------------------------------------------------------------------------------------
\subsection{Minimização com Restrições Lineares de Igualdade e Desigualdade}

Nesta seção abordaremos o problema geral de otimização, isto é, com restrições de igualdade e desigualdade. Para tanto, utilizando os argumentos das \Cref{section:restricoes_igualdade,section:restricoes_desigualdade}, pretendemos obter as condições de otimalidade de Karush-Kuhn-Tucker (KKT) e as condições necessárias e suficientes de segunda ordem para o problema
\[ \label{problema_geral_minimizacao_igualdade_desigualdade}
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad Ax = b, \ Wx \leq c,
\end{aligned}
\]
em que $A\in \RR^{m\times n}$, com $m<n$ e $\posto (A) = m$, $W\in \RR^{p\times n}$, $b\in \RR^{m}$ e $c\in \RR^{p}$. 

Para o problema \eqref{problema_geral_minimizacao_igualdade_desigualdade}, o conjunto de factibilidade $\Omega$ é um poliedro em $\RR^{n}$ definido por 
\[
\Omega \coloneqq \{ x\in \RR^{n} \mid Ax =b \ \text{e} \ Wx \leq c \}.
\]

Por convenção, as restrições de igualdade são consideradas ativas em todo ponto factível, e em decorrência disso, as restrições correspondentes às linhas de $A$ são sempre ativas. Assim, dado $x^{*} \in \Omega$, o conjunto dos índices das restrições ativas em $x^{*}$ é dado por
\[
\mathcal{I}(x^{*}) = \{ 1,2, \ldots ,m,i_{1} , i_{2} , \ldots , i_{s(x)} \} ,
\]
em que $\mathcal{J}(x^{*}) \coloneqq \{ i_{1} , i_{2} , \ldots , i_{s(x)} \}$ será o conjunto de índices que correspondem às restrições de desigualdade que são ativas em $x^{*}$. Ademais, temos que $s(x^{*})$, com $0\leq s(x^{*}) \leq p$, é a quantidade de restrições de desigualdade ativas em $x^{*}$, enquanto que $r(x^{*})$, com $m \leq r(x^{*}) \leq m+p$, é o número total de restrições ativas em $x^{*}$.

Por conseguinte, as caracterizações das direções factíveis estabelecidas pelos \Cref{teo:direcao_factivel_igualdade,teo:direcoes_factiveis_restricao_desigualdade} implicam no seguinte teorema. 
\begin{teo} \label{teo:direcao_factivel_desigualdade_e_igualdade}
Dado um ponto $x^{*} \in \Omega$, o vetor $d\in \RR^{n}$ é uma direção factível em $x^{*}$ se, e somente se, $Ad =0$ e $w_{j}^{T}d \leq 0$ para todo $j\in \mathcal{J}(x^{*})$.
\end{teo}


\subsubsection{Condições Necessárias de Primeira Ordem}

Para enunciar os teoremas a seguir vamos considerar os conjuntos $\mathcal{I}(x^{*})$ e $\mathcal{J}(x^{*})$ como definidos anteriormente. Além disso, denotaremos por $W_{\mathcal{J}}$ a submatriz de $W$ cujas linhas são as que têm os índices em $\mathcal{J}(x^{*})$, por $c_{\mathcal{J}} \in \RR^{s(x^{*})}$ o vetor formado pelas componentes de $c$ correspondentes a $\mathcal{J}(x^{*})$ e por $B \in \RR^{r(x^{*})\times n}$ a matriz dada por
\[
B = \begin{bmatrix*} A \\ W_{\mathcal{J}} \end{bmatrix*} ,
\]
em que $\posto (B)= r(x^{*})$.
\begin{teo}(\textbf{Condição necessária de primeira ordem}) \label{teo:KKT}
Considere o problema \eqref{problema_geral_minimizacao_igualdade_desigualdade} com $f\in \mathcal{C}^{1}$ e $x^{*} \in \Omega$ tal que $m\leq r(x^{*}) \leq n$ e $s(x^{*}) \geq 1$.
Se $x^{*}$ é minimizador local de \eqref{problema_geral_minimizacao_igualdade_desigualdade}, e $\posto (B)= r(x^{*})$, então existem $\boldsymbol{\lambda} \in \RR^{m}$ e $\boldsymbol{\mu} \in \RR^{s(x^{*})}$ tais que
\[ \label{eq:1_teo_KKT}
\nabla f(x^{*}) + A^{T}\boldsymbol{\lambda} + W^{T}_{\mathcal{J}} \boldsymbol{\mu} = 0 \ \ \text{e} \ \ \mu_{k} \geq 0, \ 1 \leq k \leq s(x^{*}) .
\]
\end{teo}
\begin{proof}
Suponhamos por contradição que \eqref{eq:1_teo_KKT} não ocorre. Isso pode acontecer por dois motivos:
\begin{enumerate}
\item[(i)] $\nabla f(x^{*}) + A^{T}\boldsymbol{\lambda} + W^{T}_{\mathcal{J}} \boldsymbol{\mu} \neq 0$ para todo $\boldsymbol{\mu} \in \RR^{s(x^{*})}$.

Assim, temos que $x^{*}$ não é minimizador local do problema
\[ \label{eq:1_proof_KKT}
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad Ax=b, W_{\mathcal{J}}x=c_{\mathcal{J}} ,
\end{aligned}
\]
pois $x^{*}$ não satisfaz a condição necessária de primeira ordem para o problema \eqref{eq:1_proof_KKT} dada pelo \Cref{teo:multiplicadores_de_lagrange}. Em virtude disso, $x^{*}$ não é minimizador local do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade}, o que é uma contradição.


\item[(ii)] $\nabla f(x^{*}) + A^{T}\boldsymbol{\lambda} + W^{T}_{\mathcal{J}} \boldsymbol{\mu} = 0$, mas existe $j$ tal que $\mu_{j} <0$. 

Considere $\tilde{W}_{\mathcal{J}}$ a matriz obtida retirando a linha $w_{i_j}$ que corresponde ao multiplicador $\mu_{j}$, e a matriz $\tilde{B}$ dada por $\tilde{B} = \begin{bmatrix*} A \\ \tilde{W}_{\mathcal{J}} \end{bmatrix*}$. Então, tomando $d = \proj_{\mathcal{N}(\tilde{B})} (-\nabla f(x^{*}))$, temos que
\[
(-\nabla f(x^{*}) -d)^{T} d =0,
\]
pois são ortogonais, e isso implica que
\[ \label{eq:2_proof_KKT}
\nabla f(x^{*})^{T} d = - d^{T}d = -\Vert d \Vert^{2} <0.
\]
Portanto, $d$ é uma direção de descida. Agora, vejamos que $d$ é uma direção factível. De \eqref{eq:1_teo_KKT} temos que
\[
\nabla f(x^{*}) + \lambda_{1} a_{1} + \ldots + \lambda_{m} a_{m} + \mu_{1} w_{i_{1}} + \ldots + \mu_{j} w_{i_{j}} + \ldots + \mu_{s(x^{*})} w_{i_{s(x^{*})}} =0,
\]
com $\mu_{j} <0$. Como tomamos $d\in \mathcal{N}(\tilde{B})$, temos que 
\[ \label{eq:3_proof_KKT}
Ad=0 \quad \text{e} \quad w_{i_k}^{T}d = 0,
\]
para todo $k\neq j$. Então, 
\[
\nabla f(x^{*})^{T}d + \mu_{j} w_{i_{j}}^{T}d =0,
\]
e reescrevendo obtemos
\[
\nabla f(x^{*})^{T}d = -\mu_{j} w_{i_{j}}^{T} d.
\]
Agora, utilizando \eqref{eq:2_proof_KKT} e o fato de $\mu_{j} <0$, concluímos que
\[ \label{eq:4_proof_KKT}
w_{i_{j}}^{T} d <0.
\]
Desse modo, de \eqref{eq:3_proof_KKT} e \eqref{eq:4_proof_KKT} resulta que 
\[ 
Ad=0 \quad \text{e} \quad w_{i_k}^{T}d \leq 0,
\]
para todo $k$ tal que $1 \leq k \leq s(x^{*})$. Logo, $d$ é uma direção factível. Portanto, $d$ é uma direção factível e de descida a partir de $x^{*}$, contradizendo a hipótese de $x^{*}$ ser minimizador local de \eqref{problema_geral_minimizacao_igualdade_desigualdade}.
\end{enumerate}	
\end{proof}

As condições em \eqref{eq:1_teo_KKT} são conhecidas como condições de Karush-Kuhn-Tucker (KKT). A partir delas podemos definir \emph{ponto estacionário}.

\begin{defi}(Ponto estacionário ou KKT) \label{defi:ponto_estacionario_KKT}
Um ponto $x^{*} \in \RR^{n} $ é um \emph{ponto estacionário}, ou ainda, um \emph{ponto KKT} do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade} quando $x^{*} \in \Omega$ e existem $\boldsymbol{\lambda}^{*} \in \RR^{m}$ e $\boldsymbol{\mu}^{*} \in \RR^{p}$, tais que as condições \eqref{eq:1_teo_KKT} são satisfeitas. Os elementos $\boldsymbol{\lambda}^{*}$ e $\boldsymbol{\mu}^{*}$ são chamados de \emph{multiplicadores de Lagrange} associados ao ponto estacionário $x^{*}$.
\end{defi}
Em outras palavras, $x^{*}$ é dito estacionário quando satisfaz as condições KKT. 

%Outro conceito importante e que está implícito no enunciado do \Cref{teo:KKT} é o de \emph{ponto regular}.

%\begin{defi} \label{defi:ponto_regular}
%Seja $x^{*}$ um ponto que satisfaz as restrições
%\[
%h(x^{*}) = 0, \quad g(x^{*}) \leq 0, \label{eq:1_ponto_regular}
%\]
%e seja $\mathcal{J}(x^{*})$ o conjunto dos índices $j$ tais que $g_{j}(x^{*}) = 0$. Então $x^{*}$ é chamado \emph{ponto regular} das restrições \eqref{eq:1_ponto_regular} se os gradientes das restrições de igualdade e desigualdade ativas, isto é, $\nabla h_{i}(x^{*})$ e $\nabla g_{j}(x^{*})$, com $1 \leq i \leq m$ e $j \in \mathcal{J}(x^{*})$, são linearmente independentes.
%\end{defi}

%Note que, se as restrições $h$ e $g$, dadas por $h(x) = Ax-b$ e $g(x) = Wx-c$, são funções afim, um ponto ser regular é equivalente a dizer que as matrizes $A$ e $W_{\mathcal{J}}$ são posto completo, condição esta que independe de $x$. Assim, é importante observar que se não for verificada a hipótese de regularidade, podemos ter minimizadores que não cumprem KKT, o que dificulta a caracterização de tais pontos. Vejamos a seguir um exemplo que justifica essa afirmação.

%\begin{exem}
%Considere o problema 
%\[ \label{exem:minimizador_nao_satisfaz_KKT}
%\begin{aligned}
%\min_{x} & \quad f(x)=x_{1} \\
%\text{s.a} & \quad g_{1}(x)=-x_{1}^{3} + x_{2} \leq 0 , \\
%& \quad g_{2}(x) = -x_{2} \leq 0.
%\end{aligned}
%\]
%O ponto $x^{*} = (0,0)$ é o minimizador do problema \eqref{exem:minimizador_nao_satisfaz_KKT}, no entanto não cumpre as condições KKT. Com efeito, manipulando as restrições de desigualdade temos que $0\leq x_{2} \leq x_{1}^{3}$, o que implica em $f(x^{*}) =0 \leq x_{1} = f(x)$, para todo ponto factível $x$.
%Contudo, como
%\[
%\nabla f(x^{*}) = \begin{bmatrix*} 1 \\ 0 \end{bmatrix*} , \nabla g_{1}(x^{*}) = \begin{bmatrix*} 0 \\ 1 \end{bmatrix*} \ \text{e} \ \nabla g_{2}(x^{*}) = \begin{bmatrix*} 0 \\ -1 \end{bmatrix*},
%\]
%temos que os gradientes das restrições ativas em $x^{*}$ não são LI, e portanto, $x^{*}$ não é um ponto regular. Assim, as condições KKT não são satisfeitas, pois não existem $\mu_{1}^{*} , \mu_{2}^{*} \in \RR^{+}$ tais que 
%\[
%\begin{bmatrix*} 1 \\ 0 \end{bmatrix*} + \mu_{1}^{*} \begin{bmatrix*} 0 \\ 1 \end{bmatrix*} + \mu_{2}^{*} \begin{bmatrix*} 0 \\ -1 \end{bmatrix*} = \begin{bmatrix*} 0 \\ 0 \end{bmatrix*}.
%\]
%\end{exem}

\subsubsection{Condições Necessárias e Suficientes de Segunda Ordem}

As condições necessárias e suficientes de segunda ordem para problemas com restrições de igualdade e desigualdade podem ser obtidas considerando-se essencialmente as restrições ativas.

\begin{teo}(\textbf{Condição necessária de segunda ordem}) \label{teo:condicao_necessaria_2ordem_igualdade_desigualdade}
Considere o problema \eqref{problema_geral_minimizacao_igualdade_desigualdade} com $f \in \mathcal{C}^{2}$ e $r(x^{*})$, $s(x^{*})$, $\mathcal{J}(x^{*})$ e $B$ definidos como anteriormente. Se $x^{*}$ é um minimizador local do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade}, então
\begin{enumerate}
	\item[(i)] existem $\boldsymbol{\lambda} \in \RR^{m}$ e $\boldsymbol{\mu} \in \RR^{s(x^{*})}$ tais que $\nabla f(x^{*}) + A^{T}\boldsymbol{\lambda} + W^{T}_{\mathcal{J}} \boldsymbol{\mu} =0$ e $\mu_{k} \geq 0$ para todo $k\in \{ 1,2, \ldots , s(x^{*}) \}$;
	\item[(ii)] para todo $y\in \mathcal{N}(B)$ temos que $y^{T}\nabla^{2} f(x^{*})y \geq 0$ .
\end{enumerate}	
\end{teo}
\begin{proof}
Primeiramente, se $x^{*}$ é minimizador local do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade} então, pelo \Cref{teo:KKT}, existem $\boldsymbol{\lambda} \in \RR^{m}$ e $\boldsymbol{\mu} \in \RR^{s(x^{*})}$ tais que
\[
\nabla f(x^{*}) + A^{T}\boldsymbol{\lambda} + W^{T}_{\mathcal{J}} \boldsymbol{\mu} =0 \ \ \text{e} \ \ \mu_{k} \geq 0, \ k\in \{ 1,2, \ldots , s(x^{*}) \} .
\]

Ademais, se $x^{*}$ é minimizador local de \eqref{problema_geral_minimizacao_igualdade_desigualdade} então ele é solução do seguinte problema obtido considerando apenas as restrições ativas em $x^{*}$
\[
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad Ax=b, W_{\mathcal{J}}x = c_{\mathcal{J}} .
\end{aligned}
\]
Assim, como $B = \begin{bmatrix*} A \\ W_{\mathcal{J}} \end{bmatrix*}$, concluímos pelo \Cref{teo:condicao_necessaria_2ordem_igualdade} que 
\[
y^{T}\nabla^{2} f(x^{*})y \geq 0 ,
\] 
para todo $y\in \mathcal{N}(B)$.
\end{proof}

Agora, para saber se um determinado ponto é um mínimo local do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade}, é necessário que ele cumpra certas condições suficientes que são estabelecidas pelo teorema a seguir. 

\begin{teo}(\textbf{Condição suficiente de segunda ordem}) \label{teo:condicao_suficiente_igualdade_desigualdade}
Considere o problema \eqref{problema_geral_minimizacao_igualdade_desigualdade} com $f \in \mathcal{C}^{2}$ e $r(x^{*})$, $s(x^{*})$, $\mathcal{J}(x^{*})$ e $B$ definidos como anteriormente. Se $x^{*} \in \Omega$ verifica
\begin{enumerate}
	\item[(i)] existem $\boldsymbol{\lambda} \in \RR^{m}$ e $\boldsymbol{\mu} \in \RR^{s(x^{*})}$ tais que $\nabla f(x^{*}) + A^{T}\boldsymbol{\lambda} + W^{T}_{\mathcal{J}} \boldsymbol{\mu} =0$ e $\mu_{k} \geq 0$ para todo $k\in \{ 1,2, \ldots , s(x^{*}) \}$;
	\item[(ii)] se $y^{T}\nabla^{2} f(x^{*})y >0$ para todo $y\in \mathcal{N}(\tilde{B})$, em que
	\[
	\tilde{B} = \begin{bmatrix*} A \\ W_{\mathcal{K}} \end{bmatrix*}
	\]
	e
	\[
	\mathcal{K} = \{ j\in \mathcal{J} \mid \mu_{j} >0 \};
	\]
\end{enumerate}
então $x^{*}$ é um minimizador local de \eqref{problema_geral_minimizacao_igualdade_desigualdade}.
\end{teo}
\begin{proof}
Suponhamos, inicialmente, que $\mu_{j} >0$ para todo $j\in \mathcal{J}(x^{*})$, então $W_{\mathcal{J}} = W_{\mathcal{K}}$ e, consequentemente, 
\[
B = \tilde{B} .
\]
Desse modo, se $x^{*} \in \Omega$ satisfaz
\[
\nabla f(x^{*}) + A^{T}\boldsymbol{\lambda} + W^{T}_{\mathcal{K}} \boldsymbol{\mu} =0
\]
e
\[
y^{T}\nabla^{2} f(x^{*})y >0,
\]
para todo $y\in \mathcal{N}(\tilde{B})$ tal que $y\neq 0$, então, pelo \Cref{teo:condicao_suficiente_2ordem_igualdade}, $x^{*}$ é minimizador local do problema
\[ 
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad Ax=b, \ W_{\mathcal{K}}x=c_{\mathcal{K}} ,
\end{aligned}
\]
e consequentemente, é minimizador local do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade}.

Agora, suponhamos que existe pelo menos um $j$ tal que $\mu_{j} =0$. Dessa forma, 
\[
W_{\mathcal{J}}^{T}\boldsymbol{\mu} = W_{\mathcal{K}}^{T}\boldsymbol{\mu}_{\mathcal{K}} ,
\]
em que $\boldsymbol{\mu}_{\mathcal{K}}$ é o vetor formado pelas componentes $\mu_{j}$ que possuem índices em $\mathcal{K}$. Então, se $x^{*} \in \Omega$ satisfaz
\[
\nabla f(x^{*}) + A^{T}\boldsymbol{\lambda} + W^{T}_{\mathcal{K}} \boldsymbol{\mu} =0
\]
e
\[
y^{T}\nabla^{2} f(x^{*})y >0,
\]
para todo $y\in \mathcal{N}(\tilde{B})$ e $y\neq 0$, então, pelo \Cref{teo:condicao_suficiente_2ordem_igualdade}, temos que $x^{*}$ é minimizador local do problema
\[ 
\begin{aligned}
\min_{x} & \quad f(x) \\
\text{s.a} & \quad Ax=b, \ W_{\mathcal{K}}x=c_{\mathcal{K}} .
\end{aligned}
\]
Assim, concluímos que $x^{*}$ é minimizador local do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade}.
\end{proof}

Portanto, com este teorema finalizamos o capítulo Conceitos de Otimização, no qual discutimos elementos da teoria de otimização irrestrita e com restrições lineares, abordando as condições de otimalidade para cada caso. Tendo em vista que o problema decorrente da modelagem da técnica SVM, que será formulado na \Cref{chap:maquinas_vetores_suporte}, consiste num problema de programação quadrática convexa com restrições lineares, no próximo capítulo daremos continuidade aos estudos da teoria de otimização, abordando alguns conceitos de otimização convexa, que fornece resultados importantes aos problemas de otimização.

%Para que as condições KKT possam ser consideradas uma condição de otimalidade, isto é, que sejam satisfeitas por um ponto que é minimizador, é necessário assumir alguma hipótese adicional às restrições do problema, que será chamada de condição de qualificação.

%\begin{defi}
%Dizemos que as restrições $g(x) \leq 0$ e $h(x)=0$ cumprem uma \emph{condição de qualificação} em $x^{*} \in \Omega$ quando, dada qualquer função diferenciável $f$ que tenha mínimo em $x^{*}$, relativamente a $\Omega$, sejam satisfeitas as condições de otimalidade de KKT.
%\end{defi}
%Logo, um ponto $x \in \RR^{n}$ é dito qualificado quando atende uma condição de qualificação. Vejamos primeiramente a condição de regularidade.

%A \Cref{defi:ponto_regular} também é conhecida como condição de qualificação de independência linear dos gradientes das restrições ativas (LICQ, do inglês \emph{Linear Independence Constraint Qualification}).

%\textbf{Condição de Qualificação de Independência Linear:} Dizemos que a condição de qualificação de independência linear (LICQ) é satisfeita em $\xbar$ quando o conjunto formado pelos gradientes das restrições de igualdade e das restrições de desigualdade ativas é linearmente independente, isto é, 
%\[
%\{ \nabla g_{i}(\xbar) \mid i \in \mathcal{I}(\xbar) \} \cup \{ \nabla h_{i}(\xbar), i=1, \ldots , m \} \ \text{é LI}.
%\] 

%Outra condição interessante aos nossos estudos, pois menciona a convexidade, é a \emph{Condição de Qualificação de Slater}.

%\textbf{Condição de Qualificação de Slater:} Dizemos que a condição de qualificação de Slater é satisfeita quando $h$ é linear, cada componente $g_{i}$, $i=1, \ldots , p$, é convexa e existe $\tilde{x} \in \Omega$ tal que $h(\tilde{x})=0$ e $g(\tilde{x})<0$.

%Uma classe especial de problemas de otimização se refere ao caso em que o conjunto viável $\Omega$ é um conjunto poliedral.

%\begin{defi}(\textbf{Conjunto viável poliedral:}) Um conjunto viável é \emph{poliedral} quando as restrições $g$ e $h$ são afins. Ou seja, o problema terá o seguinte formato
%\[
%\begin{aligned} \label{eq:conjunto_viavel_poliedral}
%\min_{x} & \quad f(x)  \\
%\text{s.a} & \quad Ax \leq b  \\
%& \quad Mx = r.
%\end{aligned}
%\]
%em que $A \in \RR^{mxn}$, $M\in \RR^{pxn}$, $b \in \RR^{m}$, $r \in \RR^{p}$. 
%Neste contexto, dizemos que $g:\RR^{n} \rightarrow \RR^{m}$, com $g(x) = Ax-b$, e $h:\RR^{n}\rightarrow \RR^{p}$, com $h(x) = Mx - r$, são funções afins.
%\end{defi}

%Portanto, se $x^{*}$ é um minimizador local do problema \eqref{eq:conjunto_viavel_poliedral}, então $x^{*}$ satisfaz as condições KKT.

%-------------------------------------------------------------------------------------------------------------------------
\newpage

\section{Otimização Convexa} \label{chap:otimizacao_convexa}

Neste capítulo o objetivo principal é apresentar alguns resultados e definições relacionados aos problemas de otimização convexa, isto é, quando a função objetivo e o conjunto factível são convexos. A noção de convexidade é muito importante na teoria de otimização, pois ela garante que pontos estacionários são minimizadores e permite concluir que minimizadores locais são globais. Além disso, as condições necessárias de otimalidade tornam-se suficientes sob a hipótese de convexidade. A partir disso, os problemas de classificação podem ser formulados em termos de otimização convexa. Os conceitos e resultados abordados neste capítulo foram desenvolvidos, principalmente, com base nas seguintes referências: \textcite{Evelin2017,Ademir2013,Izmailov2014ac}.

\subsection{Conjuntos Convexos}

\begin{defi} 
Um \emph{conjunto} $C \subset \RR^{n}$ é dito \emph{convexo} quando dados $x,y \in C$, o segmento $[x,y] \coloneqq \{ (1-t)x + ty \mid t\in [0,1] \}$ estiver inteiramente contido em $C$.
\end{defi}

Esta definição pode ser interpretada geometricamente afirmando que um conjunto é convexo se, dados dois pontos no conjunto, cada ponto no segmento de linha que une esses dois pontos também for um membro do conjunto. A \Cref{fig:conjuntos_convexos} ilustra a noção de convexidade de conjuntos, em que o conjunto $C_{1}$ representa um conjunto convexo e o conjunto $C_{2}$ não é convexo.


%criar minha própria figura
\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.60\textwidth]{conjuntos_convexo_naoconvexo}
	\caption{ Conjunto convexo e não-convexo. \label{fig:conjuntos_convexos}}
\end{figure}


Alguns exemplos de conjuntos convexos são o conjunto vazio, o espaço $\RR^{n}$, qualquer hiperplano do $\RR^{n}$ e um conjunto que contém um ponto só. 

No próximo lema veremos que se dois vetores têm a mesma norma euclidiana, então qualquer combinação linear que não seja a trivial ($t=0$ ou $t=1$) terá norma estritamente menor.

\begin{lema} \label{lema:1_convexidade}
Considere $\Vert \cdot \Vert$ a norma euclidiana em $\RR^{n}$. Sejam $u,v \in \RR^{n}$ com $u\neq v$. Se $\Vert u \Vert = \Vert v \Vert = r$, então $\Vert (1-t)u + tv \Vert < r$, para todo $t \in (0,1)$.
\end{lema}

\begin{proof}
Seja $t \in (0,1)$ e suponha que $\Vert u \Vert = \Vert v \Vert = r$. Aplicando a desigualdade triangular, temos
\[
\Vert (1-t)u + tv \Vert \leq (1-t)\Vert u \Vert + t\Vert v \Vert = (1-t)r + tr = r.
\]
Agora, suponha por absurdo que $\Vert (1-t)u + tv \Vert = r$. Então
\[ \label{eq:1_lema1_convexidade}
(1-t)^{2} u^{T}u + 2t(1-t)u^{T}v + t^{2}v^{T}v = \Vert (1-t)u + tv \Vert^{2} = r^{2}.
\]
Como $u^{T}u = v^{T}v = r^{2}$ e $t \in (0,1)$, substituindo em \eqref{eq:1_lema1_convexidade} e desenvolvendo, obtemos 
\begin{align}
r^{2} &= (1-2t+t^{2}) u^{T}u + (2t-2t^{2})u^{T}v + t^{2}v^{T}v \\
&= (1-2t+t^{2}) r^{2} + (2t-2t^{2})u^{T}v + t^{2}r^{2} \\
&= r^{2} - 2tr^{2} + t^{2}r^{2} + (2t-2t^{2})u^{T}v + t^{2}r^{2} .
\end{align}
Evidenciando $r^{2}$, temos
\[ 
(2t-2t^{2})r^{2} = (2t-2t^{2})u^{T}v,
\]
e, portanto,
\[ \label{eq:2_lema1_convexidade}
r^{2} = u^{T}v.
\]
Assim, por \eqref{eq:2_lema1_convexidade},
\[
\Vert u-v \Vert^{2} = u^{T}u - 2u^{T}v + v^{T}v = r^{2} - 2r^{2} + r^{2} = 0,
\]
o que é uma contradição, pois por hipótese $u \neq v$. 
Portanto, concluímos que $\Vert (1-t)u + tv \Vert < r$, para todo $t \in (0,1)$.
\end{proof}

Agora, dado um conjunto $S \subset \RR^{n}$ e um ponto $z \in \RR^{n}$, considere o problema de encontrar um ponto de $S$ mais próximo de $z$, em outras palavras, queremos minimizar a distância de um ponto a um conjunto. Assim, os próximos resultados garantem a existência da solução no caso de $S$ ser um conjunto fechado e sua unicidade se, além de fechado, $S$ for convexo. Tal solução é chamada de projeção de $z$ sobre $S$, e denotada por $\proj_{S} (z)$. 

\begin{lema} \label{lema:existencia_projecao}
Seja $S \subset \RR^{n}$ um conjunto fechado não vazio. Dado $z \in \RR^{n}$, existe $\bar{z} \in S$ tal que
\[
\Vert z - \bar{z} \Vert \leq \Vert z - x \Vert,
\]
para todo $x \in S$.
\end{lema}
\begin{proof}
Seja $\alpha = \inf \{\Vert z-x \Vert \mid x \in S\}$. Então, para cada $n \in \mathds{N}$, existe $x^{n} \in S$ tal que
\[ \label{eq:1_lema_existencia_projecao}
\alpha \leq \Vert z-x^{n} \Vert \leq \alpha + \dfrac{1}{n}. 
\]
Em particular, $\Vert z-x^{n} \Vert \leq \alpha + 1$, para todo $n \in \mathds{N}$. Logo, pelo Teorema de Bolzano-Weierstrass, existe uma subsequência $(x^{n^{k}})$ convergente, com $k \in \mathds{N}'$, tal que $x^{n^{k}} \longrightarrow \bar{z}$. Como $S$ é fechado temos que $\bar{z} \in S$. Além disso, 
\[
\Vert z-x^{n} \Vert \longrightarrow \Vert z-\bar{z} \Vert .
\]
Mas, por \eqref{eq:1_lema_existencia_projecao}, temos que $\Vert z-x^{n} \Vert \longrightarrow \alpha $, e portanto, concluímos que $\Vert z-\bar{z} \Vert = \alpha$.

\end{proof}


\begin{lema}  \label{lema:unicidadeprojecao_convexidade}
Seja $S \subset \RR^{n}$ um conjunto não vazio, convexo e fechado. Dado $z \in \RR^{n}$, existe um único $\bar{z} \in S$ tal que
\[
\Vert z - \bar{z} \Vert \leq \Vert z - x \Vert
\]
para todo $x \in S$.
\end{lema}
\begin{proof}
A existência é garantida pelo \Cref{lema:existencia_projecao}. Para provar a unicidade suponha que existam $\bar{z}, \tilde{z} \in S$, com $\bar{z} \neq \tilde{z}$, ta{}is que
\[ \label{eq:1_lema_unicidade_projecao}
\Vert z-\bar{z} \Vert \leq \Vert z-x \Vert \quad \text{e} \quad \Vert z-\tilde{z} \Vert \leq \Vert z-x \Vert ,
\]
para todo $x \in S$. Tomando $x=\tilde{z}$ na primeira desigualdade e $x=\bar{z}$ na segunda, obtemos
\[
\Vert z-\bar{z} \Vert = \Vert z-\tilde{z} \Vert .
\]
Por outro lado, o ponto $z^{*} = \dfrac{\bar{z} - \tilde{z}}{2}$ pertence ao conjunto convexo $S$. Além disso, pelo \Cref{lema:1_convexidade}, com $r= \Vert z-\bar{z} \Vert = \Vert z - \tilde{z} \Vert$ e $t=1/2$, temos
\begin{align}
\Vert z-z^{*} \Vert &=  \Vert z-t(\bar{z}+\tilde{z}) \Vert \\
& = \Vert z - t\bar{z} - t\tilde{z} \Vert \\
& = \Vert (1-t)(z-\bar{z}) + t(z-\tilde{z}) \Vert \\
& < r ,
\end{align}
o que é uma contradição, pois por \eqref{eq:1_lema_unicidade_projecao} teríamos
\[
r = \Vert z-\bar{z} \Vert = \Vert z - \tilde{z} \Vert \leq \Vert z-z^{*} \Vert < r .
\]
Portanto, $\bar{z} = \tilde{z}$.
\end{proof}


No \Cref{lema:unicidadeprojecao_convexidade} denotamos $\bar{z} = \proj_{S} (z)$.


\begin{teo}  \label{lema_condicaoprojecao_convexidade}
Sejam $S \subset \RR^{n}$ um conjunto não vazio, convexo e fechado, $z \in \RR^{n}$ e $\bar{z} = \proj_{S} (z)$. Então, 
\[
(z - \bar{z})^{T}(x - \bar{z}) \leq 0 ,
\]
para todo $x \in S$.
\end{teo}
\begin{proof}
Sejam $x \in S$ um ponto arbitrário e $\bar{z} = \proj_{S} (z)$. Pelo \Cref{lema:existencia_projecao} $\bar{z} \in S$ e, dado $t \in (0,1)$, pela convexidade de $S$, temos que $(1-t)\bar{z} + tx \in S$. Assim, 
\[
\Vert z-\bar{z} \Vert \leq \Vert z-(1-t)\bar{z} - tx \Vert = \Vert (z-\bar{z}) - t(x-\bar{z}) \Vert.
\]
Então, 
\[
\Vert z-\bar{z} \Vert^{2} \leq \Vert (z-\bar{z}) - t(x-\bar{z}) \Vert^{2} = \Vert z-\bar{z} \Vert^{2} - 2t(z-\bar{z})^{T}(x-\bar{z}) + t^{2}\Vert x- \bar{z} \Vert^{2} ,
\]
e como $t>0$, temos
\[ \label{eq:1_lema_condicaoprojecao_convexidade}
2(z-\bar{z})^{T}(x-\bar{z}) \leq t \Vert x- \bar{z} \Vert^{2} .
\]
Passando o limite em \eqref{eq:1_lema_condicaoprojecao_convexidade} quando $t\rightarrow 0$, obtemos
\[
(z-\bar{z})^{T}(x-\bar{z}) \leq 0, 
\]
completando a demonstração.
\end{proof}


O \Cref{lema_condicaoprojecao_convexidade} estabelece uma condição necessária e suficiente para caracterizar a projeção. Este resultado é provado no \Cref{lema:defineprojecao_convexidade}.


\begin{lema} \label{lema:defineprojecao_convexidade}
Sejam $S \subset \RR^{n}$ um conjunto não vazio, convexo e fechado e $z \in \RR^{n}$. Se $\bar{z} \in S$ satisfaz
\[
(z - \bar{z})^{T}(x - \bar{z}) \leq 0 ,
\]
para todo $x \in S$, então $\bar{z} = \proj_{S} (z)$.
\end{lema}
\begin{proof}
Dado $x\in S$ arbitrário, temos
\begin{align}
\Vert z-\bar{z} \Vert^{2} - \Vert z-x \Vert^{2} & = z^{T}z - 2z^{T}\bar{z} + \bar{z}^{T}\bar{z} - z^{T}z + 2z^{T}x - x^{T}x \\
& = - 2z^{T}\bar{z} + \bar{z}^{T}\bar{z} + 2z^{T}x - x^{T}x \\
& = (x-\bar{z})^{T}(2z-x-\bar{z}) \\
& = (x-\bar{z})^{T}(2(z-\bar{z})-(x-\bar{z})) \\
& = 2(x-\bar{z})^{T}(z-\bar{z}) - (x-\bar{z})^{T}(x-\bar{z}) \\
& \leq 0 .
\end{align}
pois $(x-\bar{z})^{T}(z-\bar{z}) \leq 0$ por hipótese, e $(x-\bar{z})^{T}(x-\bar{z}) = \Vert (x-\bar{z}) \Vert \geq 0$.
Logo,
\[
\Vert z-\bar{z} \Vert^{2} - \Vert z-x \Vert^{2} \leq 0,
\]
e, então
\[
\Vert z-\bar{z} \Vert^{2} \leq \Vert z-x \Vert^{2} ,
\]
para todo $x\in S$. Portanto, $\bar{z} = \proj_{S} (z)$.
\end{proof}


Uma aplicação do \Cref{lema:defineprojecao_convexidade} relacionada à otimização é que o mesmo fornece uma condição necessária de otimalidade para minimizadores de uma função diferenciável restrita a um conjunto convexo fechado.


\begin{lema} \label{lema:associa_projecao_minimizador}
Sejam $f: \RR^{n} \rightarrow \RR$ uma função diferenciável e $C \subset \RR^{n}$ um conjunto convexo e fechado. Se $x^{*} \in C$ é minimizador local de $f$ em $C$, então
\[
\proj_{C} (x^{*} - \alpha \nabla f(x^{*})) = x^{*} ,
\]
para todo $\alpha \geq 0$.
\end{lema}
\begin{proof}
Seja $x^{*} \in C$ minimizador local de $f$ em $C$. Fixando $x\in C$, como $C$ é um conjunto convexo, temos
\[ \label{eq:1_lema_associa_projecao_minimizador}
f(x^{*}) \leq f((1-t)x^{*} + tx),
\]
para todo $t\geq 0$ suficientemente pequeno. Pelo \Cref{fato:Taylor_primeira_ordem}, 
\[ \label{eq:2_lema_associa_projecao_minimizador}
f(x^{*}+t(x-x^{*})) = f(x^{*}) + t\nabla f(x^{*})^{T}(x-x^{*}) + o(t) ,
\]
em que $\displaystyle\lim_{t\rightarrow 0} \dfrac{o(t)}{t} = 0$. Dessa forma, por \eqref{eq:1_lema_associa_projecao_minimizador} e \eqref{eq:2_lema_associa_projecao_minimizador}, temos
\[
0 \leq f(x^{*}+t(x-x^{*})) - f(x^{*}) = t\nabla f(x^{*})^{T}(x-x^{*}) + o(t) ,
\]
e portanto, 
\[ \label{eq:3_lema_associa_projecao_minimizador}
t\nabla f(x^{*})^{T}(x-x^{*}) + o(t) \geq 0.
\]
Dividindo por $t$ e passando o limite quando $t \rightarrow 0$ em \eqref{eq:3_lema_associa_projecao_minimizador}, obtemos
\[
\nabla f(x^{*})^{T}(x-x^{*}) \geq 0.
\]
Assim, dado $\alpha \geq 0$,
\[
(x^{*} - \alpha \nabla f(x^{*}) - x^{*})^{T}(x-x^{*}) = -\alpha \nabla f(x^{*})^{T}(x-x^{*}) \leq 0,
\]
para todo $x\in C$. Portanto, pelo \Cref{lema:defineprojecao_convexidade} concluímos que $\proj_{C} (x^{*} - \alpha \nabla f(x^{*})) = x^{*}$, para todo $\alpha \geq 0$.
\end{proof}


%----------------------------------------------------------------------------------------------------------------------------


\subsection{Funções Convexas}

\begin{defi} \label{defi:funcao_convexa}
Seja $C \subset \RR^{n}$ um conjunto convexo. Dizemos que a \emph{função} $f: \RR^{n} \rightarrow \RR$ é \emph{convexa} em $C$ quando
\[
f((1-t)x + ty) \leq (1-t)f(x) + tf(y),
\]
para todos $x,y \in C$ e $t \in [0,1]$.

Se para todo $t \in (0,1)$ e $x \neq y$ vale que
\[
f((1-t)x + ty) < (1-t)f(x) + tf(y),
\]
dizemos que $f$ é \emph{estritamente convexa}.
\end{defi}
Na \Cref{fig:funcao_convexa} temos um exemplo de função convexa, enquanto que na \Cref{fig:funcao_nao_convexa} podemos visualizar uma função não-convexa.

\begin{figure}[!ht] 
\centering
\begin{subfigure}[h]{0.42\textwidth}
\centering
\includegraphics[width=\textwidth]{funcao_convexa_1}
\caption{Função Convexa \label{fig:funcao_convexa}}
\end{subfigure}
\begin{subfigure}[!ht]{0.40\textwidth}
	\centering
	\includegraphics[width=\textwidth]{funcao_nao_convexa}
	\caption{Função não convexa. \label{fig:funcao_nao_convexa}}
\end{subfigure}
\caption{Funções convexas e não-convexas. \label{fig:exemplos_funcao_convexa}}
\end{figure}
Geometricamente, uma função é convexa se qualquer arco do seu gráfico está sempre abaixo do segmento que liga as extremidades. Tal noção é representada na \Cref{fig:nocao_funcao_convexa}. 

\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.73\textwidth]{nocao_geometrica_funcao_convexa}
	\caption{Noção geométrica de uma função convexa. \label{fig:nocao_funcao_convexa}}
\end{figure}

\begin{defi}
Seja $C \subset \RR^{n}$ um conjunto convexo. Uma função $f: C \rightarrow \RR $ é uma \emph{função côncava} em $C$ se a função $-f$ é convexa em $C$. A função $f$ é \emph{estritamente côncava} se $-f$ é estritamente convexa.
\end{defi}

Assim, maximizar uma função côncava num conjunto convexo equivale a minimizar uma função convexa num conjunto convexo.

Ademais, a proposição a seguir nos garante que a combinação de funções convexas também é uma função convexa. Além disso, quando as funções da restrição de um problema de otimização são convexas, o conjunto de restrições resultante é convexo.

\begin{prop} \label{prop:combinacao_funcoes_convexa}
Sejam $f$ e $g$ funções convexas no conjunto convexo $C$, $\alpha \geq 0$ e $c$ um número real qualquer. Então
\begin{enumerate}
	\item[(i)] A função $f+g$ é convexa em $C$;
	\item[(ii)] A função $\alpha f$ é convexa para qualquer $\alpha \geq 0$.
\end{enumerate}
\end{prop}
\begin{proof}
Sejam $x_{1}, x_{2} \in C$ e $t\in (0,1)$. Então
\begin{enumerate}
\item[(i)] 
\[
\begin{aligned}
(f+g)((1-t)x_{1} + tx_{2}) &= f((1-t)x_{1} + tx_{2}) + g((1-t)x_{1} + tx_{2}) \\
&\leq (1-t)f(x_{1}) + tf(x_{2}) + (1-t)g(x_{1}) + tg(x_{2}) \\
&= (1-t)[f(x_{1}) + g(x_{1})] + t[f(x_{2}) + g(x_{2})] .
\end{aligned}
\]
Portanto, $f+g$ é convexa em $C$.
\item[(ii)] 
\[
\begin{aligned}
\alpha f((1-t)x_{1} + tx_{2}) &\leq \alpha [(1-t)f(x_{1}) + tf(x_{2})] \\
&= (1-t)\alpha f(x_{1}) + t\alpha f(x_{2}) .
\end{aligned}
\]
Portanto, $\alpha f$ é convexa em $C$.
\end{enumerate}	
\end{proof}

Observe também que, se aplicarmos os itens (i) e (ii) da \Cref{prop:combinacao_funcoes_convexa} de maneira indutiva, a combinação positiva de funções convexas $\alpha_{1} f_{1} +\alpha_{2} f_{2} + \ldots +\alpha_{m} f_{m}$ também é convexa. 

Agora, vamos analisar conjuntos definidos por restrições de desigualdade, quando as funções que as definem são convexas.

\begin{prop}
Seja $g$ uma função convexa em $C$. O conjunto $S_{c} = \{ x \mid x\in C, g(x)\leq c \}$ é convexo para todo número real $c$.
\end{prop}
\begin{proof}
Sejam $x_{1}, x_{2} \in S_{c}$. Então, como $g(x_1) \leq c$ e $g(x_2) \leq c$, para todo $t\in (0,1)$, temos que
\[
g((1-t)x_{1} + tx_{2}) \leq (1-t)g(x_{1}) + tg(x_{2}) \leq (1-t)c + tc = c.
\]
Portanto, $(1-t)x_{1} + tx_{2} \in S_{c}$.
\end{proof}

Dessa forma, considerando que a intersecção de conjuntos convexos é também um conjunto convexo, concluímos que o conjunto de pontos que satisfaz simultaneamente
\[
g_{1}(x) \leq c_{1}, \ g_{2}(x) \leq c_{2}, \ \ldots, \ g_{m}(x) \leq c_{m} ,
\]
em que cada $g_{i}$ é uma função convexa, define um conjunto convexo. Esta consideração é importante, pois o conjunto de restrições dos problemas de otimização que estamos interessados em resolver são definidos dessa forma. 

Por conseguinte, quando uma função é diferenciável, a convexidade pode ser caracterizada de diferentes formas. Tais caracterizações são úteis para determinar se uma função é convexa, de modo a não depender apenas da definição usual de função convexa. O teorema a seguir apresenta outra forma de caracterizar a convexidade de uma função quando temos hipótese de diferenciabilidade.

\begin{teo}  \label{teo:diferenciabilidade_e_convexidade}
Sejam $f: \RR^{n} \rightarrow \RR $ uma função diferenciável e $C \in \RR^{n}$ um conjunto convexo. A função $f$ é convexa em $C$ se, e somente se, 
\[
f(y) \geq f(x) + \nabla f(x)^{T}(y-x),
\]
para todos $x, y \in C$.
\end{teo}

\begin{proof}
Primeiramente, suponha que $f$ é uma função convexa. Para quaisquer $x,y \in C$ e $t \in (0,1]$ temos que $(1-t)x + ty = x + t(y-x)$. Então, tomando $d = y-x$, temos que $x + td \in C$ e, pela convexidade de $f$,
\[ \label{eq:1_diferenciabilidade_e_convexidade}
f(x+td) = f((1-t)x + ty) \leq (1-t)f(x) + tf(y).
\]
Reescrevendo \eqref{eq:1_diferenciabilidade_e_convexidade}, obtemos
\[
f(x+td) \leq f(x) + t(f(y)-f(x)),
\]
que, para $t\in (0,1]$, implica em 
\[
f(y)-f(x) \geq \dfrac{f(x+td)-f(x)}{t} .
\]
Passando o limite quando $t \rightarrow 0^{+}$, temos
\[
f(y)-f(x) \geq \displaystyle\lim_{t \rightarrow 0^{+}} \dfrac{f(x+td)-f(x)}{t} = \nabla f(x)^{T} d = \nabla f(x)^{T} (y-x) ,
\]
e, portanto,
\[
f(y) \geq f(x) + \nabla f(x)^{T} (y-x) .
\]

Reciprocamente, fixe $x, y \in C$ e $t \in (0,1)$. Tomando $z = (1-t)x + ty$, observe que 
\[
f(x) \geq f(z) + \nabla f(z)^{T}(x-z) \quad \text{e} \quad f(y) \geq f(z) + \nabla f(z)^{T}(y-z) .
\]
Agora, multiplicando a primeira desigualdade por $(1-t)$, a segunda por $t$ e somando, obtemos
\begin{align}
(1-t)f(x) + tf(y) & \geq (1-t)(f(z) + \nabla f(z)^{T}(x-z)) + t(f(z) + \nabla f(z)^{T}(y-z)) \\
%& \geq f(z) + \nabla f(z)^{T}(x-z) - tf(z) -t\nabla f(z)^{T}(x-z) + tf(z) + t\nabla f(z)^{T}(y-z) \\
& \geq f(z) + \nabla f(z)^{T}(x-z) + t\nabla f(z)^{T}(-x+z+y-z) \\
& \geq f(z) + \nabla f(z)^{T}(x-z) + t\nabla f(z)^{T}(y-x) \\
& \geq f(z) + \nabla f(z)^{T}(x-z) + t\nabla f(z)^{T} \dfrac{(z-x)}{t} \\
& \geq f(z) + \nabla f(z)^{T}(x-z) - \nabla f(z)^{T}(x-z) \\
& = f(z) \\
& = f((1-t)x + ty) .
\end{align}
Portanto, a função $f$ é convexa em $C$.
\end{proof}

Uma interpretação geométrica do \Cref{teo:diferenciabilidade_e_convexidade} é que a aproximação linear através da derivada local de uma função convexa diferenciável está sempre abaixo do gráfico da função. Tal resultado está ilustrado na \Cref{fig:funcao_convexa_aproximacao_linear_derivada}. 
\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.50\textwidth]{funcao_convexa_aproximacao_linear_derivada}
	\caption{Ilustração da \Cref{teo:diferenciabilidade_e_convexidade}. \label{fig:funcao_convexa_aproximacao_linear_derivada} \\ Fonte: \textcite{luenberger2008linear}}
\end{figure}

Para funções convexas duas vezes diferenciáveis, existe outra forma de caracterizá-las que é apresentada no \Cref{teo:hessiana_convexidade}. Para a demonstração desse resultado vamos utilizar o \Cref{fato:Taylor_com_resto_lagrange}, que fornece a fórmula de Taylor com resto de Lagrange. Sua demonstração pode ser encontrada em \textcite[p.196]{Elon2019}.

\begin{fato}(\textbf{Taylor com Resto de Lagrange}) \label{fato:Taylor_com_resto_lagrange}
Considere $f: \RR^{n} \rightarrow \RR$ uma função de classe $\mathcal{C}^{1}$ e $\xbar, d \in \RR^{n}$. Se $f$ é duas vezes diferenciável no segmento $(\xbar , \xbar + d)$, então existe $t \in (0,1)$ tal que
\[{}
f(\xbar + d) = f(\xbar) + \nabla f(\xbar)^{T} d + \dfrac{1}{2}d^{T}\nabla^{2} f(\xbar + td)d .
\]
\end{fato}

\begin{lema} \label{lema:auxiliar_para_teo_hessiana_convexidade}
Sejam $C \subset \RR^{n}$ convexo, $x \in \bar{C}$ e $y \in \interior C$. Então, $(x,y] \subset \interior C$.
\end{lema}

\begin{teo} \label{teo:hessiana_convexidade}
Sejam $f:\RR^{n} \rightarrow \RR $ uma função de classe $\mathcal{C}^{2}$ e $C \subset \RR^{n}$ um conjunto convexo com interior não vazio. A função $f$ é convexa em $C$ se, e somente se, a Hessiana $\nabla^{2} f(x)$ é semidefinida positiva para todo $x \in C$.
\end{teo}
\begin{proof}
Considere $x \in \interior C$. Então, dado $d \in \RR^{n}$ temos que $x+td \in C$, para $t$ suficientemente pequeno. Portanto, pela convexidade de $f$ e pelo \Cref{teo:diferenciabilidade_e_convexidade}, temos
\[
f(x+td) \geq f(x) + t\nabla f(x)^{T} d ,
\]
e assim, 
\[ \label{eq:1_teo_hessiana_convexidade}
f(x+td) -f(x) - t\nabla f(x)^{T} d \geq 0 . 
\]
Aplicando o \Cref{fato:Taylor_segunda_ordem} para $f(x+td)$, temos
\[
f(x+td) = f(x) + t\nabla f(x)^{T}d + \dfrac{t^{2}}{2}d^{T} \nabla^{2} f(x)d + o(t^{2}) ,
\]
e substituindo em \eqref{eq:1_teo_hessiana_convexidade}, obtemos
\[
\dfrac{t^{2}}{2}d^{T} \nabla^{2} f(x)d + o(t^{2}) \geq 0 ,
\]
com $\displaystyle\lim_{t \rightarrow 0} \dfrac{o(t^{2})}{t^{2}} = 0$. 
Dividindo por $t^{2}$ e passando o limite com $t \rightarrow 0$, temos
\[
d^{T} \nabla^{2} f(x)d \geq 0 .
\]
Agora, considere $x \in C$ arbitrário. Como existe $y \in \interior C$, o \Cref{lema:auxiliar_para_teo_hessiana_convexidade} garante que todos os pontos do segmento $(x,y] \subset \interior C$. Então, pelo que acabamos de provar, dados $d \in \RR^{n}$ e $t\in (0,1]$, vale
\[
d^{T} \nabla^{2} f((1-t)x+ty)d \geq 0 .
\]
Fazendo $t \rightarrow 0^{+}$ e usando a continuidade de $\nabla^{2} f$, obtemos
\[
d^{T} \nabla^{2} f(x)d \geq 0,
\]
para todo $x \in C$.

Reciprocamente, dados $x \in C$ e $d \in \RR^{n}$ tal que $x+d \in C$, pelo \Cref{fato:Taylor_com_resto_lagrange},
\[
f(x+d) = f(x) + \nabla f(x)^{T}d + \dfrac{1}{2}d^{T} \nabla^{2} f(x+td)d,
\]
para algum $t \in (0,1)$. Como $\nabla ^{2}f(x+td) \geq 0$, concluímos que 
\[
f(x+d) \geq f(x) + \nabla f(x)^{T}d .
\]
Logo, pelo \Cref{teo:diferenciabilidade_e_convexidade}, $f$ é convexa.
\end{proof}

O teorema a seguir estabelece que no caso em que a função objetivo é convexa, então todo minimizador local é global. Esta afirmação é muito importante e justifica o fato da convexidade ser uma propriedade tão importante em otimização.

\begin{teo} \label{teo:convexidade_e_minimizador}
Sejam $C \subset \RR^{n}$ um conjunto convexo e $f:C \rightarrow \RR$ uma função convexa. Se $x^{*} \in C$ é minimizador local de $f$, então $x^{*}$ é minimizador global de $f$.
\end{teo}
\begin{proof}
Seja $x^{*}$ um minimizador local de $f$. Então, existe $\delta > 0$ tal que 
\[
f(x^{*}) \leq f(x), 
\]
para todo $x \in B(x^{*}, \delta) \cap C$. Considere $y \in C$, tal que $y \notin B(x^{*}, \delta)$, e tome $t \in (0,1]$ de modo que $t \Vert y-x^{*} \Vert < \delta$. Assim, o ponto $x = (1-t)x^{*} + ty$ satisfaz
\[
\Vert x - x^{*} \Vert = \Vert (1-t)x^{*} + ty - x^{*} \Vert = \Vert x^{*} - tx^{*} + ty - x^{*} \Vert = t \Vert y - x^{*} \Vert < \delta ,
\]
e, portanto, $x \in B(x^{*}, \delta) \cap C$.
Desse modo, como $f$ é uma função convexa, temos
\[
f(x^{*}) \leq f(x) \leq (1-t)f(x^{*}) + tf(y) = f(x^{*}) + t(f(y)-f(x^{*})),
\]
donde segue que $f(x^{*}) \leq f(y)$.
Portanto, $x^{*}$ é minimizador global de $f$.
\end{proof}

Agora, considere o problema geral de otimização dado em \eqref{problema_geral_minimizacao_igualdade_desigualdade}, em que as restrições são definidas pelas funções $g(x) = Wx - c \leq 0$ e $h(x) = Ax - b = 0$. O teorema a seguir estabelece que pontos estacionários de problemas convexos são minimizadores globais.

\begin{teo} \label{teo:pontoestacionario_minimizadorglobal_convexidade}
Seja $x^{*} \in \RR^{n}$ um ponto estacionário do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade}, no sentido da \Cref{defi:ponto_estacionario_KKT}. Sejam $f$ e $g$ funções convexas e seja $h$ uma função afim. Então, $x^{*}$ é minimizador global do problema \eqref{problema_geral_minimizacao_igualdade_desigualdade}.
\end{teo}
\begin{proof}
Sejam $\boldsymbol{\lambda}^{*} \in \RR^{m}$ e $\boldsymbol{\mu}^{*} \in \RR^{p}_{+}$ um par de multiplicadores associados ao ponto estacionário $x^{*}$. Além disso, para todo $x\in \Omega$, temos que $h(x) = 0$ e $g(x) \leq 0$. Pela convexidade de $g_{i}$, usando o \Cref{teo:diferenciabilidade_e_convexidade}, obtemos
\[
\nabla g_{i}(x^{*})^{T} (x - x^{*}) = g_{i}(x^{*}) + \nabla g_{i}(x^{*})^{T}(x - x^{*}) \leq g_{i}(x) \leq 0,
\]
para todo $i \in \mathcal{I}(x^{*})$ e $x\in \Omega$. Ademais, como $h$ é afim podemos escrever $h_{i}(x) = a_{i}^{T}x - b$, e disso segue que
\[
\nabla h_{i}(x^{*})^{T}(x - x^{*}) = v_{i}^{T}(x - x^{*}) = (v_{i}^{T}x - b) - (v_{i}^{T}x^{*} - b) = 0,
\]
para todo $i = 1, \ldots , m$ e $x \in \Omega$. Como $x^{*}$ é um ponto estacionário,
\[
\nabla f(x^{*}) = - \sum_{i\in \mathcal{I}(x^{*})} \mu_{i}^{*} \nabla g_{i}(x^{*}) - \sum_{i=1}^{m} \lambda_{i}^{*} \nabla h_{i}(x^{*}) ,
\]
e fazendo o produto interno por $(x - x^{*})$ em ambos os lados da igualdade, obtemos
\[
\nabla f(x^{*})^{T} (x - x^{*}) = - \sum_{i\in \mathcal{I}(x^{*})} \mu_{i}^{*} \nabla g_{i}(x^{*})^{T} (x - x^{*}) - \sum_{i=1}^{m} \lambda_{i}^{*} \nabla h_{i}(x^{*})^{T} (x - x^{*}) \geq 0.
\]
Desse modo, pela convexidade de $f$ e usando o \Cref{teo:diferenciabilidade_e_convexidade}, temos que
\[
f(x^{*}) \leq f(x^{*}) + \nabla f(x^{*})^{T} (x - x^{*}) \leq f(x),
\]
e portanto,
\[
f(x^{*}) \leq f(x),
\]
para todo $x \in \Omega$.
\end{proof}

O \Cref{teo:pontoestacionario_minimizadorglobal_convexidade} se constitui num resultado de grande relevância, pois no caso em que o problema de otimização é convexo, isto é, a função objetivo é convexa e o conjunto factível determinado pelas restrições é convexo, então as condições de KKT são necessárias e suficientes para $x^{*}$ ser minimizador global.


\subsection{Convexidade dos problemas de SVM}

Tendo em vista que a função objetivo dos problemas de SVM são funções quadráticas, é necessário avaliar sob quais condições uma função quadrática é convexa. Tal resultado é apresentado a seguir e sua demonstração decorre do \Cref{teo:hessiana_convexidade}.

\begin{teo} \label{teo:funcao_quadratica_e_convexa}
Seja $C \in \RR^{n}$ um conjunto convexo e $Q \in \RR^{n\times n}$ uma matriz quadrada e simétrica. Seja $f:C \rightarrow \RR$ tal que $f(x) = \dfrac{1}{2}x^{T}Qx$ é uma função quadrática. Então, $f$ é convexa se, e somente se, $Q$ é semidefinida positiva.
\end{teo}
\begin{proof}
Primeiramente, suponhamos que $f$ é convexa em $C$. Então, pelo \Cref{teo:hessiana_convexidade}, a Hessiana $\nabla^{2} f(x)$ é semidefinida positiva para todo $x\in C$, e como $\nabla^{2} f(x) = Q$, concluímos que $Q$ é semidefinida positiva. Reciprocamente, se $\nabla^{2} f(x) = Q$ é semidefinida positiva, então, pelo \Cref{teo:hessiana_convexidade}, a função $f$ é convexa em $C$.
\end{proof}

Consequentemente, nos problemas de minimização irrestrita em que a função objetivo é uma quadrática da forma
\[
f(x) = \dfrac{1}{2}x^{T}Qx - b^{T}x,
\]
com $Q\in \RR^{n\times n}$ simétrica e $b\in \RR^{n}$, 
temos que se $x^{*}$ é mínimo local de $f$ então, pelas condições necessárias de otimalidade, \Cref{teo:condicao_necessaria_1_ordem,teo:condicao_necessaria_2_ordem}, devemos ter
\[
\nabla f(x^{*})= Qx^{*}-b =0 \quad \text{e} \quad \nabla^{2} f(x^{*})=Q \succcurlyeq 0.
\]
Logo, se $Q$ é semidefinida positiva, pelo \Cref{teo:funcao_quadratica_e_convexa} $f$ é convexa, e pelo \Cref{teo:convexidade_e_minimizador} qualquer ponto $x^{*}$ que satisfaça a condição de otimalidade de primeira ordem 
\[ \label{eq:condicao_1_ordem_quadratica_convexidade}
\nabla f(x^{*})= Qx^{*}-b =0
\] 
é um minimizador global de $f$. Por outro lado, cabe ressaltar que pode não existir solução para a equação \eqref{eq:condicao_1_ordem_quadratica_convexidade} se $Q$ é singular. No entanto, temos que se $Q$ é definida positiva então $Q$ é invertível, e resolvendo a equação \eqref{eq:condicao_1_ordem_quadratica_convexidade} o vetor
\[
x^{*} = Q^{-1}b
\]  
é o único mínimo global.

Portanto, apresentamos neste capítulo alguns conceitos de otimização convexa que serão importantes para a formulação matemática da técnica SVM, que será abordada no próximo capítulo.

%lém de quadrática, a função objetivo do problema de classificação \eqref{eq:problemageralSVM} envolve a norma de um vetor $w \in \RR^{n}$. Em vista disso, a proposição a seguir garante a convexidade da norma.

%\begin{prop} \label{prop:norma_funcao_convexa}
%Considere a função $f: \RR^{n} \rightarrow \RR$ dada por
%\[
%f(x) = \dfrac{1}{2}\Vert x \Vert,
%\]
%com $x\in \RR^{n}$. Então a função $f$ é uma função convexa.
%\end{prop}
%\begin{proof}
%Seja $\Vert \cdot \Vert$ a norma de um vetor. Para quaisquer $x, y \in \RR^{n}$ e qualquer $\alpha \in [0,1]$, temos que
%\begin{align}
%f(\alpha x + (1-\alpha)y) &= \dfrac{1}{2} \Vert \alpha x + (1-\alpha)y \Vert \\
%&\leq \dfrac{1}{2} \Vert \alpha x\Vert + \dfrac{1}{2}\Vert (1-\alpha)y \Vert \\
%&= \alpha \dfrac{1}{2} \Vert x \Vert + (1-\alpha)\dfrac{1}{2} \Vert y \Vert \\
%&= \alpha f(x) + (1-\alpha)f(y).
%\end{align}
%Portanto, $f$ é uma função convexa.
%\end{proof}


\newpage
\section{Máquinas de Vetores Suporte} \label{chap:maquinas_vetores_suporte}

A Aprendizagem de Máquina desempenha um papel importante dentro do campo de reconhecimento de padrões, o qual, de acordo com \textcite{bishop2016pattern},
\begin{quote}
    (...) se preocupa com a descoberta automática de regularidades em dados através do uso de algoritmos de computador e com o uso destas regularidades para executar ações, como classificar dados em diferentes categorias.
    \end{quote}
Assim, em problemas que exigem a análise de uma grande quantidade de dados para classificá-los, um processo manual torna-se inviável, motivando o desenvolvimento de técnicas computacionais capazes de reconhecer padrões para desempenhar tal tarefa. Neste capítulo desenvolveremos a modelagem matemática de uma técnica de aprendizagem supervisionada aplicada ao problema de classificação de dados, as Máquinas de Vetores Suporte (SVM, do inglês \textit{Support Vector Machine}). 

No primeiro momento discutiremos a formulação matemática de tal técnica para a classificação de dados linearmente separáveis, característica que qualifica a SVM com margem rígida. Posteriormente, estenderemos tal formulação para o caso em que os dados não são linearmente separáveis, obtendo assim a modelagem do problema de classificação com margem flexível.

Para o desenvolvimento deste capítulo, as principais bibliografias utilizadas foram \textcite{Faisal2019,Evelin2017}


\subsection{Conceitos Básicos de Aprendizagem de Máquina}

Nas palavras de \textcite[p. 11]{Faisal2019}, a área da Aprendizagem de Máquina está interessada em 
\begin{quote}
(...) projetar algoritmos que extraem automaticamente informações valiosas dos dados. A ênfase aqui está em ``automático", \emph{i.e.}, a aprendizagem de máquina está preocupada com metodologias de uso geral que possam ser aplicadas em muitos conjuntos de dados, enquanto produz algo significativo (tradução nossa). 
\end{quote}
O aprendizado da máquina pode ser comparado ao do cérebro humano, pois os algoritmos de aprendizagem de máquina buscam "aprender" com a experiência, o que ocorre através do reconhecimento de padrões em dados \cite{Evelin2017}. 

Considere o exemplo de reconhecimento de dígitos numéricos escritos à mão. Suponha que desejamos construir um algoritmo capaz de diferenciá-los dentre $0, 1, ...,9$. Perceba que neste caso, primeiramente é preciso fornecer à máquina imagens dos diferentes dígitos com suas respectivas classificações, isto é, informando quais dígitos são $0$, quais são $1$ e assim sucessivamente. A partir disso, esta técnica detecta padrões entre as características de cada dígito e sua classificação, e cria um modelo para deduzir a classificação de novos dígitos. Este é um exemplo de \emph{aprendizado supervisionado}, em que através de um conjunto de dados de entrada fornecidos na forma (entrada, saída desejada), a máquina detecta padrões e produz um modelo capaz de deduzir as saídas corretas para novos dados \cite{Lorena&Carvalho}. Algumas técnicas para aprendizagem supervisionada são as Máquinas de Vetores Suporte (SVM), Regressão Linear, Regressão Logística e Redes Neurais.

A \emph{aprendizagem não-supervisionada} por sua vez é empregada em problemas que não possuem dados previamente rotulados. Em vista disso, tais técnicas são geralmente utilizadas com o intuito de auxiliar no entendimentos dos dados e obter informações acerca destes \cite{Lorena&Carvalho}. A Decomposição em Valor Singular (SVD), Clusterização e Análise de Componentes Principais \cite{Evelin2017} são exemplos de técnicas de aprendizagem não-supervisionada. 

A aprendizagem supervisionada é composta por uma etapa denominada \emph{fase de treinamento}, na qual um conjunto de treinamento que funciona como exemplo, formado por vários pares na forma $(x^{i}, y_{i})$, em que $x^{i}$ representa o vetor de características (ou atributos) e $y_{i}$ corresponde à saída (ou rótulo), são fornecidos e a partir do qual a máquina detecta padrões e cria um modelo para deduzir a saída de novos dados não rotulados. Após este processo ocorre a \emph{fase de teste}, em que novas entradas serão testadas, denominadas conjunto de teste, no intuito de analisar se a máquina está gerando as saídas corretas. Na aprendizagem supervisionada, assim como o nome já salienta, há a presença de um supervisor externo, o qual será responsável por fornecer à máquina os dados devidamente rotulados e averiguar se ela está gerando as saídas corretas.

Em muitas situações a aprendizagem supervisionada é aplicada a problemas cujo objetivo é obter uma classificação dos dados. Alguns exemplos são a detecção de \emph{spam} em e-mails, o reconhecimento facial e o reconhecimento de dígitos escritos à mão. Em todos estes casos a técnica SVM pode ser aplicada, pois além de apresentar bons resultados quando comparadas a outras técnicas de classificação, ela também  possui uma boa capacidade de generalização e é indicada nos casos em que ocorrem dados de dimensões elevadas e com altos níveis de ruídos. 

Cada dado (atributo) será representado por um vetor de características $x^{i}$ pertencente ao conjunto de entrada e a saída será representada por $y_{i} \in \{ -1,1 \}$, de modo que diremos que $x_{i}$ pertence à classe positiva se $y_{i} = 1$, e $x^{i}$ pertence à classe negativa se $y_{i} = -1$. Sucintamente, a técnica SVM se concentra em obter um hiperplano ótimo $(w^{*})^{T}x + b^{*} = 0$ que separe os dados de entrada $x^{i}$ em duas saídas (classes) $y_{i}$ através de uma função de decisão. Para uma melhor compreensão considere um conjunto de dados, pertencentes a duas classes distintas, conforme \Cref{fig:classificacao_dados}.

\begin{figure}[!ht] 
\centering
\begin{subfigure}[h]{0.3\textwidth}
\centering
\includegraphics[width=\textwidth]{hiperplano_SVM_linear}
\caption{Linear. \label{fig:classificacao_dados:hiperplano_SVM_linear}}
\end{subfigure}
\begin{subfigure}[!ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hiperplano_SVM_flexivel}
	\caption{Flexível. \label{fig:classificacao_dados:hiperplano_SVM_flexivel}}
\end{subfigure}
\begin{subfigure}[!ht]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{hiperplano_SVM_nao_linear}
	\caption{Não Linear. \label{fig:classificacao_dados:hiperplano_SVM_nao_linear}}
\end{subfigure}
\caption{Dados lineares, com margem flexível e não lineares. \label{fig:classificacao_dados}}
\end{figure}

Observe que na \Cref{fig:classificacao_dados:hiperplano_SVM_linear} os dados podem ser separados corretamente através de um hiperplano, que em $\RR^{2}$ é uma reta. Já na \Cref{fig:classificacao_dados:hiperplano_SVM_flexivel} é possível encontrar uma reta que separa alguns poucos dados, porém incorretamente. E na \Cref{fig:classificacao_dados:hiperplano_SVM_nao_linear} não é mais possível separar os dados como nos casos anteriores. Nestes exemplos temos representados os três casos de SVM: o linear com margem rígida, o linear com margem flexível e o não linear, respectivamente.

Assim, mostraremos que a modelagem matemática do problema de classificação resulta num problema de programação quadrática, convexa e com restrições lineares, em que o objetivo é encontrar o hiperplano de máxima margem, isto é, que maximiza a distância entre os hiperplanos $w^{T}x+b = 1$ e $w^{T}x+b = -1$ que definem a faixa que separa os dados \cite{Evelin2017}. 

É importante salientar que neste trabalho trataremos unicamente do problema de classificação binária, isto é, quando existem apenas duas saídas possíveis $\{-1,1\}$. No entanto é possível generalizar para problemas que envolvam mais saídas, se configurando neste caso num problema multi-classes. 

Considere, por exemplo, o problema de detecção de câncer, em que desejamos identificar se um câncer é maligno ou benigno. Neste caso, a cada célula será associado um vetor de características $x^{i} \in \RR^{n}$ que contém $n$ informações relevantes observadas, dentre as quais, por exemplo, medidas de raio, perímetro, área e textura. As células classificadas como sendo do tipo câncer maligno teriam saída $y_{i} = 1$, enquanto que células que possuem saída $y_{i} = -1$, seriam classificadas como do tipo câncer benigno. 

Problemas de classificação que envolvem situações reais dificilmente apresentam um conjunto de dados linearmente separáveis. Neste contexto é necessário generalizar os resultados obtidos na modelagem da SVM com margem rígida para o caso não linear. Em vista disso, estudaremos também a SVM com margem flexível, em que os dados não são linearmente separáveis e para garantir a classificação correta através de um hiperplano no espaço de característica utilizamos regularização, introduzindo variáveis de folga $\xi_{i}$ às restrições. No intuito de evitar classificações incorretas, tais variáveis são penalizadas na função objetivo através de um parâmetro $C>0$, por isso a técnica é também denominada CSVM.

O nome da técnica SVM se refere aos vetores do conjunto de treinamento que estão sobre os hiperplanos que determinam a margem ótima. Tais vetores são os que apresentam maior relevância na hora de determinar o hiperplano ótimo, de modo que os demais vetores possam ser descartados sem interferir na determinação do classificador. 

A seguir, desenvolveremos a modelagem matemática da SVM com margem rígida e posteriormente sua generalização para margem flexível.

%A Aprendizagem de Máquina (AM, do inglês \textit{Machine Learning}) é o estudo do uso de técnicas computacionais para automaticamente detectar padrões em dados e usá-los para fazer predições e tomar decisões.  

%Neste trabalho nosso objetivo será realizar um estudo teórico matemático, do ponto de vista da Otimização, de uma técnica de aprendizagem supervisionada: as Máquinas de Vetores Suporte (SVM, do inglês \textit{Support Vector Machine}). Esta técnica é fundamentada na Teoria de Aprendizagem Estatística e foi desenvolvida por Vladimir Vapnik, Bernhard Boser, Isabelle Guyon e Corrina Cortes \cite{Vladimir1992,Vladimir1995}. Conforme destacado por \textcite{Evelin2017}, as SVM são amplamente empregadas em problemas de regressão e de classificação por possuírem um embasamento teórico bem consolidado e por apresentarem uma boa capacidade de generalização, sendo indicadas nos casos em que ocorrem dados de dimensões elevadas e com altos níveis de ruídos. Algumas aplicações de SVM em problemas práticos são o reconhecimento facial, leitura de placas automotivas e detecção de spam.

%Nossa intenção é estudar a técnica SVM aplicada na classificação de dados. Neste caso, na fase de treinamento é fornecido um conjunto de dados na forma $(x^{i}, y_{i})$, em que $x^{i}$ representa o vetor de características (ou atributos) e $y_{i}$ corresponde à saída ou rótulo, e a partir dos quais a técnica SVM irá gerar um classificador capaz de predizer a saída correta de novos dados.  Na \Cref{fig:SVM_classificador} a seguir temos uma representação simplificada da atuação de técnicas de AM, como a técnica SVM por exemplo, em problemas de classificação.

%\begin{figure}[!ht] %refazer figura, resolução não está boa.
%	\centering
%	\includegraphics[width=0.70\textwidth]{representacao_tecnica_SVM}
%	\caption{Atuação da técnica SVM. \\ Fonte: \textcite{Lorena&Carvalho} \label{fig:SVM_classificador}}
%\end{figure}

%Na figura a seguir, por exemplo, temos representado um conjunto de dados pertencentes a duas classes distintas: azul ou laranja. Assim reta em vermelho é o hiperplano separador, o qual fará a classificação de novos dados em azul se estiverem a direita do hiperplano e em laranja se estiverem a sua esquerda. Neste exemplo perceba que o hiperplano consegue separar corretamente todos os dados nas duas classes e portanto o denominamos de SVM linear de margem rígida. No entanto, em problemas que envolvam situações reais os dados costumam ser mais "embaralhados" como apresentado na figura a seguir. Nestes exemplos não é mais possível separar corretamente os dados através de um hiperplano, sendo necessária a moelagem de SVM com margem flexível e SVM não linear. Portanto, a depender da conjunto de dados a modelagem da técnica SVM pode assumir três tipos: linear com margem rígida e suas generalizações, que são margem flexível e não linear. Neste trabalho abordaremos apenas o casos linear com margem rígida e margem flexível. 



\subsection{Máquinas de Vetores Suporte - Margem Rígida} \label{subsection:SVM_margem_rigida}

Nesta seção formularemos matematicamente a técnica SVM para a classificação de dados linearmente separáveis, que caracteriza o problema com margem rígida. Para tanto, abordaremos alguns conceitos importantes para a modelagem do problema, como a definição de hiperplano, conjuntos linearmente separáveis, além de discutir a noção de margem. Veremos também que a modelagem de tal problema recai num problema de otimização. 

Considere um conjunto de dados de treinamento representado no $\RR^2$ como na \Cref{fig:dados_e_hiperplanos:dados_treinamento}, em que os pontos em azul representam a classe positiva, e os pontos em laranja a classe negativa. Para obter a classificação desse conjunto em duas classes, azul ou laranja, a técnica SVM se concentra em determinar o hiperplano que melhor separa os dados corretamente. No entanto, para dados linearmente separáveis podem existir infinitos hiperplanos que separam corretamente os dados de treinamento; veja \Cref{fig:dados_e_hiperplanos:hiperplanos_separadores}. Assim, a escolha do hiperplano ideal se baseia no conceito de distância entre os pontos que representam os dados e o hiperplano. Isto é, o \emph{hiperplano ótimo}, como será denominado o hiperplano que melhor separa os dados, será aquele que possibilita a maior faixa entre os dados positivos e negativos. Tal hiperplano é representado na \Cref{fig:dados_e_hiperplano_otimo:hiperplano_otimo} pela cor vermelha. Em outras palavras, estamos interessados em encontrar o hiperplano que maximiza a distância entre ele e os dados do conjunto de treinamento mais próximos. Note que, caso a faixa seja muito estreita pequenas perturbações no hiperplano ou no conjunto de dados podem resultar uma classificação incorreta. 

\begin{figure}[htbp] 
	\centering
	\begin{subfigure}[h]{0.38\textwidth}
		\centering
		\includegraphics[width=\textwidth]{dados_treinamento}
		\caption{Dados de treinamento. \label{fig:dados_e_hiperplanos:dados_treinamento}}
	\end{subfigure}
	\begin{subfigure}[h]{0.38\textwidth}
		\centering
		\includegraphics[width=\textwidth]{hiperplanos_separadores}
		\caption{Hiperplanos separadores. \label{fig:dados_e_hiperplanos:hiperplanos_separadores}}
	\end{subfigure}
\caption{Conjunto de Dados e Hiperplanos. \label{fig:dados_e_hiperplanos}}
\end{figure}
\begin{figure}[hbtp] 
	\centering
	\begin{subfigure}[h]{0.38\textwidth}
		\centering
		\includegraphics[width=\textwidth]{hiperplano_otimo}
		\caption{Hiperplano ótimo. \label{fig:dados_e_hiperplano_otimo:hiperplano_otimo}}
	\end{subfigure}
	\begin{subfigure}[h]{0.40\textwidth}
		\centering
		\includegraphics[width=\textwidth]{hiperplano_maxima_margem}
		\caption{Máxima margem. \label{fig:dados_e_hiperplano_otimo:hiperplano_maxima_margem}}	
	\end{subfigure}
\caption{Hiperplano Ótimo. \label{fig:dados_e_hiperplano_otimo}}
\end{figure}

Inicialmente, para formular matematicamente o problema de classificação, é preciso estabelecer o conjunto de treinamento. Assim, considere o conjunto de treinamento $\Xset=\{(x^1, y_1), \ldots , (x^m, y_m)\mid x^i \in \RR^{n} \, e \, y_i \in \{-1,1\}\}$, formado por pares de entrada $x^{i}$ e saída $y_{i}$, com a seguinte partição 
\[ \label{conj1}
\Xset ^{+} =\{x^i \in \Xset\mid y_i=1\} \quad e \quad \Xset^{-}=\{x^i \in \Xset\mid y_i=-1\}.
\]
Diremos que $\Xset ^{+}$ e $\Xset^{-}$ são os conjuntos formados pelos atributos pertencentes às classes positiva e negativa, respectivamente.

\begin{defi} Considere um vetor não nulo $w\in \RR^n$ e um escalar $b\in \RR$. Um \emph{hiperplano} com vetor normal $w$ e constante $b$ é um conjunto da forma $\Hset(w,b)=\{x\in \RR^n \mid w^{T}x+b=0\}$.
\end{defi}

O hiperplano $\Hset(w,b)$ divide o espaço $\RR^n$ em dois semiespaços, dados por
\[ \label{conj2}
\mathcal{S}^{+}=\{x\in \RR^n \mid w^{T}x+b\geq 0\} \quad e \quad \mathcal{S}^{-}=\{x\in \RR^n \mid w^{T}x+b\leq 0\}.
\]


\begin{defi} \label{def:dados_linearmente_separaveis} Os conjuntos $\Xset^{+}, \Xset^{-} \subset \RR^n$ são ditos \emph{linearmente separáveis} quando existem $w\in \RR^n$ e $b\in \RR$  tais que $w^{T}x+b>0$ para todo $x\in \Xset^{+}$ e $w^{T}x+b<0$ para todo $x\in \Xset^{-}$. O hiperplano $\Hset(w,b)$ é chamado hiperplano separador dos conjuntos $\Xset^{+}$ e $\Xset^{-}$.
\end{defi}

\begin{lema} \label{lema:hiperplanos_separadores} 
Suponha que os conjuntos $\Xset^{+}, \Xset^{-} \subset \RR^n$ são finitos e linearmente separáveis, com hiperplano separador $\Hset(w,b)$. Então, existem $\overline{w}\in \RR^n$ e $\overline{b}\in \RR$ tais que $\Hset(w,b)$ pode ser descrito por
\[
\wbar^{T}x+\bbar =0,
\]
satisfazendo
\begin{align}
\wbar^{T}x+\bbar &\geq 1, \text{ para todo } x\in \Xset^{+}, \label{eq:1_hiperplanos_separadores} \\
\wbar^{T}x+\bbar &\leq -1, \text{ para todo } x\in \Xset^{-}. \label{eq:2_hiperplanos_separadores}
\end{align}
\end{lema} 

\begin{proof}
Pela  \Cref{def:dados_linearmente_separaveis}, temos que existem $w\in \RR^n$ e $b\in \RR$ tais que
\begin{align}
w^{T}x+b &>0, \text{ para todo } x\in \Xset^{+}, \\
w^{T}x+b &<0, \text{ para todo } x\in \Xset^{-}.
\end{align}  
Como $\Xset^{+}\cup \Xset^{-}$ é um conjunto finito, podemos definir
\[ \boldsymbol{\gamma} \coloneqq \min_{x\in \Xset^{+}\cup \Xset^{-}} \vert w^{T}x+b\vert  >0. \]
Portanto, para todo $x\in \Xset^{+}\cup \Xset^{-}$, $\boldsymbol{\gamma} \leq \vert w^{T}x+b\vert$ e consequentemente, $\dfrac{\vert w^{T}x+b\vert }{\boldsymbol{\gamma}} \geq 1$. Assim, para $x\in \Xset^{+}$ temos
\[ \dfrac{w^{T}x+b}{\boldsymbol{\gamma}} = \dfrac{\vert w^{T}x+b\vert }{\boldsymbol{\gamma}} \geq 1, \]
e para $x\in \Xset^{-}$, temos
\[- \dfrac{w^{T}x+b}{\boldsymbol{\gamma}} = \dfrac{\vert w^{T}x+b\vert }{\boldsymbol{\gamma}} \geq 1. \]
Logo, definindo $\wbar \coloneqq \dfrac{w}{\boldsymbol{\gamma}}$ e $\bbar \coloneqq \dfrac{b}{\boldsymbol{\gamma}}$, obtemos as desigualdades \eqref{eq:1_hiperplanos_separadores} e \eqref{eq:2_hiperplanos_separadores}. 

\end{proof}


Sem perda de generalidade, a partir do \Cref{lema:hiperplanos_separadores} podemos considerar o hiperplano $\Hset (w,b)$ com $w$ e $b$ satisfazendo \eqref{eq:1_hiperplanos_separadores} e \eqref{eq:2_hiperplanos_separadores}. Logo, temos que $\Hset^{+} (w,b) \coloneqq \{x\in \RR^n \mid w^{T}x+b= 1\}$ e $\Hset^{-} (w,b) \coloneqq \{x\in \RR^n \mid w^{T}x+b= -1\}$ são os hiperplanos que definem a faixa que separa os conjuntos $\Xset^{+}$ e $\Xset^{-}$.

Como mencionado anteriormente, estamos interessados em obter o hiperplano que maximiza a margem entre os dados das classes positiva e negativa. Para tanto, precisamos calcular a distância entre um vetor $x^{i}$ e o hiperplano $\Hset(w,b)$, para a partir disso obter a medida da margem. Tal distância será dada pela projeção ortogonal. Os dois resultados a seguir estabelecem, respectivamente, a projeção de um ponto $x \in \RR^{n}$ qualquer sobre um hiperplano e a distância entre os hiperplanos $\Hset^{+}$ e $\Hset^{-}$.

\begin{prop} \label{prop:projecao_ortogonal_sobre_hiperplano} 
A projeção ortogonal de um vetor $\xbar\in \RR^n$ sobre um hiperplano afim $\Hset(w,b)$, é dada por
\[ \proj_{\Hset (w,b)}(\xbar)= \xbar - \dfrac{w^{T}\xbar+b}{w^{T}w}w. \]
Além disso, a $\proj_{\Hset (w,b)}(\xbar)$ satisfaz a menor distância.
\end{prop}

\begin{proof}
Sejam $w\in \RR^n$ o vetor normal ao hiperplano $\Hset(w,b)$, $\bar{z}\in \Hset(w,b)$ e $x^{*} \coloneqq \proj_{\Hset (w,b)}(\xbar)$ a projeção ortogonal de $\xbar$ sobre $\Hset(w,b)$. Assim, temos que 
\[ \label{eq:1_projecao_ortogonal_sobre_hiperplano} 
w^{T}(x^{*}-\bar{z})=0 
\]
e
\[ \label{eq:2_projecao_ortogonal_sobre_hiperplano} 
\xbar-x^{*}=\lambda w \Longrightarrow x^{*}=\xbar-\lambda w. 
\]
Substituindo \eqref{eq:2_projecao_ortogonal_sobre_hiperplano} em \eqref{eq:1_projecao_ortogonal_sobre_hiperplano}, obtemos
\begin{align}
0 &= w^{T}(\xbar-\lambda w-\overline{z}) \\
&= w^{T}\xbar-\lambda w^{T}w - w^{T}\bar{z}.
\end{align}
Resolvendo para $\lambda$ e como $w^{T}\bar{z} = -b$, temos
\[ \lambda =\dfrac{w^{T}\xbar-w^{T}\bar{z}}{w^{T}w} =\dfrac{w^{T}\xbar+b}{w^{T}w}. \]
Portanto, 
\[ x^{*}=\xbar-\dfrac{w^{T}\xbar+b}{w^{T}w}w . \]
Ademais, vamos provar que a $\proj_{\Hset (w,b)}(\xbar)$ satisfaz a menor distância, isto é,
\[ \Vert\xbar-x^{*}\Vert_{2} \leq \Vert \xbar-x\Vert_{2}, \]
para todo $x\in \Hset(w,b)$.
De fato, tomando $u=\xbar-x^{*}$ e $v=x^{*}-x$ observe que estes vetores são ortogonais, pois 
\begin{align}
u^{T}v&= (\xbar-x^{*})^{T}(x^{*}-x) \\
&= (\xbar-\xbar+\lambda w)^{T}(x^{*}-x) \\
&= \lambda w^{T}(x^{*}-x) \\
&= \lambda (w^{T}x^{*}-w^{T}x) \\
&= \lambda (-b-(-b)) \\
&= 0.
\end{align}
Assim, podemos utilizar Pitágoras, isto é, obtemos
\[ \Vert u+v\Vert^{2}=\Vert u\Vert^{2} + 2u^{T}v + \Vert v\Vert^{2}=\Vert u\Vert^{2} + \Vert v\Vert^{2} , \]
e utilizamos as definições de $u$ e $v$ para obter
\[ 
\Vert \xbar-x\Vert^{2}=\Vert \xbar-x^{*}\Vert^{2} + \Vert x^{*}-x\Vert^{2} \geq \Vert x^{*} - x \Vert^{2} . 
\]
\end{proof}


Agora, utilizando a \Cref{prop:projecao_ortogonal_sobre_hiperplano} podemos demonstrar o \Cref{lema:distancia_entre_hiperplanos}, o qual estabelece a largura da faixa entre os hiperplanos separadores $\Hset^{+} (w,b)$ e $\Hset^{-} (w,b)$.

\begin{lema} \label{lema:distancia_entre_hiperplanos} 
A distância entre os hiperplanos $\Hset^{+} (w,b)$ e $\Hset^{-} (w,b)$ é dada por 
\[
\dist(\Hset^{+}, \Hset^{-})=\dfrac{2}{\Vert w\Vert}.
\] 
\end{lema}
\begin{proof}
Considere um ponto arbitrário $\xbar\in \Hset^{+} (w,b)$ e seja $x^{*}\in \Hset^{-} (w,b)$ a projeção ortogonal de $\xbar$ sobre $\Hset^{-} (w,b)$. Usando a \Cref{prop:projecao_ortogonal_sobre_hiperplano}, temos
\[ \label{eq:1_distancia_entre_hiperplanos} 
x^{*}= \proj_{\Hset^{-} (w,b)}(\xbar)= \xbar - \dfrac{w^{T}\xbar+b+1}{\Vert w\Vert^{2}}w. 
\] 
Além disso, a distância entre dois conjuntos é definida por
\[ 
\dist(\Hset^{+}, \Hset^{-}) \coloneqq  \inf\{\Vert x^{+}-x^{-} \Vert : x^{+}\in \Hset^{+} (w,b)\ \text{e} \ x^{-}\in \Hset^{-} (w,b) \},
\]
e como a $\proj_{\Hset^{-} (w,b)}(\xbar)$ satisfaz a menor distância entre $\xbar$ e $\Hset^{-} (w,b)$ e, $\Hset^{+} (w,b)$ é paralelo a $\Hset^{-} (w,b)$, temos que 
\[ \label{eq:2_distancia_entre_hiperplanos} 
\dist(\Hset^{+},\Hset^{-})=\Vert \xbar-x^{*}\Vert. 
\]
Substituindo \eqref{eq:1_distancia_entre_hiperplanos} em \eqref{eq:2_distancia_entre_hiperplanos}, obtemos
\begin{align} 
\dist(\Hset^{+},\Hset^{-}) &= \Vert \xbar-x^{*}\Vert \\
&= \left\Vert \xbar -\xbar +\dfrac{w^{T}\xbar+b+1}{\Vert w\Vert^{2}}w \right\Vert \\
&=  \dfrac{\vert w^{T}\xbar+b+1 \vert}{\Vert w\Vert^{2}} \Vert w\Vert \\
&= \dfrac{\vert w^{T}\xbar+b+1 \vert}{\Vert w\Vert},
\end{align}
e como $\xbar\in \Hset^{+} (w,b)$,  $ w^{T}\xbar+b=1$ implica
\[  
w^{T}\xbar =1-b, 
\]
concluimos que 
\begin{align} 
\dist(\Hset^{+},\Hset^{-})&= \dfrac{\vert 1-b+b+1 \vert}{\Vert w\Vert} \\
&= \dfrac{2}{\Vert w\Vert }. 
\end{align}
\end{proof}

%\subsubsection{Formulação Matemática do Problema de Classificação - Margem Rígida}

Portanto, encontrar o hiperplano que melhor separa os dados implica maximizar a largura da margem, isto é, maximizar $\dist(\Hset^{+} , \Hset^{-}) =\dfrac{2}{\Vert w\Vert }$. Isso equivale a minimizar seu inverso $\dfrac{1}{2}\Vert w\Vert $ ou ainda minimizar $\dfrac{1}{2}\Vert w\Vert^{2}$. De fato, seja $w^{*}=\argmax\dfrac{2}{\Vert w\Vert}$. Então, para todo $w\in \RR^n$,
\[ 
\dfrac{2}{\Vert w^{*}\Vert} \geq \dfrac{2}{\Vert w\Vert} 
\]
implica
\[ \label{eq:maximizar_distancia_equivalente_minimizar} 
\Vert w\Vert \geq \Vert w^{*}\Vert. 
\]
Logo, $w^{*} \in \argmin\Vert w\Vert$. Além disso, como $\Vert \cdot \Vert$ é não negativa, elevando ao quadrado ambos os lados da desigualdade \eqref{eq:maximizar_distancia_equivalente_minimizar} temos que  $\Vert w\Vert^{2} \geq \Vert w^{*}\Vert^{2}$ implica
\[ 
\dfrac{1}{2}\Vert w\Vert^{2} \geq \dfrac{1}{2}\Vert w^{*}\Vert^{2}. 
\]
Portanto, 
\[ \argmax\dfrac{2}{\Vert w\Vert} = \argmin\dfrac{1}{2}\Vert w\Vert^2. \]

Ademais, como a faixa deve separar os dados das duas classes, as seguintes restrições devem ser satisfeitas
\begin{align}
w^{T}x+b &\geq 1 , \text{ para  todo } x\in \Xset^{+}, \\
w^{T}x+b &\leq -1 , \text{ para  todo } x\in \Xset^{-}.
\end{align}

Considerando que $\Xset^{+}=\{x^i \in \Xset\mid y_i=1\}$ e $\Xset^{-}=\{x^i \in \Xset \mid y_i=-1\}$, podemos reescrever as restrições acima de uma forma mais compacta 
\[ y_{i}(w^{T}x^{i}+b)\geq 1, \quad i=1, \ldots ,m. \]

Portanto, o problema de encontrar o hiperplano ótimo pode ser formulado da seguinte maneira
\[ \label{eq:problemageralSVM}
\begin{aligned}
\min_{w,b} & \quad \dfrac{1}{2} \Vert w\Vert^{2} \\
\text{s.a.} &  \quad y_i(w^{T}x^{i}+b) \geq 1, \quad i=1, \ldots , m, 
\end{aligned}
\]
em que $w\in \RR^{n}$ e $b\in \RR$. 

O problema \eqref{eq:problemageralSVM} possui função objetivo 
\[ f(w,b)=\dfrac{1}{2}\Vert w\Vert^{2} \]
convexa, e restrições lineares
\[ g_i(w,b)=1-y_i(w^{T}x^{i}+b) \leq 0, \quad i=1, \ldots, m, \]
em que a função $g:\RR^{n+1} \rightarrow \RR^{m}$ pode ser escrita da forma 
\[ g(w,b)= e - (YX^{T}w+by) \leq 0, \]
com $e$ sendo o vetor cujas $m$ componentes são todas iguais a $1$, $Y=\diag(y_{i})$, $X=\diag(x^{i})$, $y^{T}=[y_{1} \ \ldots \ y_{m}]$, $w\in \RR^n$ e $b\in \RR$.

Primeiramente, observe que o conjunto factível $\Omega = \{ (w,b) \in \RR^{n+1} \mid g_{i}(w,b) \leq 0, i = 1, \ldots , m \}$ é um poliedro não-vazio, pois os conjuntos $\Xset^{+}$ e $\Xset^{-}$ são linearmente separáveis. Além disso, a função objetivo é limitada inferiormente, pois $\dfrac{1}{2}\Vert w \Vert^{2} >0$. Vale ressaltar também que por se tratar de um problema convexo, o \Cref{teo:convexidade_e_minimizador} garante que todo minimizador local é global.

Agora, com base em \textcite[Teo. 2.5 e 2.7]{Evelin2017}, nosso objetivo será garantir a existência e unicidade de um minimizador para o problema \eqref{eq:problemageralSVM}.

\begin{teo} \label{teo:existencia_minimizador_problema_SVM}
O problema \eqref{eq:problemageralSVM} possui um minimizador global.
\end{teo}
\begin{proof}
Primeiramente, note que a função objetivo do problema \eqref{eq:problemageralSVM} é limitada inferiormente e o conjunto $\Omega \neq \emptyset$. Logo, podemos definir
\[
f^{*} \coloneqq \inf_{(w,b) \in \Omega} f(w,b) > -\infty .
\]
Pela definição de ínfimo, temos que para todo $k \in \mathds{N}$, existe $(w_{k},b_{k})_{k\in \mathds{N}} \subset \Omega$ tal que
\[ \label{eq:1_existencia_minimizador_problema_SVM}
\dfrac{1}{2}\Vert w_{k} \Vert^{2} = f(w_{k}, b_{k}) \rightarrow f^{*} ,
\]
e portanto, a sequência $(\Vert w_{k} \Vert )_{k\in \mathds{N}}$ é convergente. Consequentemente, como toda sequência convergente é limitada, concluímos que $(w_{k})_{k\in \mathds{N}}$ é limitada.

Mostraremos agora que $(b_{k})_{k\in \mathds{N}}$ também é limitada. Para tanto, considere $\tilde{x} \in \Xset^{+}$ e $\xbar \in \Xset^{-}$ arbitrários. Temos que 
\[
w_{k}^{T}\tilde{x} + b_{k} \geq 1
\]
e
\[
w_{k}^{T}\xbar + b_{k} \leq -1,
\]
e portanto,
\[
1-w_{k}^{T}\tilde{x} \leq b_{k} \leq -1-w_{k}^{T}\xbar .
\]
Então, como $(w_{k})$ é uma sequência limitada segue que $(b_{k})$ é limitada também. 

Como a sequência $(w_{k}, b_{k})$ é limitada, pelo Teorema de Bolzano-Weierstrass, ela possui uma subsequência convergente $(w_{k_{j}}, b_{k_{j}})_{j\in \mathds{N}}$, isto é, 
\[
(w_{k_{j}}, b_{k_{j}}) \rightarrow (w^{*}, b^{*}).
\]
Logo, pela continuidade de $f$ obtemos
\[
f(w_{k_{j}}, b_{k_{j}}) \rightarrow f(w^{*}, b^{*}),
\]
e por \eqref{eq:1_existencia_minimizador_problema_SVM} temos que $f(w^{*}, b^{*}) = f^{*}$. Portanto, existe $(w^{*}, b^{*}) \in \Omega$ tal que
\[
f(w^{*}, b^{*}) = f^{*} \leq f(w,b),
\]
para todo $(w,b) \in \Omega$.
\end{proof}

Ademais, além da existência também podemos garantir a unicidade da solução do problema \eqref{eq:problemageralSVM}. Para sua demonstração utilizaremos o lema a seguir.

\begin{lema} \label{lema:solucao_otima_SVM_margens}
Se $(w^{*}, b^{*})$ é uma solução ótima para o problema \eqref{eq:problemageralSVM} então existem $\xbar \in \Xset^{+}$ e $\tilde{x} \in \Xset^{-}$ tais que $(w^{*})^{T}\xbar + b^{*} = 1$ e $(w^{*})^{T}\tilde{x} + b^{*} = -1$.
\end{lema}
\begin{proof}
Como $(w^{*}, b^{*})$ é uma solução ótima para o problema \eqref{eq:problemageralSVM}, temos que
\[
f(w^{*}, b^{*}) \leq f(w, b),
\]
para todo $(w, b) \in \Omega$. Suponhamos que não existe $\xbar \in \Xset^{+}$ satisfazendo $(w^{*})^{T}\xbar + b^{*} = 1$. Então, 
\[
(w^{*})^{T}\xbar + b^{*} \geq 1 + \delta ,
\]
para todo $x \in \Xset^{+}$, em que $\delta \coloneqq \min_{x \in \Xset^{+}} [(w^{*})^{T}x + b^{*}] > 0$. Desse modo, 
\[
(w^{*})^{T}\xbar + b^{*} -\dfrac{\delta}{2} \geq 1 + \dfrac{\delta}{2},
\]
e dividindo ambos os lados da desigualdade por $1+\dfrac{\delta}{2}$, obtemos
\[
\dfrac{(w^{*})^{T}\xbar + b^{*} -\dfrac{\delta}{2}}{1 + \dfrac{\delta}{2}} \geq 1.
\]
Agora, tomando $\bar{w} = \dfrac{w^{*}}{1 + \dfrac{\delta}{2}}$ e $\bar{b} = \dfrac{b^{*} - \dfrac{\delta}{2}}{1 + \dfrac{\delta}{2}}$, temos
\[
\bar{w}^{T}x + \bar{b} \geq 1,
\]
para todo $x \in \Xset^{+}$.

Por conseguinte, para $x \in \Xset^{-}$ temos $(w^{*})^{T}\xbar + b^{*} \leq -1$, o que equivale a 
\[
(w^{*})^{T}\xbar + b^{*} -\dfrac{\delta}{2} \leq -1 - \dfrac{\delta}{2}.
\]
Assim, temos que $\bar{w}^{T}x + \bar{b} \leq -1$, para todo $x \in \Xset^{-}$.

Portanto, encontramos $(\bar{w}, \bar{b})$ satisfazendo as restrições do problema \eqref{eq:problemageralSVM} e 
\[
f(\bar{w}, \bar{b}) = \dfrac{1}{2}\Vert \bar{w} \Vert^{2} = \dfrac{1}{2} \dfrac{\Vert w^{*} \Vert^{2}}{\left( 1 + \dfrac{\delta}{2}\right)^{2}} < \dfrac{1}{2}\Vert w^{*} \Vert^{2} = f(w^{*}, b^{*}),
\]
contradizendo a hipótese de que $(w^{*}, b^{*})$ é ótimo.
\end{proof}

Utilizando o \Cref{lema:solucao_otima_SVM_margens}, podemos agora demonstrar a unicidade da solução do problema \eqref{eq:problemageralSVM}.

\begin{teo} \label{teo:unicidade_solucao_problema_SVM_margem_rigida}
O minimizador global para o problema \eqref{eq:problemageralSVM} é único.
\end{teo}
\begin{proof}
Suponhamos que $(\bar{w}, \bar{b})$ e $(\tilde{w}, \tilde{b})$ são soluções ótimas para o problema \eqref{eq:problemageralSVM}. Então, temos que $f(\bar{w}, \bar{b}) = f(\tilde{w}, \tilde{b})$, o que implica 
\[
\dfrac{1}{2}\Vert \bar{w} \Vert^{2} = \dfrac{1}{2}\Vert \tilde{w} \Vert^{2},
\]
e consequentemente, 
\[
\Vert \bar{w} \Vert = \Vert \tilde{w} \Vert .
\]
Como $\Omega$ é um conjunto convexo, temos que o segmento $[(1-t)(\bar{w}, \bar{b}) + t(\tilde{w}, \tilde{b})]$, tal que $t\in [0,1]$, está contido em $\Omega$. Em particular, tomando $t=\dfrac{1}{2}$,
\[ \label{eq:1_teo_unicidade_solucao_SVM_rigida}
(\hat{w}, \hat{b}) = \dfrac{1}{2}(\bar{w}, \bar{b}) + \dfrac{1}{2}(\tilde{w}, \tilde{b}) \in \Omega .
\]
Pela igualdade de vetores em \eqref{eq:1_teo_unicidade_solucao_SVM_rigida}, temos que o vetor $\hat{w} = \dfrac{1}{2}(\bar{w} + \tilde{w})$. Ademais, pelo \Cref{lema:1_convexidade} temos que, se $\Vert \bar{w} \Vert = \Vert \tilde{w} \Vert = r$, então $\Vert \dfrac{1}{2}(\bar{w} + \tilde{w}) \Vert  < r$. Portanto,
\[
\Vert \hat{w} \Vert = \Vert \dfrac{1}{2}(\bar{w} + \tilde{w}) \Vert < r = \Vert \bar{w} \Vert ,
\]
o que implica 
\[
f(\hat{w}, \hat{b}) < f(\bar{w}, \bar{b}),
\]
contradizendo o fato de $(\bar{w}, \bar{b})$ ser uma solução ótima. Portanto, temos a unicidade de $w$.

Agora, vamos mostrar a unicidade de $b$. Suponha que $(w, \bar{b})$ e $(w, \tilde{b})$ são soluções ótimas para o problema \eqref{eq:problemageralSVM} com, por exemplo, $\tilde{b} < \bar{b}$. Pelo \Cref{lema:solucao_otima_SVM_margens}, existe $x \in \Xset^{+}$ tal que $w^{T}x + \bar{b} = 1$. Por outro lado, 
\[
1 \leq w^{T}x + \tilde{b} < w^{T}x + \bar{b} = 1,
\]
o que é uma contradição.
\end{proof}

A seguir apresentamos a definição de vetores suporte, conceito que nomeia a técnica SVM, a abordamos brevemente sua relevância com base nas considerações feitas em \cite{Evelin2017,bishop2016pattern}.

\begin{defi} \label{defi:vetores_suporte}
Considere um conjunto $\Xset$ de vetores linearmente separáveis e $(w^{*}, b^{*})$ a solução do problema \eqref{eq:problemageralSVM}. Os \emph{vetores suporte} são os vetores $x^{i} \in \Xset$ tais que $y_{i}((w^{*})^{T} x^{i} + b^{*}) = 1$.
\end{defi}

Desse modo, os vetores suporte serão aqueles localizados sobre os hiperplanos que definem a máxima margem que separa os dados, ou, em outras palavras, são os vetores que correspondem às restrições ativas na solução $(w^{*}, b^{*})$ do problema \eqref{eq:problemageralSVM}. Tal propriedade detém grande importância na aplicação prática da técnica SVM, pois de acordo com o \Cref{teo:vetores_suporte} a seguir, temos que após o modelo ser treinado uma significativa quantidade de dados pode ser descartada e apenas os vetores suporte mantidos de modo que o classificador se mantém o mesmo. 

\begin{teo} \label{teo:vetores_suporte}
Considere $(w^{*}, b^{*})$ a solução do problema \eqref{eq:problemageralSVM} e $\mathcal{I}^{*} = \mathcal{I} (w^{*}, b^{*})$ o conjunto dos índices das restrições ativas na solução. Então, a solução do problema
\[ \label{eq:problema_teo_vetores_suporte}
\begin{aligned}
\min_{w,b} & \quad \dfrac{1}{2} \Vert w\Vert^{2} \\
\text{s.a.} &  \quad y_i(w^{T}x^{i}+b) \geq 1, \quad i \in \mathcal{I}^{*}
\end{aligned}
\]
é $(w^{*}, b^{*})$.
\end{teo}
\begin{proof}
Seja $(\bar{w}, \bar{b} )$ solução do problema \eqref{eq:problema_teo_vetores_suporte}. Como o conjunto factível do problema \eqref{eq:problemageralSVM} está contido no conjunto factível do problema \eqref{eq:problema_teo_vetores_suporte}, temos que
\[
f(\bar{w}, \bar{b} ) \leq f(w^{*}, b^{*} ).
\]
Agora, considere para cada $t\in (0,1)$ 
\[
(w_{t}, b_{t}) = (1-t)(w^{*}, b^{*}) + t(\bar{w}, \bar{b}) .
\]
Para $i\in \mathcal{I}^{*}$, a convexidade da restrição $g_{i}$ e a factibilidade das soluções garantem que 
\[
g_{i} (w_{t}, b_{t}) \leq (1-t)g_{i} (w^{*}, b^{*}) + tg_{i} (\bar{w}, \bar{b}) \leq 0.
\]

Por outro lado, para $i\notin \mathcal{I}^{*}$, temos que $g_{i}(w^{*}, b^{*}) <0$ e portanto, pela continuidade de $g_{i}$, vale $g_{i} (w_{t}, b_{t}) <0$ para $t$ suficientemente pequeno. Portanto, $(w_{t}, b_{t})$ é factível para o problema \eqref{eq:problemageralSVM}.

Por conseguinte, afirmamos que $f(\bar{w}, \bar{b}) = f(w^{*}, b^{*})$. Com efeito, se $f(\bar{w}, \bar{b}) < f(w^{*}, b^{*})$, então
\[
\Vert w_{t} \Vert  = \Vert (1-t) w^{*} + t\bar{w} \Vert \leq (1-t) \Vert w^{*} \Vert + \Vert \bar{w} \Vert < \Vert w^{*} \Vert ,
\]
implicando $f(w_{t}, b_{t}) < f(w^{*}, b^{*})$, o que é uma contradição. 
Concluímos então que $(w_{t}, b_{t})$ e $(w^{*}, b^{*})$ são factíveis para o problema \eqref{eq:problema_teo_vetores_suporte} com o mesmo valor ótimo da função objetivo, ou seja, ambos são minimizadores globais do problema \eqref{eq:problemageralSVM}. Pelo \Cref{teo:unicidade_solucao_problema_SVM_margem_rigida}, segue que $(\bar{w}, \bar{b}) = (w^{*}, b^{*})$.
\end{proof}

Ademais, conforme mencionado por \textcite{Evelin2017}, como somente os vetores suporte possuem um papel relevante na determinação do hiperplano ótimo, o cálculo de $w^{*}$ torna-se mais barato computacionalmente.

Portanto, nesta seção desenvolvemos a formulação matemática da técnica SVM com margem rígida para o problema de classificação. Concluímos que tal formulação resulta em um problema de otimização e demonstramos a existência de solução e sua unicidade. No entanto, como já mencionado em alguns momentos no decorrer do texto, problemas que envolvam situações reais dificilmente apresentam dados linearmente separáveis. Em vista disso, na seção a seguir trataremos da modelagem do problema com margem flexível, caso em que os dados não são linearmente separáveis.

%----------------------------------------------------------------------------------------------------------------------

\subsection{Máquinas de Vetores Suporte - Margem Flexível (CSVM)}  \label{subsection:SVM_margem_flexivel}

No caso em que os dados não são linearmente separáveis não é possível determinar pela técnica SVM com margem rígida um hiperplano no espaço de entrada que separa corretamente todos os dados do conjunto de treinamento como representado na \Cref{fig:dados_linearmente_separaveis_e_nao_separaveis:a}. Assim, como classificar um conjunto de dados como o representado na \Cref{fig:dados_linearmente_separaveis_e_nao_separaveis:b} através de um hiperplano, sendo que alguns dados poderiam estar localizados na região da margem ou até mesmo do lado errado do hiperplano? É pensando neste cotexto que estenderemos agora os conceitos desenvolvidos na modelagem da técnica SVM com margem rígida para formular a técnica SVM com margem flexível, aplicada no caso em que desejamos classificar dados que não são linearmente separáveis mas que ainda é possível encontrar um hiperplano separador no espaço de entrada ao promover uma flexibilização da margem. 

%Situações reais dificilmente envolvem problemas cujos dados são linearmente separáveis. Em vista disso, faz-se necessário estender os conceitos e resultados estudados nas SVMs lineares de margem rígida para o caso de SVM com margem flexível, quando os dados não são linearmente separáveis. Para tanto, considere um conjunto de dados não-linearmente separável como da \Cref{fig:dados_linearmente_separaveis_e_nao_separaveis:b}, isto é, não existe um hiperplano separador. 

\begin{figure}[!ht] 
	\centering
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{dados_separaveis_SVMflexivel.png}
		\caption{Dados linearmente separáveis. \label{fig:dados_linearmente_separaveis_e_nao_separaveis:a}}
	\end{subfigure}
	\begin{subfigure}[h]{0.45\textwidth}
		\centering
		\includegraphics[width=\textwidth]{dados_naoseparaveis_SVMflexivel.png}
		\caption{Dados não-linearmente separáveis. \label{fig:dados_linearmente_separaveis_e_nao_separaveis:b}}
	\end{subfigure}
\caption{Conjunto de dados. \label{fig:dados_linearmente_separaveis_e_nao_separaveis} }
\end{figure}

Considerando um conjunto de dados não-linearmente separável, como na \Cref{fig:dados_linearmente_separaveis_e_nao_separaveis:b} por exemplo, não existe um hiperplano separador como aquele estabelecido pela \Cref{def:dados_linearmente_separaveis} para conjuntos linearmente separáveis. Neste caso, temos que o conjunto factível
\[
\Omega = \{ (w,b) \in \RR^{n+1} \mid 1-y_{i}(w^{T}x^{i} + b) \leq 0 , \quad i=1, \ldots , m \}
\]
é vazio, comprovando que a formulação dada pelo problema \eqref{eq:problemageralSVM} não fornece um classificador nesta situação.

Assim, a ideia central por trás da técnica SVM com margem flexível é introduzir variáveis de folga $\xi_{i} \geq 0$ associadas aos dados de treinamento $x_{i}$, com $i=1, \ldots , m$, suavizando a margem e permitindo desse modo que o problema de estimar as variáveis $w$ e $b$ torne-se mais flexível. Em outras palavras, as restrições $1-y_{i}(w^{T}x^{i} + b) \leq 0$ são relaxadas e substituídas por $1-y_{i}(w^{T}x^{i} + b) \leq \xi_{i} $, em que $\xi_{i} \geq 0$. Tal processo é também denominado de regularização.

Cada variável de folga $\xi_{i}$ corresponde à distância que determinado dado $x_{i}$ está do hiperplano que delimita a margem, isto é, $\Hset^{+}$ ou $\Hset^{-}$. Dessa forma, caso um dado $x^{i}$ esteja localizado no semiespaço correto a variável de folga $\xi_{i} = 0$, se estiver localizado no lado correto em relação ao hiperplano separador, porém na região da margem, temos que $0< \xi_{i} < 1$, e se tal dado estiver no lado errado, então $\xi_{i} >1$. Portanto, este processo de regularização flexibiliza a margem, permitindo que pontos da classe positiva permaneçam fora do semiespaço $\mathcal{S}^{+}=\{x\in \RR^n \mid w^{T}x+b\geq 1\}$ ou pontos da classe negativa fora do semiespaço $\mathcal{S}^{-}=\{x\in \RR^n \mid w^{T}x+b\leq -1\}$.

\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.50\textwidth]{variaveis_de_folga}
	\caption{Variáveis de folga. \label{fig:variaveis_de_folga}}
\end{figure}

Nesta formulação, o hiperplano separador $\Hset(w,b)$ é denominado hiperplano de margem flexível e as restrições que correspondem aos hiperplanos $\Hset^{+} (w,b)$ e $\Hset^{-} (w,b)$, que delimitam a margem, são reformuladas da seguinte maneira 
\begin{align}
w^{T}x^{i}+b &\geq 1 - \xi_{i} , \text{ para  todo } x^{i} \in \Xset^{+}, \label{restricoesCSVM+} \\
w^{T}x^{i}+b &\leq -1 +\xi_{i} , \text{ para  todo } x^{i} \in \Xset^{-}. \label{restricoesCSVM-}
\end{align}

%Agora nosso objetivo é encontrar $w$ e $b$ ótimos de modo a obter um bom classificador. 
Observe que, dados $w$ e $b$ arbitrários, podemos escolher $\xi_{i} \geq 0$ de modo que as restrições \eqref{restricoesCSVM+} e \eqref{restricoesCSVM-} sejam satisfeitas. Para tanto, basta definir
\[
\xi_{i} =  \left \{ \begin{array}{cc} \max\{0, 1-w^{T}x^{i}-b\}, & \quad \text{se} \quad x^{i} \in \Xset^{+}, \\
\max\{0, 1+w^{T}x^{i}+b\}, & \quad \text{se} \quad x^{i} \in \Xset^{-}.
\end{array} \right .
\]
 
No entanto, para obter um bom classificador não basta apenas maximizar a margem definida pelos hiperplanos $\Hset^{+} (w,b)$ e $\Hset^{-} (w,b)$ e introduzir as variáveis de folga nas restrições, mantendo a mesma função objetivo. Como exemplificado por \textcite[p. 45]{Evelin2017}, a \Cref{fig:hiperplano_nao_classificador} ilustra o hiperplano dado por $w_{0}^{T}x+b_{0} = 0$, o qual satisfaz as restrições \eqref{restricoesCSVM+} e \eqref{restricoesCSVM-} mas não serve para classificar os dados. 

\begin{figure}[!ht] 
	\centering
	\includegraphics[width=0.38\textwidth]{hiperplano_nao_classifica_dados}
	\caption{Exemplo de hiperplano que satisfaz as restrições mas não classifica os dados. \label{fig:hiperplano_nao_classificador}}
\end{figure}
 
Portanto, além  de acrescentar as variáveis de folga às restrições, é preciso controlar seu valor para encorajar uma correta classificação. Em vista disso, acrescentamos à função objetivo o somatório dessas variáveis de folga $\xi_{i}$ multiplicado a um parâmetro de penalização $C>0$. Assim, o problema \eqref{eq:problemageralSVM} é reformulado da seguinte forma

\[ \label{eq:problemaCSVM}
\begin{aligned}
\min_{w,b,\xi} & \quad \dfrac{1}{2} \Vert w\Vert^{2} + C \sum_{i=1}^{m} \xi_{i} \\
\text{s.a.} &  \quad y_i(w^{T}x^{i}+b) \geq 1 - \xi_{i}, \quad i=1, \ldots , m, \\
& \quad \xi_{i} \geq 0, \quad i=1, \ldots , m.\end{aligned}
\]

O parâmetro de penalização $C$ tem como objetivo controlar a importância das variáveis de folga ao minimizar a função objetivo. Note que, caso seja atribuído a ele um valor muito alto o problema se concentra em minimizar as variáveis de folga, deixando de lado seu principal objetivo que é minimizar o termo $\dfrac{1}{2} \Vert w \Vert^{2}$, que implica na maximização da margem. Por outro lado, caso $C$ assuma valores muito pequenos uma maior quantidade de vetores recebe folga, o que pode resultar numa classificação incorreta. Dessa forma, \textcite{Evelin2017} salienta que o valor do parâmetro $C$ que fornece uma boa classificação dos dados é escolhido de maneira heurística nas fases de treinamento e teste, sendo necessário considerar também natureza do problema. 

É devido a presença do parâmetro $C$ no problema \eqref{eq:problemaCSVM} formulado para a técnica SVM com margem flexível que ela também é denominada CSVM.

%O termo $C \sum_{i=1}^{m} \xi_{i}$ na função objetivo do problema \eqref{eq:problemaCSVM} pode ser pensado como uma medida de erro de classificação, pois minimiza o valor das variáveis de folga e reduz desse modo o número de pontos classificados incorretamente. De fato, aumentando o valor do parâmetro $C$ aumenta-se a penalização sobre a violação da restrição original do problema SVM. Por outro lado, diminuindo o valor de $C$ o modelo se torna mais flexível a esse tipo de violação. 

O problema com margem flexível \eqref{eq:problemaCSVM} também possui restrições lineares 
\[ 
g_{i}(w,b,\xi) = 1-\xi_{i} - y_i(w^{T}x^{i}+b) \leq 0 
\]
e 
\[
h_{i}(w,b,\xi) = - \xi_{i} \leq 0,
\]
para $i=1, \ldots, m$, e assim, o conjunto factível
\[ \label{conjunto_factivel_problema_SVM_flexivel}
\Omega = \{(w,b,\xi) \in \RR^{n+1+m} \mid g_{i}(w,b,\xi) \leq 0, \, h_{i}(w,b,\xi) \leq 0, \, i=1, \ldots, m \} 
\]
é um poliedro não vazio. Ademais, a função objetivo é quadrática e limitada inferiormente em $\Omega$, pois 
\[ \label{funcao_objetivo_problema_SVM_flexivel_limitada}
f(w,b,\xi) = \dfrac{1}{2} \Vert w\Vert^{2} + \underbrace{C}_{> 0} \sum_{i=1}^{m} \underbrace{\xi_{i}}_{\geq 0} \geq 0.
\]

Novamente, baseando-se em \textcite[Teo. 2.13]{Evelin2017}, apresentamos a seguir o teorema que garante a existência de um minimizador global para o problema \eqref{eq:problemaCSVM}.

\begin{teo} \label{teo:unicidade_solucao_problema_SVM_flexivel}
O problema \eqref{eq:problemaCSVM} possui um minimizador global.
\end{teo}
\begin{proof}
Como visto em \eqref{conjunto_factivel_problema_SVM_flexivel} e \eqref{funcao_objetivo_problema_SVM_flexivel_limitada}, o conjunto factível do problema \eqref{eq:problemaCSVM} é não vazio e a função objetivo é limitada inferiormente. Logo, definimos
\[
f^{*} \coloneqq \inf_{(w,b,\xi) \in \Omega} f(w,b,\xi) > -\infty.
\]
Pela definição de ínfimo, podemos concluir que para todo $k\in \mathds{N}$, existe $(w_{k}, b_{k}, \xi_{k}) \subset \Omega $ tal que
\[
f^{*} \leq f(w_{k}, b_{k}, \xi_{k}) < f^{*} + \dfrac{1}{k} .
\]
Desse modo, pelo Teorema do Sanduíche, 
\[ \label{eq:1_teo_existencia_solucao_SVM_flexivel}
\dfrac{1}{2} \Vert w_{k} \Vert^{2} + C\sum_{i=1}^{m} (\xi_{k})_{i} = f(w_{k}, b_{k}, \xi_{k}) \rightarrow f^{*} .
\]
Como
\[
0\leq \dfrac{1}{2} \Vert w_{k} \Vert^{2} \leq f(w_{k}, b_{k}, \xi_{k}) \quad \text{e} \quad 0 \leq C\sum_{i=1}^{m} (\xi_{k})_{i} \leq f(w_{k}, b_{k}, \xi_{k}) ,
\]
temos que as sequências $(w_{k})$ e $(\xi_{k})$ são limitadas. Vejamos que a sequência $(b_{k})$ também é limitada. Com efeito, dados $x^{j} \in \Xset^{+}$ e $x^{l} \in \Xset^{-}$ arbitrários, de \eqref{restricoesCSVM+} e \eqref{restricoesCSVM-} resulta que 
\[
1 - (\xi_{k})_{j} - w_{k}^{T} x^{j} \leq b_{k} \leq -1 + (\xi_{k})_{l} - w_{k}^{T} x^{l} .
\]
Dessa forma, a sequência $(w_{k}, b_{k}, \xi_{k})$ é limitada e, pelo Teorema de Bolzano-Weierstrass, possui uma subsequência convergente 
\[
(w_{k_{j}}, b_{k_{j}}, \xi_{k_{j}}) \rightarrow  (w^{*}, b^{*}, \xi^{*}) .
\]
Logo, pela continuidade de $f$, obtemos
\[
f(w_{k_{j}}, b_{k_{j}}, \xi_{k_{j}}) \rightarrow  f(w^{*}, b^{*}, \xi^{*}) ,
\]
e de \eqref{eq:1_teo_existencia_solucao_SVM_flexivel} decorre que $f(w^{*}, b^{*}, \xi^{*}) = f^{*}$. Assim, provamos que existe $(w^{*}, b^{*}, \xi^{*}) \in \Omega$ tal que
\[
f(w^{*}, b^{*}, \xi^{*}) = f^{*} \leq f(w, b, \xi)
\]
para todo $(w, b, \xi) \in \Omega$.
\end{proof}

Portanto, para o caso em que os dados não são linearmente separáveis, mas é possível determinar um hiperplano com margem flexível no espaço de entrada que os separe, o problema de classificação pode ser resolvido através do problema \eqref{eq:problemaCSVM}.

Em síntese, com base na formulação matemática da técnica SVM desenvolvida neste capítulo, sua atuação consiste em resolver, para o conjunto de treino, o problema \eqref{eq:problemageralSVM} caso o conjunto de dados seja linearmente separável ou o problema \eqref{eq:problemaCSVM} caso contrário. Em seguida, através da solução ($w^{*}, b^{*})$ encontrada definimos a função de decisão $F: \RR^{n} \rightarrow \RR$ dada por
\[
F(x) = \sgn ((w^{*})^{T} x + b^{*}),
\]
em que $\sgn (a)$ é $1$ se $a$ for não-negativo e $-1$ se $a$ for negativo, e através da qual podemos classificar um novo dado $x\in \RR^{n}$. Assim, se $F(x) >0$, o ponto $x$ será classificado como da classe $\Xset^{+}$, e se $F(x)<0$, o ponto $x$ será classificado como da classe $\Xset^{-}$.

No próximo capítulo utilizaremos o referencial teórico apresentado até o momento, assim como os problemas formulados em \eqref{eq:problemageralSVM} e \eqref{eq:problemaCSVM}, para testar numericamente através de uma implementação computacional, a técnica SVM para classificação.

\newpage
\section{Experimentos Numéricos} \label{chap:experimentos_numericos}


Este capítulo é dedicado a apresentar alguns experimentos numéricos que desenvolvemos e que têm como
objetivo visualizar na prática a implementação da técnica Máquinas de
Vetores Suporte (SVM), possibilitando assim analisar suas particularidades. Para tanto, o capítulo é dividido em
duas seções. Na primeira, discutimos a aplicação de SVM para classificar
o conjunto de dados flor Íris entre suas espécies. A segunda seção
aborda a aplicação da técnica CSVM para detectar num conjunto de dados
sobre células de câncer de mama quais apresentam tumor maligno ou
benigno.

Para desenvolvimento desses experimentos utilizamos o \emph{software} de
programação Julia em sua versão 1.4.0, além dos pacotes \texttt{Plots}
para gerar as imagens, \texttt{JuMP} e \texttt{Ipopt} para resolução dos
problemas de otimização, \texttt{RDatasets} para ter acesso ao conjunto
de dados Íris e o pacote \texttt{DataFrame} para gerar as tabelas de
dados. Ademais, nos baseamos nos vídeos de \textcite{AbelSVM1,AbelSVM2} para desenvolvimento dos códigos na plataforma Jupyter Notebook. Assim, os principais códigos serão apresentados no decorrer das seções seguintes associados às explicações da implementação computacional.

Como abordado no capítulo anterior, Máquinas de Vetores Suporte é uma
técnica de Aprendizagem de Máquina muito utilizada para classificação e
regressão, e nosso objetivo será sua aplicação em problemas que envolvam
a classificação binária de dados. Primeiramente, vamos relembrar que em
problemas de classificação estamos interessados, assim como o nome já
antecipa, em classificar da melhor maneira possível um determinado
conjunto de dados. No caso em que os dados são linearmente separáveis,
isto é, existe um hiperplano que os separa corretamente, aplica-se SVM
de margem rígida e o problema costuma ter uma resolução mais simples.
Entretanto, os problemas de classificação que envolvem situações reais
costumam ser mais elaborados, pois neste caso os dados geralmente não
são linearmente separáveis. Nestas situações é necessário utilizar SVM
com margem flexível (CSVM), se os dados forem, a grosso modo, mais
``comportados'', ou a SVM não-linear. Este último caso exige um
desenvolvimento teórico matemático mais avançado e que foge do escopo
deste trabalho e portanto não será abordado, se constituindo numa
proposta de estudos a ser desenvolvida em projetos futuros.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{1}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{k}{using} \PY{n}{RDatasets}\PY{p}{,} \PY{n}{JuMP}\PY{p}{,} \PY{n}{Ipopt}\PY{p}{,} \PY{n}{Random}\PY{p}{,} \PY{n}{LinearAlgebra}\PY{p}{,} \PY{n}{Plots}\PY{p}{,} \PY{n}{StatsBase}\PY{p}{,} \PY{n}{DataFrames}\PY{p}{,} \PY{n}{CSV}\PY{p}{,} \PY{n}{StatsPlots}
%\PY{n}{include}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{p}\PY{l+s}{a}\PY{l+s}{i}\PY{l+s}{r}\PY{l+s}{p}\PY{l+s}{l}\PY{l+s}{o}\PY{l+s}{t}\PY{l+s}{.}\PY{l+s}{j}\PY{l+s}{l}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

            %\begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{1}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%pairplot (generic function with 1 method)
%\end{Verbatim}
%\end{tcolorbox}
        
    \hypertarget{implementauxe7uxe3o-de-svm-para-classificauxe7uxe3o-do-conjunto-de-dados-uxedris}{%
\subsection{Implementação de SVM para Classificação do Conjunto de Dados
Íris}\label{implementauxe7uxe3o-de-svm-para-classificauxe7uxe3o-do-conjunto-de-dados-uxedris}}

Neste primeiro momento nosso propósito será implementar a técnica SVM
num exemplo prático: o conjunto de dados flor Íris. Tal conjunto de
dados consiste em 150 amostras de três espécies da planta Íris, sendo 50
amostras da Íris setosa, 50 da Íris virginica e 50 da Íris versicolor.
Cada dado amostral contém as medidas de quatro variáveis morfológicas:
comprimento e largura das sépalas e das pétalas, medidas em centímetros.
É com base nas diferenciações e semelhanças dessas características que é
possível distinguir uma espécie da outra.

\begin{figure}
    \centering
    \includegraphics[width=0.80\textwidth]{Flor_Iris.PNG}
    \caption{As três espécies da flor Íris: versicolor, setosa e virginica. \\ Fonte: https://www.datacamp.com/community/tutorials/machine-learning-in-r}
    \label{fig:imagem_flor_iris}
\end{figure}

%A seguir importamos o conjunto de dados Íris do pacote
%\texttt{RDatasets}. Perceba que as amostras pertencem a três diferentes espécies: setosa, versicolor e virginica. 
Como nossos estudos se concentram na classificação binária, nosso objetivo inicial será
aprender a separar os dados em setosa e não setosa e posteriormente em virginica e não virginica. Assim, o conjunto de dados Íris é representado na \Cref{fig:pairplot_dados_iris} a seguir, em que os pontos em azul pertencem à espécie setosa, os pontos em vermelho à espécie versicolor e os pontos em verde à espécie virginica. Tais gráficos foram construídos considerando-se duas características de cada vez.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{2}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{iris} \PY{o}{=} \PY{n}{dataset}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{d}\PY{l+s}{a}\PY{l+s}{t}\PY{l+s}{a}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{s}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{i}\PY{l+s}{r}\PY{l+s}{i}\PY{l+s}{s}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\PY{n}{levels}\PY{p}{(}\PY{n}{iris}\PY{p}{[}\PY{o}{!}\PY{p}{,}\PY{o}{:}\PY{n}{Species}\PY{p}{]}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{2}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%3-element Array\{String,1\}:
% "setosa"
% "versicolor"
% "virginica"
%\end{Verbatim}
%\end{tcolorbox}
    

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{3}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{plt} \PY{o}{=} \PY{n}{pairplot}\PY{p}{(}\PY{n}{iris}\PY{p}{,}\PY{o}{:}\PY{n}{Species}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
 
% \prompt{Out}{outcolor}{3}{}
    
    \begin{center} 
    \begin{figure}
    \includegraphics[width=0.95\textwidth]{Implementacao_de_SVM_em_Julia_TCC_files/Implementacao_de_SVM_em_Julia_TCC_6_0.pdf} 
    \caption{Conjunto de dados Flor Íris.} \label{fig:pairplot_dados_iris}
    \end{figure}
    \end{center}
    

\hypertarget{classificauxe7uxe3o-com-duas-caracteruxedsticas}{%
\subsubsection{Classificação com duas
características}\label{classificauxe7uxe3o-com-duas-caracteruxedsticas}}

De acordo com a \Cref{chap:maquinas_vetores_suporte}, a técnica SVM com margem rígida pode ser
formulada pelo problema \eqref{eq:problemageralSVM}. Assim, utilizando esta formulação, apresentamos a seguir uma aplicação na classificação do conjunto de dados Íris em setosa e não setosa com base em duas características apenas.

Inicialmente, é importante lembrar que na modelagem do problema cada
dado é representado por um vetor no espaço $n$-dimensional, em que $n$
corresponde ao número de características do problema em questão. Neste
exemplo, como o problema compreende apenas duas características os dados
de entrada $x^{i}$ pertencem ao espaço $\RR^{2}$. Logo,
podemos representar estes dados através de uma matriz
$X_{150\times 2}$, em que cada linha corresponde a um vetor $x^{i}$
e cada coluna às suas características, que neste caso serão comprimento
e largura das sépalas, medidas em centímetros.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{X} \PY{o}{=} \PY{n}{convert}\PY{p}{(}\PY{k+kt}{Array}\PY{p}{,}\PY{n}{iris}\PY{p}{[}\PY{o}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{o}{:}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)} 
\PY{n}{p}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{size}\PY{p}{(}\PY{n}{X}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
(150, 2)
\end{Verbatim}
\end{tcolorbox}
        
Por conseguinte, a SVM é uma técnica de aprendizagem supervisionada e
portanto, a obtenção do classificador é feita com base num conjunto de
dados de entrada para os quais há o prévio conhecimento da classe
$y_{i}$ a qual cada amostra $i$ pertence. Assim, amostras da espécie
setosa serão classificadas como $1$, enquanto que amostras das
espécies versicolor ou virginica serão classificadas em $-1$, ou seja,
$y_{i} \in \{ -1,1 \}$.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{iris\PYZus{}df} \PY{o}{=} \PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{;}
\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y} \PY{o}{=} \PY{p}{[}\PY{n}{species} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{\PYZdq{}} \PY{o}{?} \PY{l+m+mf}{1.0} \PY{o}{:} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0} \PY{k}{for} \PY{n}{species} \PY{k+kp}{in} \PY{n}{iris}\PY{p}{[}\PY{o}{!}\PY{p}{,}\PY{o}{:}\PY{n}{Species}\PY{p}{]}\PY{p}{]}\PY{p}{;}
\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Especie} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{Species}
\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x1} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Comprimento\PYZus{}sepala}\PY{p}{)}\PY{p}{)}
\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x2} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Largura\PYZus{}sepala}\PY{p}{)}\PY{p}{)}
\PY{n}{first}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

Portanto, nosso objetivo é classificar o conjunto de dados Íris em
setosa e não setosa com base nas características ``comprimento de
sépala'' e ``largura de sépala''. O gráfico a seguir (\Cref{fig:grafico_Iris_duas_caracteristicas}) representa tais
dados em relação às duas características citadas. Através dele podemos
ter uma noção acerca da separabilidade do conjunto Íris neste contexto.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{6}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y} \PY{o}{.==} \PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{blue}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{square}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{S}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y} \PY{o}{.==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter!}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{red}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{circle}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{N}\PY{l+s}{a}\PY{l+s}{o}\PY{l+s}{ }\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{o}{:}\PY{n}{outerbottomright}\PY{p}{)}
%\PY{n}{plot!}\PY{p}{(}\PY{n}{xlabel}\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{L}\PY{l+s}{a}\PY{l+s}{r}\PY{l+s}{g}\PY{l+s}{u}\PY{l+s}{r}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{s}\PY{l+s}{é}\PY{l+s}{p}\PY{l+s}{a}\PY{l+s}{l}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{c}\PY{l+s}{m}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{ylabel}\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{C}\PY{l+s}{o}\PY{l+s}{m}\PY{l+s}{p}\PY{l+s}{r}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{n}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{ }\PY{l+s}{s}\PY{l+s}{é}\PY{l+s}{p}\PY{l+s}{a}\PY{l+s}{l}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{c}\PY{l+s}{m}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
 
{ %\prompt{Out}{outcolor}{6}{} 
        \begin{figure}
            \centering
            \includegraphics[width=0.75\textwidth]{Implementacao_de_SVM_em_Julia_TCC_files/Implementacao_de_SVM_em_Julia_TCC_12_0.pdf}
            \caption{Conjunto de dados Íris de acordo com comprimento e largura das sépalas.}
            \label{fig:grafico_Iris_duas_caracteristicas}
        \end{figure}
    }    

Assim, no gráfico acima estão representados os 150 dados do conjunto
Íris, em que os pontos em azul representam a espécie setosa, isto é, os
vetores \(x^{i}\) tais que \(y_{i} = 1\), e os pontos em vermelho as
espécies versicolor e virginica, que correspondem aos vetores \(x^{i}\)
tais que \(y_{i} = -1\). Como os dados aparentam ser linearmente
separáveis, aplicaremos primeiramente SVM com margem rígida.

A aprendizagem supervisionada é composta por dois momentos, a fase de
treino e a fase de testes. Em vista disso, inicialmente dividimos, de
maneira aleatória, o conjunto Íris em dois subconjuntos, o conjunto de
treinamento (\texttt{train\_set}), com 50 dados, e o conjunto de teste
(\texttt{test\_set}), com os 100 dados restantes. É através do conjunto
de treinamento que a técnica SVM irá ``aprender'' a classificar os
dados detectando padrões entre suas características e a espécie a qual
pertencem. Já o conjunto de teste será utilizado para analisar a
eficácia do classificador encontrado, averiguando se ele gera as saídas
corretas para tais dados.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Random}\PY{o}{.}\PY{n}{seed!}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{trainsize} \PY{o}{=} \PY{l+m+mi}{50}
\PY{n}{train\PYZus{}set} \PY{o}{=} \PY{n}{sample}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{p}\PY{p}{,}\PY{n}{trainsize}\PY{p}{,}\PY{n}{replace}\PY{o}{=}\PY{k+kc}{false}\PY{p}{,}\PY{n}{ordered}\PY{o}{=}\PY{k+kc}{true}\PY{p}{)} 
\PY{n}{test\PYZus{}set} \PY{o}{=} \PY{n}{setdiff}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{p}\PY{p}{,}\PY{n}{train\PYZus{}set}\PY{p}{)} 
\PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{train\PYZus{}set}\PY{p}{,}\PY{o}{:}\PY{p}{]}
\PY{n}{Ytrain} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y}\PY{p}{[}\PY{n}{train\PYZus{}set}\PY{p}{]}
\PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{,}\PY{o}{:}\PY{p}{]}
\PY{n}{Ytest} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{]}
\PY{n}{ptrain} \PY{o}{=} \PY{n}{length}\PY{p}{(}\PY{n}{Ytrain}\PY{p}{)}
\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{conjunto} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{p}\PY{p}{)}
\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{conjunto}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{]} \PY{o}{.=} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    Como abordado na \Cref{chap:maquinas_vetores_suporte}, a modelagem do problema de classificação
utilizando a técnica SVM consiste em determinar o hiperplano que melhor
separa os dados, classificando-os assim em duas classes. Ademais, o
hiperplano ótimo $\mathcal{H}(w,b)$ é aquele que maximiza a margem que
não contenha nenhum dado, ou seja, desejamos que os pontos $x^{i}$
satisfaçam a seguinte restrição

\[
y_i(w^{T}x^{i}+b) \geq 1, \quad i=1, \ldots , 50.
\]

Logo, o problema de encontrar o hiperplano ótimo \(\mathcal{H}(w,b)\) é
formulado da seguinte forma
\begin{align}
\min_{w,b} & \quad \dfrac{1}{2} \Vert w\Vert^{2} \\
\text{s.a.} &  \quad y_i(w^{T}x^{i}+b) \geq 1, \quad i=1, \ldots , 50, 
\end{align} 
em que, neste exemplo, $w \in \RR^{2}$ e $b\in \RR$. É importante salientar que como neste caso os dados pertencem ao $\RR^{2}$ o hiperplano ótimo será uma reta.

Assim, utilizando o modelo matemático formulado no capítulo anterior,
criamos a função \texttt{SVM\_rigida}, a qual adapta o problema de
otimização formulado em \eqref{eq:problemageralSVM} para os dados do problema de classificação
que desejamos resolver e, com o auxílio do pacote \texttt{Ipopt} para
resolver este problema, determina o hiperplano ótimo.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{function} \PY{n}{SVM\PYZus{}rigida}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{)}
    \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{n}{optimizer\PYZus{}with\PYZus{}attributes}\PY{p}{(}\PY{n}{Ipopt}\PY{o}{.}\PY{n}{Optimizer}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{p}\PY{l+s}{r}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{t}\PY{l+s}{\PYZus{}}\PY{l+s}{l}\PY{l+s}{e}\PY{l+s}{v}\PY{l+s}{e}\PY{l+s}{l}\PY{l+s}{\PYZdq{}}\PY{o}{=\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}

    \PY{n+nd}{@variable}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{n}\PY{p}{]}\PY{p}{)} \PY{c}{\PYZsh{} Aqui declaramos as variáveis.}
    \PY{n+nd}{@variable}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{b}\PY{p}{)}

    \PY{n+nd}{@objective}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{Min}\PY{p}{,} \PY{n}{dot}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{w}\PY{p}{)} \PY{o}{/} \PY{l+m+mi}{2}\PY{p}{)} \PY{c}{\PYZsh{} Esta é a função objetivo.}

    \PY{n+nd}{@constraint}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{p}{[}\PY{n}{i}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{ptrain}\PY{p}{]}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{dot}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{o}{:}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{)} \PY{o}{≥} \PY{l+m+mi}{1}\PY{p}{)} \PY{c}{\PYZsh{} Esta é a restrição.}

    \PY{n}{optimize!}\PY{p}{(}\PY{n}{model}\PY{p}{)}

    \PY{n}{w}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{value}\PY{o}{.}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{,} \PY{n}{value}\PY{o}{.}\PY{p}{(}\PY{n}{b}\PY{p}{)} \PY{c}{\PYZsh{} Com este comando queremos que os valores ótimos sejam apresentados.}
    \PY{k}{return} \PY{n}{w}\PY{p}{,} \PY{n}{b}
\PY{k}{end}
\PY{n}{w}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{SVM\PYZus{}rigida}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%    \begin{Verbatim}[commandchars=\\\{\}]
%
%******************************************************************************
%This program contains Ipopt, a library for large-scale nonlinear optimization.
% Ipopt is released as open source code under the Eclipse Public License (EPL).
%         For more information visit http://projects.coin-or.org/Ipopt
%******************************************************************************
%
%    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
([-2.857142829807275, 3.3333333011819755], 4.99999995276859)
\end{Verbatim}
\end{tcolorbox}
        
Assim, com o auxílio da função \texttt{SVM\_rigida}, encontramos os
valores ótimos para $w$ e $b$, os quais definem o hiperplano
separador $\mathcal{H}(w,b)$, que é dado por
\[
(-2.857142829807275, 3.333333301181975)^{T} x + 4.99999995276859 = 0,
\]
com $x \in \RR^{2}$.

Para melhor visualizar a classificação dos dados de treinamento vamos
representá-los graficamente a seguir junto ao hiperplano ótimo.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{9}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Ytrain} \PY{o}{.==} \PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{blue}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{square}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{S}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{o}{:}\PY{n}{outerbottomright}\PY{p}{)}
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Ytrain} \PY{o}{.==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter!}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{red}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{circle}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{N}\PY{l+s}{ã}\PY{l+s}{o}\PY{l+s}{ }\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\PY{c}{\PYZsh{} plot!(leg=false)}
%\PY{n}{plot!}\PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{magenta}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{o}{:}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{S}\PY{l+s}{o}\PY{l+s}{l}\PY{l+s}{u}\PY{l+s}{ç}\PY{l+s}{ã}\PY{l+s}{o}\PY{l+s}{ }\PY{l+s}{Ó}\PY{l+s}{t}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{a}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\PY{n}{plot!}\PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{blue}\PY{p}{,} \PY{n}{l}\PY{o}{=}\PY{o}{:}\PY{n}{dash}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\PY{n}{plot!}\PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{red}\PY{p}{,} \PY{n}{l}\PY{o}{=}\PY{o}{:}\PY{n}{dash}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
 
{           %\prompt{Out}{outcolor}{9}{}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth]{Implementacao_de_SVM_em_Julia_TCC_files/Implementacao_de_SVM_em_Julia_TCC_18_0.pdf}
        \caption{Separação dos dados de treinamento pelo hiperplano ótimo.}
        \label{fig:dados_treinamento_iris_hiperplano_otimo_margemrigida}
    \end{figure}
    % { \hspace*{\fill} \\}
    }

Portanto, no gráfico da \Cref{fig:dados_treinamento_iris_hiperplano_otimo_margemrigida} a reta magenta corresponde ao hiperplano ótimo e as retas tracejadas são os hiperplanos que delimitam a máxima
margem possível. Observe que o conjunto de dados de treinamento é linearmente separável, pois o hiperplano ótimo os está separando
corretamente.

Além disso, de acordo com a \Cref{defi:vetores_suporte} da \Cref{subsection:SVM_margem_rigida}, temos que os
vetores que estão sobre os hiperplanos da margem são os \emph{vetores
suporte}. Observe que tais vetores dão suporte ao hiperplano ótimo, de
modo que todos os demais vetores poderiam ser descartados sem alterá-lo.

Agora, tendo determinado o classificador (hiperplano ótimo), vamos para
a fase de testes, na qual estamos interessados em analisar se o
classificador encontrado é eficaz. Para tanto, vamos acrescentar ao
gráfico anterior os dados do conjunto de teste e observar se o
hiperplano encontrado também os separa corretamente.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{10}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Ytest} \PY{o}{.==} \PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter!}\PY{p}{(}\PY{n}{Xtest}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtest}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{magenta}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{square}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{S}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{o}{:}\PY{n}{outerbottomright}\PY{p}{)}
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Ytest} \PY{o}{.==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter!}\PY{p}{(}\PY{n}{Xtest}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtest}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{black}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{circle}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{N}\PY{l+s}{ã}\PY{l+s}{o}\PY{l+s}{ }\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
 
            
{%\prompt{Out}{outcolor}{10}{}

     \begin{figure}
         \centering
         \includegraphics[width=0.75\textwidth]{Implementacao_de_SVM_em_Julia_TCC_files/Implementacao_de_SVM_em_Julia_TCC_20_0.pdf}
         \caption{Classificação do conjunto de teste pelo hiperplano ótimo.}
         \label{fig:classificacao_conjunto_teste_margem_rigida}
     \end{figure}
     % { \hspace*{\fill} \\}
    } 

Analisando o gráfico na \Cref{fig:classificacao_conjunto_teste_margem_rigida} percebe-se que há um ponto da espécie setosa
do conjunto de teste que está localizado ligeiramente do lado direito do
hiperplano ótimo, fazendo com que seja classificado incorretamente como
não setosa. Assim, como determinar se o classificador encontrado é o
melhor?

Para responder a esta pergunta é preciso medir a eficácia do modelo,
comparando a real saída dos dados de teste com a respectiva
classificação obtida pelo modelo. De modo geral, quanto mais
classificações corretas o classificador predizer para o conjunto de
teste, mais eficiente ele é.

Para analisarmos o desempenho do classificador utilizaremos a metodologia empregada por \textcite{Evelin2017}, que usa os conceitos de matriz de confusão e acurácia. A Matriz de Confusão, também denominada Matriz de Erro, consiste em uma medida de desempenho muito utilizada para fazer avaliações de modelos de classificação da aprendizagem de máquina supervisionada, como a SVM por exemplo. Ela é uma tabela que apresenta quatro combinações entre a classificação real e a prevista, o que nos permite analisar se a previsão sugerida pelo classificador encontrado ao implementar a SVM é condizente com a verdadeira classificação dos dados. Em síntese, a matriz de confusão apresenta as seguintes frequências: Verdadeiro Positivo (VP), Verdadeiro Negativo (VN), Falso Positivo (FP) e Falso Negativo (FN).

Vamos compreender essas terminologias com base na classificação dos
dados Íris. Assim, para o problema em que desejamos classificar amostras
em setosa $(y_{i} = 1)$ e não setosa $(y_{i} = -1)$, temos que

\begin{itemize}
\item
  Verdadeiro Positivo (VP): quantidade de dados que são setosa
  $(y_{i} = 1)$ e foram classificados como tal $(y_{i} = 1)$;
\item
  Verdadeiro Negativo (VN): se refere ao número de dados que não são
  setosa $(y_{i} = -1)$ e foram classificados corretamente como não
  setosa $(y_{i} = -1)$);
\item
  Falso Positivo (FP): quantidade de dados não setosa $(y_{i} = -1)$
  classificados como setosa $(y_{i} = 1)$;
\item
  Falso Negativo (FN): quantidade de dados setosa $(y_{i} = 1)$ que
  receberam classificação não setosa $(y_{i} = -1)$.
\end{itemize}

Assim, a matriz de confusão permite observar a relação entre resultados
falsos/verdadeiros e negativos/positivos, fornecendo na diagonal
principal o número de acertos da classificação predita em relação a real
classificação (VP e VN), enquanto os demais elementos correspondem aos
erros na classificação (FP e FN). Vale observar que um classificador
ideal apresentaria uma matriz de confusão com os elementos não
pertencetes a diagonal principal iguais a zero, pois isso significaria
que tal classificador não comete erros.

Ademais, usando os valores fornecidos pela matriz de confusão podemos
calcular a acurácia do modelo, que fornece a porcentagem de dados
positivos e negativos classificados corretamente. Ela é dada pela
seguinte fórmula

\[
\text{Acurácia} = \dfrac{\text{VP} + \text{VN}}{\text{VP} + \text{FP} + \text{VN} + \text{FN}} .
\]
Note que, quanto mais próxima de $1$ for a acurácia, maior é a quantidade de dados classificados corretamente.

Portanto, para avaliarmos o classificador encontrado, vamos determinar a
matriz de confusão para este caso e calcular sua acurácia.
\vspace{0.5cm}
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Setosa} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Nonsetosa} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}

\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{iris\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)} 
\PY{n}{gdf}\PY{p}{[}\PY{n}{Nonsetosa}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

\PY{k}{function} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}
    \PY{n}{Verdadeiro\PYZus{}positivo} \PY{o}{=} \PY{n}{nrow}\PY{p}{(}\PY{n}{filter}\PY{p}{(}\PY{p}{[}\PY{o}{:}\PY{n}{Y}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{=\PYZgt{}} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+m+mf}{1.} \PY{o}{\PYZam{}\PYZam{}} \PY{n}{y} \PY{o}{==} \PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{gdf}\PY{p}{)}\PY{p}{)} 
    \PY{n}{Verdadeiro\PYZus{}negativo} \PY{o}{=} \PY{n}{nrow}\PY{p}{(}\PY{n}{filter}\PY{p}{(}\PY{p}{[}\PY{o}{:}\PY{n}{Y}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{=\PYZgt{}} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.} \PY{o}{\PYZam{}\PYZam{}} \PY{n}{y} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{gdf}\PY{p}{)}\PY{p}{)}
    \PY{n}{Falso\PYZus{}positivo} \PY{o}{=} \PY{n}{nrow}\PY{p}{(}\PY{n}{filter}\PY{p}{(}\PY{p}{[}\PY{o}{:}\PY{n}{Y}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{=\PYZgt{}} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.} \PY{o}{\PYZam{}\PYZam{}} \PY{n}{y} \PY{o}{==} \PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{gdf}\PY{p}{)}\PY{p}{)}
    \PY{n}{Falso\PYZus{}negativo} \PY{o}{=} \PY{n}{nrow}\PY{p}{(}\PY{n}{filter}\PY{p}{(}\PY{p}{[}\PY{o}{:}\PY{n}{Y}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{=\PYZgt{}} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{)} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+m+mf}{1.} \PY{o}{\PYZam{}\PYZam{}} \PY{n}{y} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{gdf}\PY{p}{)}\PY{p}{)}
    
    \PY{n}{VP}\PY{p}{,} \PY{n}{VN}\PY{p}{,} \PY{n}{FP}\PY{p}{,} \PY{n}{FN} \PY{o}{=} \PY{n}{Verdadeiro\PYZus{}positivo}\PY{p}{,} \PY{n}{Verdadeiro\PYZus{}negativo}\PY{p}{,} \PY{n}{Falso\PYZus{}positivo}\PY{p}{,} \PY{n}{Falso\PYZus{}negativo}
    \PY{n}{acuracia} \PY{o}{=} \PY{p}{(}\PY{n}{VP} \PY{o}{+} \PY{n}{VN}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{VP} \PY{o}{+} \PY{n}{FP} \PY{o}{+} \PY{n}{VN} \PY{o}{+} \PY{n}{FN}\PY{p}{)}
    \PY{n}{matrizconfusao} \PY{o}{=} \PY{n}{DataFrame}\PY{p}{(}\PY{n}{Classe} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZdq{}}\PY{l+s}{R}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{l}\PY{l+s}{ }\PY{l+s}{P}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{i}\PY{l+s}{t}\PY{l+s}{i}\PY{l+s}{v}\PY{l+s}{a}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{R}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{l}\PY{l+s}{ }\PY{l+s}{N}\PY{l+s}{e}\PY{l+s}{g}\PY{l+s}{a}\PY{l+s}{t}\PY{l+s}{i}\PY{l+s}{v}\PY{l+s}{a}\PY{l+s}{\PYZdq{}}\PY{p}{]}\PY{p}{,}
                           \PY{n}{Predita\PYZus{}Positiva} \PY{o}{=} \PY{p}{[}\PY{n}{VP} \PY{p}{,} \PY{n}{FP}\PY{p}{]}\PY{p}{,}
                           \PY{n}{Predita\PYZus{}Negativa} \PY{o}{=} \PY{p}{[}\PY{n}{FN}\PY{p}{,} \PY{n}{VN}\PY{p}{]} 
                           \PY{p}{)}
        \PY{k}{return}  \PY{n}{acuracia}\PY{p}{,} \PY{n}{matrizconfusao}
\PY{k}{end}
\PY{n}{Acuracia\PYZus{}Iris}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Iris} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Iris}
\PY{n}{MatrizConfusao\PYZus{}Iris}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Iris = 0.99
    \end{Verbatim}
 
            
{\prompt{Out}{outcolor}{11}{}
    
    \begin{tabular}{ccc}
    Classe & Predita\_Positiva & Predita\_Negativa\\
    \hline
    
    Real Positiva & 35 & 1 \\
    Real Negativa & 0 & 64 \\
\end{tabular}
\vspace{0.5cm}
}
    
A tabela acima corresponde à matriz de confusão para o hiperplano ótimo encontrado, em que dos $100$ dados do conjunto de teste, o número de VP $=
35$, VN $= 64$, FN $= 1$ e FP $= 0$. Ou seja, um dado da espécie setosa foi
classificado incorretamente pelo hiperplano ótimo como pertencente às
espécies versicolor ou virginica, implicando numa acurácia de $99\%$.

É possível visualizar este dado no gráfico apresentado anteriormente e
como ele é único analisamos se tal classificação incorreta se repetia
nos casos em que a quantidade de dados do conjunto de treinamento fosse
maior, averiguando assim se uma maior quantidade de dados para treinar o
modelo resultaria numa solução mais eficiente e que classificasse todos
os dados de maneira correta. Contudo, o que se observou é que mesmo para
um conjunto de treino com 100 dados, por exemplo, ainda ocorria uma
classificação incorreta, pois os dados são distribuídos de maneira
aleatória entre os conjuntos de treino e teste, de modo que tal controle
não seja possível. Ademais, é importante que o conjunto de teste não
seja tão pequeno em relação ao conjunto de treino para que não ocorram
equívocos ao avaliar se o classificador é bom.

Em vista disso, aplicamos então a técnica SVM com margem flexível (CSVM)
para classificar esse mesmo conjunto de dados Íris e analisar se dessa
forma seria possível obter um classificador com uma acurácia de $100\%$
para o conjunto de teste.

Neste caso, de acordo com a teoria desenvolvida na \Cref{subsection:SVM_margem_flexivel}, o
problema de classificação com margem flexível tem o seguinte formato
\begin{align}
\min_{w,b, \xi} & \quad \dfrac{1}{2} \Vert w\Vert^{2} + C\sum_{i=1}^{50} \xi_{i} \\
\text{s.a.} &  \quad y_i(w^{T}x^{i}+b) \geq 1 - \xi_{i} , \quad i=1, \ldots , 50, \\
& \quad \xi_{i} \geq 0 , \ \ \ \ \qquad \qquad \qquad i=1, \ldots , 50, 
\end{align}
em que $w \in \RR^{2}$, $b\in \RR$,
$\xi \in \RR^{50}$ e $C>0$.

De maneira análoga à função \texttt{SVM\_rigida}, definimos então a
função \texttt{SVM\_flexivel} com base no problema acima para aplicar
aos problemas que exigem a técnica SVM com margem flexível.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{function} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{n}{C} \PY{o}{=} \PY{l+m+mf}{1.0}\PY{p}{)}
    \PY{n}{model} \PY{o}{=} \PY{n}{Model}\PY{p}{(}\PY{n}{optimizer\PYZus{}with\PYZus{}attributes}\PY{p}{(}\PY{n}{Ipopt}\PY{o}{.}\PY{n}{Optimizer}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{p}\PY{l+s}{r}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{t}\PY{l+s}{\PYZus{}}\PY{l+s}{l}\PY{l+s}{e}\PY{l+s}{v}\PY{l+s}{e}\PY{l+s}{l}\PY{l+s}{\PYZdq{}}\PY{o}{=\PYZgt{}}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}

    \PY{n+nd}{@variable}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{n}\PY{p}{]}\PY{p}{)} \PY{c}{\PYZsh{} Aqui declaramos as variáveis.}
    \PY{n+nd}{@variable}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{b}\PY{p}{)}
    \PY{n+nd}{@variable}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{ξ}\PY{p}{[}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{ptrain}\PY{p}{]} \PY{o}{≥} \PY{l+m+mi}{0}\PY{p}{)}

    \PY{n+nd}{@objective}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{Min}\PY{p}{,} \PY{n}{dot}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{w}\PY{p}{)} \PY{o}{/} \PY{l+m+mi}{2} \PY{o}{+} \PY{n}{C} \PY{o}{*} \PY{n}{sum}\PY{p}{(}\PY{n}{ξ}\PY{p}{)}\PY{p}{)} \PY{c}{\PYZsh{} Esta é a função objetivo.}

    \PY{n+nd}{@constraint}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{p}{[}\PY{n}{i}\PY{o}{=}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{ptrain}\PY{p}{]}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{*} \PY{p}{(}\PY{n}{dot}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{[}\PY{n}{i}\PY{p}{,}\PY{o}{:}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{n}{b}\PY{p}{)}  \PY{o}{≥} \PY{l+m+mi}{1}\PY{o}{\PYZhy{}} \PY{n}{ξ}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{c}{\PYZsh{} Esta é a restrição.}

    \PY{c}{\PYZsh{} print(model) }
    \PY{n}{optimize!}\PY{p}{(}\PY{n}{model}\PY{p}{)}

    \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{value}\PY{o}{.}\PY{p}{(}\PY{n}{w}\PY{p}{)}\PY{p}{,} \PY{n}{value}\PY{o}{.}\PY{p}{(}\PY{n}{b}\PY{p}{)}\PY{p}{,} \PY{n}{value}\PY{o}{.}\PY{p}{(}\PY{n}{ξ}\PY{p}{)} \PY{c}{\PYZsh{}aqui queremos desenhar os valores ótimos.}
        \PY{k}{return} \PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ}
\PY{k}{end}
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
([-1.8054054084206983, 1.567567554887955], 4.721081137153697, [0.0,
0.42270269035295177, 0.0, 0.0, 0.0, 0.0, 0.1805405354351823, 0.2659459348641559,
0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
\end{Verbatim}
\end{tcolorbox}
        
Portanto, adaptando nosso modelo obtemos uma nova solução que é
apresentada acima. Os valores ótimos encontrados determinaram um novo
hiperplano classificador, o qual está representado na cor magenta no
gráfico da \Cref{fig:hiperplano_otimo_dados_iris_margem_flexível}.

{            %\prompt{Out}{outcolor}{13}{}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth]{Implementacao_de_SVM_em_Julia_TCC_files/Implementacao_de_SVM_em_Julia_TCC_26_0.pdf}
        \caption{Hiperplano ótimo determinado pela técnica SVM com margem flexível.}
        \label{fig:hiperplano_otimo_dados_iris_margem_flexível}
    \end{figure}
    %\begin{center}
    %\adjustimage{max size={0.8\linewidth}{0.9\paperheight}}{Implementacao_de_SVM_em_Julia_TCC_files/Implementacao_de_SVM_em_Julia_TCC_26_0.pdf}
    %\end{center}
    }
    
Fazendo uma comparação entre este hiperplano ótimo e aquele determinado
no problema em que aplicamos SVM de margem rígida, percebemos que no
caso atual os vetores que apresentam $\xi_{i} > 0$ possuem uma maior
``liberdade'', de modo que possam estar localizados na região entre as
margens e o hiperplano separador. Tal ``liberdade'' é fruto do
relaxamento promovido nas restrições através das variáveis de folga,
relaxamento que deve ser regulado. É neste contexto que entra o
parâmetro $C$. De acordo com \textcite{Evelin2017}, tal parâmetro nos
fornece um ``(...) equilíbrio entre a maximização da margem e a minimização do
erro de classificação''. Ou seja, caso o valor atribuído ao parâmetro de
penalização $C$ seja pequeno, uma maior quantidade de vetores recebe
folga, inclusive alguns para os quais não seria necessário. Caso
contrário, se valores muito altos são atribuídos ao parâmetro $C$, o
número de vetores que recebe folga diminui. Contudo, neste último caso o
programa tende a se concentrar em minimizar a penalização em vez de
maximizar a margem na função objetivo. Em decorrência disso, é de suma
importância escolher o valor correto para o parâmetro $C$.

Assim, ao definir a função \texttt{SVM\_flexivel} determinamos como
padrão $C=1$, pois como veremos na fase de testes tal valor
possibilitou uma boa classificação para o problema atual. No entanto, na
próxima seção analisaremos os classificadores obtidos para diferentes
valores de $C$.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{13}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{ξ} \PY{o}{.\PYZgt{}} \PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)} 
%\PY{n}{scatter!}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{green}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{p}{(}\PY{o}{:}\PY{n}{white}\PY{p}{,} \PY{n}{stroke}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{:}\PY{n}{green}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Ytrain} \PY{o}{.==} \PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{blue}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{square}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{S}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{o}{:}\PY{n}{outerbottomright}\PY{p}{)}
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Ytrain} \PY{o}{.==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter!}\PY{p}{(}\PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{red}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{circle}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{N}\PY{l+s}{ã}\PY{l+s}{o}\PY{l+s}{ }\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\PY{c}{\PYZsh{} plot!(leg=false)}
%\PY{n}{plot!}\PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{magenta}\PY{p}{,} \PY{n}{lw}\PY{o}{=}\PY{o}{:}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{S}\PY{l+s}{o}\PY{l+s}{l}\PY{l+s}{u}\PY{l+s}{ç}\PY{l+s}{ã}\PY{l+s}{o}\PY{l+s}{ }\PY{l+s}{Ó}\PY{l+s}{t}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{a}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\PY{n}{plot!}\PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{blue}\PY{p}{,} \PY{n}{l}\PY{o}{=}\PY{o}{:}\PY{n}{dash}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\PY{n}{plot!}\PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{w}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{x} \PY{o}{+} \PY{n}{b} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)} \PY{o}{/} \PY{n}{w}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{red}\PY{p}{,} \PY{n}{l}\PY{o}{=}\PY{o}{:}\PY{n}{dash}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
 

Novamente, após determinado o classificador, é preciso avaliar sua
eficiência. Portanto, utilizando o conjunto de teste vamos construir a
matriz de confusão e calcular a acurácia do novo hiperplano encontrado.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Setosa} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Nonsetosa} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{iris\PYZus{}df}\PY{p}{)} \PY{c}{\PYZsh{}filter é uma função. Neste caso, ela foi utilizada para \PYZdq{}filtrar\PYZdq{}/selecionar em um dataframe que denominamos por \PYZdq{}gdf\PYZdq{} todos os dados x que possuiam \PYZdq{}teste\PYZdq{} na coluna \PYZdq{}conjunto\PYZdq{}. Assim, conseguimos analisar a predição do conjunto de teste}
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)} \PY{c}{\PYZsh{}fill é uma função. Neste caso ela preencheu a coluna Ypredito com 1.}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Nonsetosa}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Iris}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Iris} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Iris}
\PY{n}{MatrizConfusao\PYZus{}Iris}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Iris = 1.0
    \end{Verbatim}
 
            
{\prompt{Out}{outcolor}{14}{}
    
    \begin{tabular}{ccc}
    Classe & Predita\_Positiva & Predita\_Negativa\\
    \hline
    
    Real Positiva & 36 & 0 \\
    Real Negativa & 0 & 64 \\
\end{tabular}
\vspace{0.5cm}
    }

Analisando a matriz de confusão temos que os valores de FN e FP são
nulos, o que nos permite concluir que o hiperplano ótimo encontrado pela
técnica SVM com margem flexível atua como um bom classificador,
apresentando uma acurácia igual a $1$ e, portanto, $100\%$ de acerto na
classificação dos dados de teste em setosa e não setosa. De fato, no
gráfico a seguir podemos visualizar que o hiperplano ótimo separa todos
os dados dos conjuntos de treino e teste corretamente.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{15}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Ytest} \PY{o}{.==} \PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter!}\PY{p}{(}\PY{n}{Xtest}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtest}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{magenta}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{square}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{S}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{legend}\PY{o}{=}\PY{o}{:}\PY{n}{outerbottomright}\PY{p}{)}
%\PY{n+nb}{I} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Ytest} \PY{o}{.==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
%\PY{n}{scatter!}\PY{p}{(}\PY{n}{Xtest}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{Xtest}\PY{p}{[}\PY{n+nb}{I}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{o}{:}\PY{n}{black}\PY{p}{,} \PY{n}{m}\PY{o}{=}\PY{o}{:}\PY{n}{circle}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{N}\PY{l+s}{ã}\PY{l+s}{o}\PY{l+s}{ }\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{ }\PY{l+s}{(}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{)}\PY{l+s}{\PYZdq{}}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
 
{%\prompt{Out}{outcolor}{15}{}
    \begin{figure}
        \centering
        \includegraphics[width=0.75\textwidth]{Implementacao_de_SVM_em_Julia_TCC_files/Implementacao_de_SVM_em_Julia_TCC_30_0.pdf}
        \caption{Classificação do conjunto de teste pelo hiperplano ótimo.}
        \label{fig:dados_treino_teste_hiperplano_otimo_SVM_flexivel}
    \end{figure}
    }

Portanto, utilizando a técnica CSVM, com $C = 1$, foi possível
determinar um hiperplano separador com uma acurácia de $100\%$ para o
conjunto de teste.

\hypertarget{classificauxe7uxe3o-com-quatro-caracteruxedsticas}{%
\subsubsection{Classificação com quatro
características}\label{classificauxe7uxe3o-com-quatro-caracteruxedsticas}}

Agora, vamos apresentar a implementação da técnica SVM com margem rígida
para a classificação do conjunto de dados Íris em setosa e não setosa
levando em consideração quatro características: comprimento e largura
das sépalas e pétalas.

De modo análogo ao exemplo anterior, os dados pertencentes à espécie
setosa serão classificados como $1$, e os demais, pertencentes às espécies
versicolor ou virginica, classificados como $-1$. A principal diferença
agora é que os dados do conjunto de entrada pertencem ao
$\RR^{4}$.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{16}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{X} \PY{o}{=} \PY{n}{convert}\PY{p}{(}\PY{k+kt}{Array}\PY{p}{,}\PY{n}{iris}\PY{p}{[}\PY{o}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{o}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)} 
%\PY{n}{p}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{size}\PY{p}{(}\PY{n}{X}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

 %           \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{16}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%(150, 4)
%\end{Verbatim}
%\end{tcolorbox}
        
%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{17}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{iris\PYZus{}df} \PY{o}{=} \PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{;}
%\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y} \PY{o}{=} \PY{p}{[}\PY{n}{species} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{a}\PY{l+s}{\PYZdq{}} \PY{o}{?} \PY{l+m+mf}{1.0} \PY{o}{:} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0} \PY{k}{for} \PY{n}{species} \PY{k+kp}{in} \PY{n}{iris}\PY{p}{[}\PY{o}{!}\PY{p}{,}\PY{o}{:}\PY{n}{Species}\PY{p}{]}\PY{p}{]}\PY{p}{;}
%\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Especie} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{Species}
%\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x1} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Comp\PYZus{}sepala}\PY{p}{)}\PY{p}{)}
%\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x2} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Larg\PYZus{}sepala}\PY{p}{)}\PY{p}{)}
%\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x3} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Comp\PYZus{}petala}\PY{p}{)}\PY{p}{)}
%\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x4} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Larg\PYZus{}petala}\PY{p}{)}\PY{p}{)}
%\PY{n}{first}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{;}
%\end{Verbatim}
%\end{tcolorbox}

Analisando os gráficos da \Cref{fig:pairplot_dados_iris} temos que os
dados setosa (em azul) e não setosa (em verde e vermelho) aparentam ser
linearmente separáveis. Guiados por essa perspectiva, vamos aplicar a
técnica SVM com margem rígida para determinar o classificador.

Inicialmente, assim como no problema anterior, é necessário separar os
dados Íris em conjunto de treinamento (\texttt{train\_set}), com $50$
dados, e conjunto de teste (\texttt{test\_set}), com $100$ dados.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Random}\PY{o}{.}\PY{n}{seed!}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{trainsize} \PY{o}{=} \PY{l+m+mi}{50} 
\PY{n}{train\PYZus{}set} \PY{o}{=} \PY{n}{sample}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{p}\PY{p}{,}\PY{n}{trainsize}\PY{p}{,}\PY{n}{replace}\PY{o}{=}\PY{k+kc}{false}\PY{p}{,}\PY{n}{ordered}\PY{o}{=}\PY{k+kc}{true}\PY{p}{)} 
\PY{n}{test\PYZus{}set} \PY{o}{=} \PY{n}{setdiff}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{p}\PY{p}{,}\PY{n}{train\PYZus{}set}\PY{p}{)} 
\PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{train\PYZus{}set}\PY{p}{,}\PY{o}{:}\PY{p}{]}
\PY{n}{Ytrain} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y}\PY{p}{[}\PY{n}{train\PYZus{}set}\PY{p}{]}
\PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{,}\PY{o}{:}\PY{p}{]}
\PY{n}{Ytest} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{]}
\PY{n}{ptrain} \PY{o}{=} \PY{n}{length}\PY{p}{(}\PY{n}{Ytrain}\PY{p}{)}
\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{conjunto} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{p}\PY{p}{)}
\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{conjunto}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{]} \PY{o}{.=} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}
\PY{n}{iris\PYZus{}df}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

    Tendo definido os conjuntos de treino e teste, vamos aplicar a técnica
SVM com margem rígida sobre o conjunto de treino para obter o hiperplano
separador. Para tanto, utilizaremos a função \texttt{SVM\_rigida}
definida no exemplo anterior.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b} \PY{o}{=} \PY{n}{SVM\PYZus{}rigida}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
([-1.9124232444455314e-7, 0.3203662687382003, -0.8237985755072654,
-0.3661327030904415], 1.315789899955701)
\end{Verbatim}
\end{tcolorbox}
        
Agora, vamos avaliar a eficiência do classificador encontrado, isto é,
se ele separa os dados corretamente. Para tanto, tomamos o conjunto de
teste, para o qual já conhecemos sua real classificação, e comparamos
com a classificação predita pelo hiperplano separador. Os resultados
desta comparação podem ser visualizados na matriz de confusão dada a
seguir e a taxa de acerto é dada pela acurácia..

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{20}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Setosa} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Nonsetosa} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{iris\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)} 
\PY{n}{gdf}\PY{p}{[}\PY{n}{Nonsetosa}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Iris}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Iris} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Iris}
\PY{n}{MatrizConfusao\PYZus{}Iris}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Iris = 1.0
    \end{Verbatim}
 
{            
\prompt{Out}{outcolor}{20}{}
    
    \begin{tabular}{ccc}
    Classe & Predita\_Positiva & Predita\_Negativa\\
    \hline
    
    Real Positiva & 36 & 0 \\
    Real Negativa & 0 & 64 \\
\end{tabular}
\vspace{0.5cm}
    }

Assim, como os valores da diagonal secundária são todos nulos temos que
o hiperplano ótimo separou todos os dados do conjunto de teste
corretamente, apresentando uma acurácia de $100\%$.

Portanto, para o problema que considera as quatro características dos
dados amostrais, temos que a técnica SVM com margem rígida nos fornece um
hiperplano que atua como um bom classificador, separando corretamente
todos os dados, tanto de treinamento quanto de teste.

\hypertarget{classificauxe7uxe3o-em-espuxe9cie-virginica-e-nuxe3o-virginica-utilizando-quatro-caracteruxedsticas}{%
\subsubsection{Classificação em espécie virginica e não virginica
utilizando quatro
características}\label{classificauxe7uxe3o-em-espuxe9cie-virginica-e-nuxe3o-virginica-utilizando-quatro-caracteruxedsticas}}

Para finalizar os experimentos numéricos com o conjunto de dados Íris,
vamos implementar neste momento a técnica SVM com margem flexível para
classificar tal conjunto de dados em virginica e não virginica.

Assim como no exemplo anterior, serão consideradas as características
comprimento e largura das sépalas e pétalas, de modo que cada vetor
$x^{i} \in \RR^{4}$. Mas agora, dados da espécie virginica serão
classificados com $y_{i} = 1$ e dados das espécies que não são virginica com
$y_{i} = -1$.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{21}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{X} \PY{o}{=} \PY{n}{convert}\PY{p}{(}\PY{k+kt}{Array}\PY{p}{,}\PY{n}{iris}\PY{p}{[}\PY{o}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{o}{:}\PY{l+m+mi}{4}\PY{p}{]}\PY{p}{)} 
%\PY{n}{p}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{size}\PY{p}{(}\PY{n}{X}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{21}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%(150, 4)
%\end{Verbatim}
%\end{tcolorbox}
        
%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{22}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{iris\PYZus{}df} \PY{o}{=} \PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{;}
%\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y} \PY{o}{=} \PY{p}{[}\PY{n}{species} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{v}\PY{l+s}{i}\PY{l+s}{r}\PY{l+s}{g}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{i}\PY{l+s}{c}\PY{l+s}{a}\PY{l+s}{\PYZdq{}} \PY{o}{?} \PY{l+m+mf}{1.0} \PY{o}{:} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0} \PY{k}{for} \PY{n}{species} \PY{k+kp}{in} \PY{n}{iris}\PY{p}{[}\PY{o}{!}\PY{p}{,}\PY{o}{:}\PY{n}{Species}\PY{p}{]}\PY{p}{]}\PY{p}{;}
%\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Especie} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{Species}
%\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x1} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Comp\PYZus{}sepala}\PY{p}{)}\PY{p}{)}
%\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x2} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Larg\PYZus{}sepala}\PY{p}{)}\PY{p}{)}
%\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x3} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Comp\PYZus{}petala}\PY{p}{)}\PY{p}{)}
%\PY{n}{rename!}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,} \PY{k+kt}{Dict}\PY{p}{(}\PY{o}{:}\PY{n}{x4} \PY{o}{=\PYZgt{}} \PY{o}{:}\PY{n}{Larg\PYZus{}petala}\PY{p}{)}\PY{p}{)}
%\PY{n}{first}\PY{p}{(}\PY{n}{iris\PYZus{}df}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{;}
%\end{Verbatim}
%\end{tcolorbox}

Observando novamente os gráficos apresentados na \Cref{fig:pairplot_dados_iris},
é possível perceber que alguns dados pertencentes às duas classes
distintas (virginica na cor verde e não virginica nas cores azul e
vermelha) acabam se sobrepondo. Logo, intuímos que eles não são
linearmente separáveis. Em vista disso, aplicaremos a técnica SVM com
margem flexível para obter o classificador.

De modo análogo ao que foi desenvolvido anteriormente, é necessário
primeiramente dividir aleatoriamente o conjunto total de dados em dois
subconjuntos: conjunto de treinamento e conjunto de testes. Neste caso,
o conjunto de treinamento será composto por 75 dados e o conjunto de
teste pelos 75 dados restantes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{23}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Random}\PY{o}{.}\PY{n}{seed!}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{trainsize} \PY{o}{=} \PY{l+m+mi}{75} 
\PY{n}{train\PYZus{}set} \PY{o}{=} \PY{n}{sample}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{p}\PY{p}{,}\PY{n}{trainsize}\PY{p}{,}\PY{n}{replace}\PY{o}{=}\PY{k+kc}{false}\PY{p}{,}\PY{n}{ordered}\PY{o}{=}\PY{k+kc}{true}\PY{p}{)}
\PY{n}{test\PYZus{}set} \PY{o}{=} \PY{n}{setdiff}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{p}\PY{p}{,}\PY{n}{train\PYZus{}set}\PY{p}{)} 
\PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{train\PYZus{}set}\PY{p}{,}\PY{o}{:}\PY{p}{]}
\PY{n}{Ytrain} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y}\PY{p}{[}\PY{n}{train\PYZus{}set}\PY{p}{]}
\PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{,}\PY{o}{:}\PY{p}{]}
\PY{n}{Ytest} \PY{o}{=} \PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{Y}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{]}
\PY{n}{ptrain} \PY{o}{=} \PY{n}{length}\PY{p}{(}\PY{n}{Ytrain}\PY{p}{)}
\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{conjunto} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{p}\PY{p}{)}
\PY{n}{iris\PYZus{}df}\PY{o}{.}\PY{n}{conjunto}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{]} \PY{o}{.=} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}
\PY{n}{iris\PYZus{}df}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

Agora, para obter o hiperplano ótimo aplicamos a função \texttt{SVM\_flexivel} com parâmetro $C = 1$.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{24}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
([-0.3456754552444701, -0.8049690813571279, 1.7254845918550477,
1.6080094600267831], -6.676855229902285, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.8965384097436062, 0.0, 4.754013257179834e-9,
0.0, 0.0, 0.0, 0.0])
\end{Verbatim}
\end{tcolorbox}
        
Assim, a solução acima nos fornece o hiperplano ótimo. Observe também
que, de acordo com a solução ótima encontrada, algumas variáveis de
folga $\xi_{i}$ assumiram valores não nulos.

Para verificarmos se tal hiperplano atua como um bom classificador
recorremos à fase de testes. Em vista disso, apresentamos a seguir a
matriz de confusão e a acurácia do hiperplano ótimo encontrado ao
classificar o conjunto de teste.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{25}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Virginica} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Nonvirginica} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{iris\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)} 
\PY{n}{gdf}\PY{p}{[}\PY{n}{Nonvirginica}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Iris}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Iris} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Iris}
\PY{n}{MatrizConfusao\PYZus{}Iris}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Iris = 1.0
    \end{Verbatim}
 
{            
\prompt{Out}{outcolor}{25}{}
    
    \begin{tabular}{ccc}
    Classe & Predita\_Positiva & Predita\_Negativa\\
    \hline
    
    Real Positiva & 23 & 0 \\
    Real Negativa & 0 & 52 \\
\end{tabular}
\vspace{0.5cm}
}

Felizmente, como a diagonal secundária apresenta somente valores nulos,
temos que não ocorreram Falsos Positivos e Falsos Negativos para a
classificação do conjunto de teste. Além disso, calculando a acurácia
para o conjunto de teste obtemos $100\%$ de acerto. Logo, o hiperplano
ótimo encontrado classifica corretamente os dados do conjunto de treino
e de teste, se configurando num ótimo classificador.

    \hypertarget{implementauxe7uxe3o-de-svm-para-classificauxe7uxe3o-de-dados-de-cuxe2ncer-de-mama}{%
\subsection{Implementação de SVM para Classificação de Dados de Câncer
de
Mama}\label{implementauxe7uxe3o-de-svm-para-classificauxe7uxe3o-de-dados-de-cuxe2ncer-de-mama}}

De acordo com o Instituto Nacional de Câncer (INCA, \cite{Inca}), 
\begin{quote}
{\raggedleft Câncer é o nome dado a um conjunto de mais de 100 doenças que têm em comum o crescimento desordenado de células, que invadem tecidos e órgãos.
Dividindo-se rapidamente, estas células tendem a ser muito agressivas incontroláveis, determinando a formação de tumores, que podem espalhar-se para outras regiões do corpo.}
\end{quote}

O câncer de mama por sua
vez ocorre quando há formação de tumor na mama, sendo o tipo de câncer
mais incidente entre as mulheres, tanto no Brasil quanto no mundo. Segundo estimativas \cite{Inca,Femama,SocBraMastologia}, no
mundo, somente em 2018, foram registrados cerca de 2,1 milhões de novos
casos, número que equivale a 11,6\% de todos os cânceres estimados. No
Brasil, em 2017, ocorreram 16.724 óbitos por câncer de mama e, segundo
estimativas do INCA \cite{Inca}, são estimados cerca de 66.280 novos casos de câncer
de mama no Brasil para cada ano do triênio 2020-2022, o que corresponde a
uma taxa de incidência de 61,61 novos casos a cada 100 mil mulheres.

O diagnóstico precoce é um dos principais fatores que contribuem para
reduzir a mortalidade por câncer, possibilitando cerca de 95\% de
chances de cura. Neste contexto, 
\begin{quote}
{\flushright A mamografia é uma das melhores
técnicas para o rastreamento do câncer de mama disponível atualmente,
capaz de registrar imagens da mama com a finalidade de diagnosticar a
presença ou ausência de estruturas que possam indicar a doença. Com esse
tipo de exame pode-se detectar o tumor antes que ele se torne palpável. \textcite[p. 229]{Silva&Leal&Lima}}   
\end{quote}

Tendo em vista as altas taxas
de incidência e mortes causadas pelo câncer de mama, muitas pesquisas
científicas vem sendo desenvolvidas nos últimos anos com o intuito de
auxiliar no diagnósticos de doenças, tornando as técnicas de
aprendizagem de máquina cada vez mais presentes na área médica.

Visto que este tema é de grande relevância e interesse, o próximo experimento numérico será realizado com um conjunto de dados sobre células de câncer de mama retirados do \emph{UC Irvine Machine Learning
Repository} \cite{Dua:2019}. Nosso objetivo será utilizar a técnica SVM com margem
flexível para classificar estes dados em tumores malignos ou benignos.

Este conjunto é composto por 569 dados, em que cada um possui um número
de identidade (ID), sua classificação em tumor maligno (M) ou benigno
(B) e 30 características acerca de núcleos celulares presentes em
imagens digitalizadas de um aspirado por agulha fina (do inglês
\emph{fine needle aspirate}, FNA), de uma massa mamária. As
características extraídas dos núcleos celulares são:

\begin{itemize}
\item
  Raio (média das distâncias do centro aos pontos do perímetro);
\item
  Textura (desvio padrão dos valores da escala cinza);
\item
  Perímetro;
\item
  Área;
\item
  Suavidade (variação local nos comprimentos do raio);
\item
  Compactação $\left( \dfrac{\text{perímetro}^{2}}{\text{área} - 1.0} \right) $;
\item
  Concavidade (severidade das porções côncavas do contorno);
\item
  Pontos côncavos (número de partes côncavas do contorno);
\item
  Simetria;
\item
  Dimensão fractal (``aproximação da costa'' $- 1$).
\end{itemize}

Para cada imagem foram calculadas a média, o desvido padrão e o ``pior''
ou maior (média dos três maiores valores) dos valores atribuídos a cada
um dos atributos acima mencionados, resultando nas 30 características da
tabela a seguir.

\vspace{0.5cm}
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{26}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{col\PYZus{}headers} \PY{o}{=} \PY{p}{[}\PY{l+s}{\PYZdq{}}\PY{l+s}{i}\PY{l+s}{d}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{d}\PY{l+s}{i}\PY{l+s}{a}\PY{l+s}{g}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{s}\PY{l+s}{i}\PY{l+s}{s}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{r}\PY{l+s}{a}\PY{l+s}{d}\PY{l+s}{i}\PY{l+s}{u}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{x}\PY{l+s}{t}\PY{l+s}{u}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{p}\PY{l+s}{e}\PY{l+s}{r}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{r}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{a}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{s}\PY{l+s}{m}\PY{l+s}{o}\PY{l+s}{o}\PY{l+s}{t}\PY{l+s}{h}\PY{l+s}{n}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{m}\PY{l+s}{p}\PY{l+s}{a}\PY{l+s}{c}\PY{l+s}{t}\PY{l+s}{n}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{c}\PY{l+s}{a}\PY{l+s}{v}\PY{l+s}{i}\PY{l+s}{t}\PY{l+s}{y}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{c}\PY{l+s}{a}\PY{l+s}{v}\PY{l+s}{e}\PY{l+s}{ }\PY{l+s}{p}\PY{l+s}{o}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{t}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{s}\PY{l+s}{y}\PY{l+s}{m}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{y}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{f}\PY{l+s}{r}\PY{l+s}{a}\PY{l+s}{c}\PY{l+s}{t}\PY{l+s}{a}\PY{l+s}{l}\PY{l+s}{\PYZus{}}\PY{l+s}{d}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{n}\PY{l+s}{s}\PY{l+s}{i}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{\PYZus{}}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{r}\PY{l+s}{a}\PY{l+s}{d}\PY{l+s}{i}\PY{l+s}{u}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{x}\PY{l+s}{t}\PY{l+s}{u}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{p}\PY{l+s}{e}\PY{l+s}{r}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{r}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{a}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{s}\PY{l+s}{m}\PY{l+s}{o}\PY{l+s}{o}\PY{l+s}{t}\PY{l+s}{h}\PY{l+s}{n}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{m}\PY{l+s}{p}\PY{l+s}{a}\PY{l+s}{c}\PY{l+s}{t}\PY{l+s}{n}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{c}\PY{l+s}{a}\PY{l+s}{v}\PY{l+s}{i}\PY{l+s}{t}\PY{l+s}{y}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{c}\PY{l+s}{a}\PY{l+s}{v}\PY{l+s}{e}\PY{l+s}{ }\PY{l+s}{p}\PY{l+s}{o}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{t}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{s}\PY{l+s}{y}\PY{l+s}{m}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{y}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{f}\PY{l+s}{r}\PY{l+s}{a}\PY{l+s}{c}\PY{l+s}{t}\PY{l+s}{a}\PY{l+s}{l}\PY{l+s}{\PYZus{}}\PY{l+s}{d}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{n}\PY{l+s}{s}\PY{l+s}{i}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{\PYZus{}}\PY{l+s}{s}\PY{l+s}{e}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{r}\PY{l+s}{a}\PY{l+s}{d}\PY{l+s}{i}\PY{l+s}{u}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{x}\PY{l+s}{t}\PY{l+s}{u}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{p}\PY{l+s}{e}\PY{l+s}{r}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{r}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{a}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{a}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{s}\PY{l+s}{m}\PY{l+s}{o}\PY{l+s}{o}\PY{l+s}{t}\PY{l+s}{h}\PY{l+s}{n}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{m}\PY{l+s}{p}\PY{l+s}{a}\PY{l+s}{c}\PY{l+s}{t}\PY{l+s}{n}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{c}\PY{l+s}{a}\PY{l+s}{v}\PY{l+s}{i}\PY{l+s}{t}\PY{l+s}{y}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{c}\PY{l+s}{a}\PY{l+s}{v}\PY{l+s}{e}\PY{l+s}{ }\PY{l+s}{p}\PY{l+s}{o}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{t}\PY{l+s}{s}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{s}\PY{l+s}{y}\PY{l+s}{m}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{y}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}} \PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{f}\PY{l+s}{r}\PY{l+s}{a}\PY{l+s}{c}\PY{l+s}{t}\PY{l+s}{a}\PY{l+s}{l}\PY{l+s}{\PYZus{}}\PY{l+s}{d}\PY{l+s}{i}\PY{l+s}{m}\PY{l+s}{e}\PY{l+s}{n}\PY{l+s}{s}\PY{l+s}{i}\PY{l+s}{o}\PY{l+s}{n}\PY{l+s}{\PYZus{}}\PY{l+s}{w}\PY{l+s}{o}\PY{l+s}{r}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{\PYZdq{}}\PY{p}{]}
\PY{n}{cancer\PYZus{}df} \PY{o}{=} \PY{n}{CSV}\PY{o}{.}\PY{n}{read}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{c}\PY{l+s}{a}\PY{l+s}{n}\PY{l+s}{c}\PY{l+s}{e}\PY{l+s}{r}\PY{l+s}{\PYZus{}}\PY{l+s}{d}\PY{l+s}{a}\PY{l+s}{t}\PY{l+s}{a}\PY{l+s}{.}\PY{l+s}{c}\PY{l+s}{s}\PY{l+s}{v}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{header} \PY{o}{=} \PY{n}{col\PYZus{}headers}\PY{p}{,}\PY{p}{)}
\PY{n}{cancer\PYZus{}df}\PY{o}{.}\PY{n}{Y} \PY{o}{=} \PY{p}{[}\PY{n}{diagnosis} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{M}\PY{l+s}{\PYZdq{}} \PY{o}{?} \PY{l+m+mf}{1.0} \PY{o}{:} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1.0} \PY{k}{for} \PY{n}{diagnosis} \PY{k+kp}{in} \PY{n}{cancer\PYZus{}df}\PY{p}{[}\PY{o}{!}\PY{p}{,}\PY{o}{:}\PY{n}{diagnosis}\PY{p}{]}\PY{p}{]}\PY{p}{;}
\PY{n}{first}\PY{p}{(}\PY{n}{cancer\PYZus{}df}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}
 {
             
 \prompt{Out}{outcolor}{26}{}
}     
%\begin{landscape}
\begin{table}[h]
    \centering
    {\footnotesize  \begin{tabular}{ccccccc}
     \hline
     id & diagnosis & radius\_mean & texture\_mean & perimeter\_mean & area\_mean & smoothness\_mean \\
     \hline
     
    842302 & M & 17.99 & 10.38 & 122.8 & 1001.0 & 0.1184  \\
    842517 & M & 20.57 & 17.77 & 132.9 & 1326.0 & 0.08474  \\
    843	00903 & M & 19.69 & 21.25 & 130.0 & 1203.0 & 0.1096  \\
    84348301 & M & 11.42 & 20.38 & 77.58 & 386.1 & 0.1425  \\
    84358402 & M & 20.29 & 14.34 & 135.1 & 1297.0 & 0.1003  \\
    843786 & M & 12.45 & 15.7 & 82.57 & 477.1 & 0.1278  \\
    844359 & M & 18.25 & 19.98 & 119.6 & 1040.0 & 0.09463  \\
    84458202 & M & 13.71 & 20.83 & 90.2 & 577.9 & 0.1189  \\
    844981 & M & 13.0 & 21.82 & 87.5 & 519.8 & 0.1273  \\
    84501001 & M & 12.46 & 24.04 & 83.97 & 475.9 & 0.1186  \\ 
     \hline
 \end{tabular}
 }
    %\caption{Caption}
    \label{tab:tabela_dados_cancer}
\end{table}
 
%\end{landscape}
    
A seguir, na \Cref{fig:pairplot_dados_cancer}, temos a representação gráfica destes dados considerando-se
algumas das características mencionadas. Nestes gráficos, os pontos em
azul correspondem aos dados que pertencem à classe benigna e os dados em
vermelho à classe maligna.

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{27}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{plt1} \PY{o}{=} \PY{n}{pairplot}\PY{p}{(}\PY{n}{select}\PY{p}{(}\PY{n}{cancer\PYZus{}df}\PY{p}{,}\PY{l+m+mi}{2}\PY{o}{:}\PY{l+m+mi}{7}\PY{p}{)}\PY{p}{,}\PY{o}{:}\PY{n}{diagnosis}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}
 
{ % \prompt{Out}{outcolor}{27}{}
        \begin{figure}
            \centering
             \includegraphics[width=0.95\textwidth]{Implementacao_de_SVM_em_Julia_TCC_files/Implementacao_de_SVM_em_Julia_TCC_54_0.pdf}
            \caption{Conjunto de dados de células de câncer de mama.}
            \label{fig:pairplot_dados_cancer}
        \end{figure}
    }    

%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{28}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{X} \PY{o}{=} \PY{n}{convert}\PY{p}{(}\PY{k+kt}{Array}\PY{p}{,}\PY{n}{cancer\PYZus{}df}\PY{p}{[}\PY{o}{:}\PY{p}{,} \PY{l+m+mi}{3}\PY{o}{:}\PY{l+m+mi}{32}\PY{p}{]}\PY{p}{)} 
%\PY{n}{p}\PY{p}{,} \PY{n}{n} \PY{o}{=} \PY{n}{size}\PY{p}{(}\PY{n}{X}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

%           \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{28}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%(569, 30)
%\end{Verbatim}
%\end{tcolorbox}
        
Neste exemplo, cada dado $x^{i}$ do conjunto de entrada pertence ao $\RR^{30}$, pois são $30$ as características. Ademais, os dados
diagnosticados como malignos (M) serão classificados como $y_{i} = 1$,
e os dados com diagnóstico benigno serão classificados com
$y_{i} = -1$.

Com base nos gráficos apresentados anteriormente na \Cref{fig:pairplot_dados_cancer}, em que apenas duas características foram consideradas em cada
vez, é possível intuir que os dados não são linearmente separáveis. Em
vista disso, vamos aplicar a técnica SVM com margem flexível para
classificar os dados deste conjunto em câncer Maligno ou Benigno e então
discutir os resultados encontrados.

Dos 569 dados do conjunto de entrada, 150 dados foram direcionados ao
conjunto de treinamento e os 419 restantes ao conjunto de teste.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{29}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Random}\PY{o}{.}\PY{n}{seed!}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{trainsize} \PY{o}{=} \PY{l+m+mi}{150} 
\PY{n}{train\PYZus{}set} \PY{o}{=} \PY{n}{sample}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{p}\PY{p}{,}\PY{n}{trainsize}\PY{p}{,}\PY{n}{replace}\PY{o}{=}\PY{k+kc}{false}\PY{p}{,}\PY{n}{ordered}\PY{o}{=}\PY{k+kc}{true}\PY{p}{)} 
\PY{n}{test\PYZus{}set} \PY{o}{=} \PY{n}{setdiff}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{:}\PY{n}{p}\PY{p}{,}\PY{n}{train\PYZus{}set}\PY{p}{)} 
\PY{n}{Xtrain} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{train\PYZus{}set}\PY{p}{,}\PY{o}{:}\PY{p}{]}
\PY{n}{Ytrain} \PY{o}{=} \PY{n}{cancer\PYZus{}df}\PY{o}{.}\PY{n}{Y}\PY{p}{[}\PY{n}{train\PYZus{}set}\PY{p}{]}
\PY{n}{Xtest} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{,}\PY{o}{:}\PY{p}{]}
\PY{n}{Ytest} \PY{o}{=} \PY{n}{cancer\PYZus{}df}\PY{o}{.}\PY{n}{Y}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{]}
\PY{n}{ptrain} \PY{o}{=} \PY{n}{length}\PY{p}{(}\PY{n}{Ytrain}\PY{p}{)}
\PY{n}{cancer\PYZus{}df}\PY{o}{.}\PY{n}{conjunto} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{r}\PY{l+s}{e}\PY{l+s}{i}\PY{l+s}{n}\PY{l+s}{o}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{p}\PY{p}{)}
\PY{n}{cancer\PYZus{}df}\PY{o}{.}\PY{n}{conjunto}\PY{p}{[}\PY{n}{test\PYZus{}set}\PY{p}{]} \PY{o}{.=} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}
\PY{n}{cancer\PYZus{}df}\PY{p}{;}
\end{Verbatim}
\end{tcolorbox}

Utilizando a função \texttt{SVM\_flexivel} criada com base na modelagem matemática desenvolvida na \Cref{subsection:SVM_margem_flexivel}, determinamos o hiperplano ótimo para os seguintes valores para o parâmetro $C$: $10^{-3}, 10^{-2}, 10^{-1} , 1, 10, 10^{2}, 10^{3}$ e $10^{4}$. Para
cada solução encontrada, apresentamos a matriz de confusão e a acurácia
da classificação do conjunto de teste.

\vspace{0.5cm}
  {  \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mf}{10e\PYZhy{}4}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{30}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%([-0.0007584283177882081, 0.024749817454506776, 0.009822094827128402,
%-0.011282507259653948, 0.00013295432105445867, 0.00041548982512510264,
%0.0006312360235894728, 0.00024384420872600567, 0.0002760113401957554,
%4.665038877030037e-5  …  0.0005756889944849921, 0.0461925021432539,
%0.024724367567079457, 0.011565605684616741, 0.0002859685759089533,
%0.0012359065820270936, 0.0014209902266727212, 0.0005180570571382945,
%0.0006807587758263868, 0.00016070250122321433], -8.404759750812234,
%[2.4959040020823596e-6, 2.4959043618068177e-6, 2.4959125710341423e-6,
%1.9276303108231725, 2.4959039350994583e-6, 2.4959039058835335e-6,
%2.495906246322225e-6, 1.2613350516644815, 2.495907890741904e-6,
%2.495316494429246e-6  …  2.495904539290505e-6, 2.49590568960438e-6,
%0.19402238303058858, 2.49590551951686e-6, 2.4959053464691815e-6,
%2.4959096108090774e-6, 2.4959059689534175e-6, 2.4959053893703754e-6,
%2.495910115702953e-6, 2.518615174704152e-6])
%\end{Verbatim}
%\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{31}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Maligno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Benigno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{cancer\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Benigno}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Cancer1}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Cancer} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Cancer1}
\PY{n}{MatrizConfusao\PYZus{}Cancer}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Cancer1 = 0.9284009546539379
    \end{Verbatim}
 
            
\prompt{Out}{outcolor}{31}{}
    
    \begin{tabular}{ccc}
	Classe & Predita\_Positiva & Predita\_Negativa\\
	\hline
	
	Real Positiva & 134 & 26 \\
	Real Negativa & 4 & 255 \\
\end{tabular}
\vspace{0.5cm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{32}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mf}{10e\PYZhy{}3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{32}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%([-0.00807183454616408, 0.029645161704839523, 0.05439311212608334,
%-0.011592597530669189, 0.0010117589836746398, 0.003059179554266445,
%0.004690942167627082, 0.001831377789000468, 0.00233941318614483,
%0.0003206848554581692  …  0.0008173819959715592, 0.07826525542595365,
%0.09129580035637944, 0.005833410145061638, 0.00209067625247758,
%0.008699190259215592, 0.010333835136872381, 0.003564572710587319,
%0.00579540977912881, 0.0009855438257824064], -15.648973824054961,
%[8.090908928375011e-8, 8.090908690357925e-8, 8.090902577159509e-8,
%1.515171798298554, 8.090908937916708e-8, 8.090908950356853e-8,
%8.090908229597614e-8, 1.12770750004926, 8.090908126371524e-8,
%1.2533920792607554e-7  …  8.090908705997125e-8, 8.090908560549803e-8,
%8.090894497521895e-8, 8.090908643441776e-8, 8.090908688233132e-8,
%8.090906837019457e-8, 8.090908456189375e-8, 8.090908662184399e-8,
%8.090895727226532e-8, 0.30469823356286685])
%\end{Verbatim}
%\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Maligno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Benigno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{cancer\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Benigno}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Cancer2}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Cancer} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Cancer2}
\PY{n}{MatrizConfusao\PYZus{}Cancer}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Cancer2 = 0.9451073985680191
    \end{Verbatim}
            
\prompt{Out}{outcolor}{33}{}
    
    \begin{tabular}{ccc}
	Classe & Predita\_Positiva & Predita\_Negativa\\
	\hline
	
	Real Positiva & 143 & 17 \\
	Real Negativa & 6 & 253 \\
\end{tabular}
\vspace{0.5cm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{34}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mf}{10e\PYZhy{}2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{34}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%([-0.0888385993594153, -0.0007301764450360921, 0.11196686858229432,
%-0.015880476369110618, 0.0077308924966434205, 0.018283210568903693,
%0.031846521211455495, 0.012830609393021906, 0.01907002373044082,
%0.0014135096052743012  …  -0.020531240865719197, 0.1513050861013814,
%0.16282498544526955, 0.002965626781945999, 0.015498496314976474,
%0.05406193808794571, 0.06897533125709271, 0.02420806592934621,
%0.050462179986322554, 0.004051358075620071], -23.61668517011338,
%[1.5059034230248166e-8, 1.5059027840153314e-8, 1.5059013504676282e-8,
%1.4993461320183272, 1.5059034010093344e-8, 1.5059034338182133e-8,
%1.505902765553629e-8, 0.8223470172586039, 1.505902766103012e-8,
%3.245062406322756e-8  …  1.5059030618342077e-8, 1.505903134657942e-8,
%1.5058982821818752e-8, 1.505903210903274e-8, 1.5059032468137176e-8,
%1.5059015417531872e-8, 1.505903027745674e-8, 1.50590322278154e-8,
%1.505789454213542e-8, 0.36972957775112136])
%\end{Verbatim}
%\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{35}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Maligno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Benigno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{cancer\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Benigno}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Cancer3}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Cancer} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Cancer3}
\PY{n}{MatrizConfusao\PYZus{}Cancer}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Cancer3 = 0.9474940334128878
    \end{Verbatim}
 
            
\prompt{Out}{outcolor}{35}{}
    
    \begin{tabular}{ccc}
	Classe & Predita\_Positiva & Predita\_Negativa\\
	\hline
	
	Real Positiva & 144 & 16 \\
	Real Negativa & 6 & 253 \\
\end{tabular}
\vspace{0.5cm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mf}{1.0}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{36}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%([-0.7729017246884616, -0.14065863740581222, 0.3284135282439728,
%-0.023165676695899372, 0.06467371096380567, 0.14063135523300033,
%0.261398355613183, 0.10085914480945435, 0.15929257739876304,
%0.008169698828665226  …  -0.20617919164135334, 0.3305765154420798,
%0.18364189309897133, 0.006979634425618022, 0.12293945576818556,
%0.4456773049378063, 0.5314645199933372, 0.17828478933449102, 0.4722469677605429,
%0.030492652655001107], -33.34340978921013, [0.0, 0.0, 0.0, 1.173797661723894,
%0.0, 0.0, 0.0, 0.28667623877021237, 0.0, 7.891659839422233e-9  …  0.0, 0.0, 0.0,
%0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
%\end{Verbatim}
%\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Maligno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Benigno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{cancer\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Benigno}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Cancer4}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Cancer} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Cancer4}
\PY{n}{MatrizConfusao\PYZus{}Cancer}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Cancer4 = 0.954653937947494
    \end{Verbatim}
 
            
\prompt{Out}{outcolor}{37}{}
    
    \begin{tabular}{ccc}
	Classe & Predita\_Positiva & Predita\_Negativa\\
	\hline
	
	Real Positiva & 151 & 9 \\
	Real Negativa & 10 & 249 \\
\end{tabular}
\vspace{0.5cm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{38}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{38}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%([-3.648776243632741, -0.29959521673733774, 0.5274961875893085,
%0.0012838659550483457, 0.31330786463628363, 0.6543943804337327,
%1.033880518051846, 0.42760649864954947, 0.9735932658298139,
%-0.006822715279178642  …  -1.5067614807593261, 0.5207062274097408,
%0.08134002685739974, 0.018028717775124204, 0.7427837309049196,
%1.2626847689040062, 1.7414994379041866, 1.0445809691645265, 2.9708814031073394,
%0.04484728066249232], -4.738710351251349, [0.0, 0.0, 0.0, 0.6794234691449968,
%0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
%0.0])
%\end{Verbatim}
%\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{39}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Maligno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Benigno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{cancer\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Benigno}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Cancer5}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Cancer} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Cancer5}
\PY{n}{MatrizConfusao\PYZus{}Cancer}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Cancer5 = 0.9618138424821002
    \end{Verbatim}
 
            
\prompt{Out}{outcolor}{39}{}
    
    \begin{tabular}{ccc}
	Classe & Predita\_Positiva & Predita\_Negativa\\
	\hline
	
	Real Positiva & 153 & 7 \\
	Real Negativa & 9 & 250 \\
\end{tabular}
\vspace{0.5cm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{40}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mf}{10e1}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{40}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%([-10.399808449871003, -0.5900061147360639, 0.7872795210500106,
%0.06187996412575949, 2.057026937218021, 2.0193520960026095, 3.6825126010562954,
%2.0213907026966615, 2.044453763802937, -0.4367825977363361  …
%2.027399546550189, 1.093645039766674, 0.3479696975387944, -0.039436117524285705,
%3.5528705359144346, 1.1156727117988485, -0.6844515145653299, 2.3460191468328078,
%6.112286759070957, -0.907583497535017], -16.708195506553967, [0.0, 0.0, 0.0,
%0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
%0.0, 0.0])
%\end{Verbatim}
%\end{tcolorbox}
        
%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{41}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{norm}\PY{p}{(}\PY{n}{ξ}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{41}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%0.3739366563914939
%\end{Verbatim}
%\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{42}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Maligno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Benigno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{cancer\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Benigno}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Cancer6}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Cancer} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Cancer6}
\PY{n}{MatrizConfusao\PYZus{}Cancer}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Cancer6 = 0.954653937947494
    \end{Verbatim}
 
            
\prompt{Out}{outcolor}{42}{}
    
    \begin{tabular}{ccc}
	Classe & Predita\_Positiva & Predita\_Negativa\\
	\hline
	
	Real Positiva & 150 & 10 \\
	Real Negativa & 9 & 250 \\
\end{tabular}
\vspace{0.5cm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mf}{10e2}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{43}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%([-11.728964719304933, -0.6531286380075365, 0.7857005039636348,
%0.07695537025813758, 2.3008892175891913, 2.2639524204024464, 4.165137374759066,
%2.3012285516249817, 2.2592551232672236, -0.5087724179919592  …
%2.7732654840040203, 1.21899738495789, 0.4555928575164795, -0.0545750115016787,
%3.971279700279385, 1.0295346824207534, -1.0368912700484516, 2.6204037204964554,
%6.95396797731379, -1.0591624084846365], -20.12810504457573, [0.0, 0.0, 0.0, 0.0,
%0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
%0.0])
%\end{Verbatim}
%\end{tcolorbox}
        
%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{44}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{norm}\PY{p}{(}\PY{n}{ξ}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{44}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%0.0
%\end{Verbatim}
%\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{45}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Maligno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Benigno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{cancer\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Benigno}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Cancer7}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Cancer} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Cancer7}
\PY{n}{MatrizConfusao\PYZus{}Cancer}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Cancer7 = 0.9522673031026253
    \end{Verbatim}
 
            
\prompt{Out}{outcolor}{45}{}
    
    \begin{tabular}{ccc}
	Classe & Predita\_Positiva & Predita\_Negativa\\
	\hline
	
	Real Positiva & 149 & 11 \\
	Real Negativa & 9 & 250 \\
\end{tabular}
\vspace{0.5cm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{w}\PY{p}{,} \PY{n}{b}\PY{p}{,} \PY{n}{ξ} \PY{o}{=} \PY{n}{SVM\PYZus{}flexivel}\PY{p}{(}\PY{n}{n}\PY{p}{,} \PY{n}{ptrain}\PY{p}{,} \PY{n}{Xtrain}\PY{p}{,} \PY{n}{Ytrain}\PY{p}{,} \PY{l+m+mf}{10e3}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{46}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%([-11.728964150295672, -0.6531283956251774, 0.785700394537014,
%0.07695536854528762, 2.3008886785932074, 2.2639527719715997, 4.1651379691945705,
%2.3012284565415895, 2.259255049240005, -0.5087721011621773  …
%2.7732665653763204, 1.2189972289698063, 0.4555928463105896,
%-0.05457501831400019, 3.9712790196973615, 1.0295369295763481,
%-1.0368898026409277, 2.6204027600116238, 6.9539703707749085,
%-1.0591615387821254], -20.128115472989027, [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,
%0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])
%\end{Verbatim}
%\end{tcolorbox}
        
%    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
%\prompt{In}{incolor}{47}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%\PY{n}{norm}\PY{p}{(}\PY{n}{ξ}\PY{p}{)}
%\end{Verbatim}
%\end{tcolorbox}

%            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
%\prompt{Out}{outcolor}{47}{\boxspacing}
%\begin{Verbatim}[commandchars=\\\{\}]
%0.0
%\end{Verbatim}
%\end{tcolorbox}
        
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{48}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Maligno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZgt{}=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{Benigno} \PY{o}{=} \PY{n}{findall}\PY{p}{(}\PY{n}{Xtest}\PY{o}{*}\PY{n}{w} \PY{o}{.+} \PY{n}{b} \PY{o}{.\PYZlt{}} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{gdf} \PY{o}{=} \PY{n}{filter}\PY{p}{(}\PY{o}{:}\PY{n}{conjunto} \PY{o}{=\PYZgt{}} \PY{n}{x} \PY{o}{\PYZhy{}}\PY{o}{\PYZgt{}} \PY{n}{x} \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{s}\PY{l+s}{t}\PY{l+s}{e}\PY{l+s}{\PYZdq{}}\PY{p}{,} \PY{n}{cancer\PYZus{}df}\PY{p}{)} 
\PY{n}{gdf}\PY{o}{.}\PY{n}{Ypredito} \PY{o}{=} \PY{n}{fill}\PY{p}{(}\PY{l+m+mf}{1.}\PY{p}{,} \PY{n}{p} \PY{o}{\PYZhy{}} \PY{n}{ptrain}\PY{p}{)}
\PY{n}{gdf}\PY{p}{[}\PY{n}{Benigno}\PY{p}{,} \PY{o}{:}\PY{n}{Ypredito}\PY{p}{]} \PY{o}{.=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
\PY{n}{Acuracia\PYZus{}Cancer8}\PY{p}{,} \PY{n}{MatrizConfusao\PYZus{}Cancer} \PY{o}{=} \PY{n}{MatrizConfusao}\PY{p}{(}\PY{n}{gdf}\PY{p}{)}\PY{p}{;}
\PY{n+nd}{@show} \PY{n}{Acuracia\PYZus{}Cancer8}
\PY{n}{MatrizConfusao\PYZus{}Cancer}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Acuracia\_Cancer8 = 0.9522673031026253
    \end{Verbatim}
 
            
\prompt{Out}{outcolor}{48}{}
    
    \begin{tabular}{ccc}
	Classe & Predita\_Positiva & Predita\_Negativa\\
	\hline
	
	Real Positiva & 149 & 11 \\
	Real Negativa & 9 & 250 \\
\end{tabular}
\vspace{0.5cm}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{49}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{Resultados} \PY{o}{=} \PY{n}{DataFrame}\PY{p}{(}\PY{n}{C} \PY{o}{=} \PY{p}{[}\PY{l+m+mf}{10e\PYZhy{}4} \PY{p}{,} \PY{l+m+mf}{10e\PYZhy{}3}\PY{p}{,} \PY{l+m+mf}{10e\PYZhy{}2} \PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mf}{10e1}\PY{p}{,} \PY{l+m+mf}{10e2}\PY{p}{,} \PY{l+m+mf}{10e3}\PY{p}{]}\PY{p}{,}
                           \PY{n}{Acurácia} \PY{o}{=} \PY{p}{[}\PY{n}{Acuracia\PYZus{}Cancer1}\PY{p}{,} \PY{n}{Acuracia\PYZus{}Cancer2}\PY{p}{,} \PY{n}{Acuracia\PYZus{}Cancer3}\PY{p}{,} 
                                       \PY{n}{Acuracia\PYZus{}Cancer4}\PY{p}{,} \PY{n}{Acuracia\PYZus{}Cancer5}\PY{p}{,} \PY{n}{Acuracia\PYZus{}Cancer6}\PY{p}{,} 
                                       \PY{n}{Acuracia\PYZus{}Cancer7}\PY{p}{,} \PY{n}{Acuracia\PYZus{}Cancer8}\PY{p}{]} 
                           \PY{p}{)}
\end{Verbatim}
\end{tcolorbox}
 
          
\prompt{Out}{outcolor}{49}{}
    
    \begin{tabular}{c|c}
    C & Acurácia\\
    \hline
    0.001 & 0.928401 \\
    0.01 & 0.945107 \\
    0.1 & 0.947494 \\
    1.0 & 0.954654 \\
    10.0 & 0.961814 \\
    100.0 & 0.954654 \\
    1000.0 & 0.952267 \\
    10000.0 & 0.952267 \\
\end{tabular}
\vspace{0.5cm}
    }

Observe que, conforme diferentes valores são atribuídos ao parâmetro
$C$ a solução ótima também varia, porém nenhuma delas apresenta $100\%$ de acerto na classificação dos dados de teste. Em vista disso, para
escolhermos o melhor hiperplano classificador dentre os hiperplanos
calculados, é necessário analisar qual é o mais eficiente. Para realizar
essa escolha é preciso compreender que no contexto do nosso problema o
classificador que desejamos obter fornecerá o diagnóstico de câncer de
mama em maligno ou benigno. Desse modo, além de possuir uma alta
acurácia, é de suma importância que o classificador apresente uma baixa
quantidade de falsos negativos, pois neste caso uma pessoa com câncer
maligno receberia um diagnóstico benigno, comprometendo seu tratamento
médico.

Assim, a tabela ``Resultados'' acima apresenta na primeira coluna os
diferentes valores atribuídos ao parâmetro $C$ e, na segunda coluna, a
respectiva acurácia na classificação dos dados de teste pelo hiperplano
ótimo encontrado em cada caso.

Primeiramente, analisando as soluções ótimas encontradas, notamos que
conforme o parâmetro $C$ aumenta, a quantidade de variáveis de folga
não nulas diminui. Isso também implica que quanto maior o parâmetro
$C$, maior é a penalização sobre as variáveis de folga e, portanto,
menor a margem de separação. Tanto é que a partir de $C$ próximo a $100$
as variáveis de folga passam a ser todas nulas, pois neste caso temos
$\Vert \xi \Vert = 0$.

Por conseguinte, com base na tabela ``Resultados'', os parâmetros
$C = 10^{3}$ e $C = 10^{4}$ apresentam a mesma acurácia de 95,22\%,
sugerindo que as soluções encontradas são muito próximas. Com efeito,
calculando a norma da diferença entre a solução $w$ encontrada para
cada um desses parâmetros obtemos um valor muito pequeno.

Por fim, temos que o parâmetro $C = 10$ determina a solução com maior
acurácia, $96,18\%$, e menor quantidade de falsos negativos, que totalizam
7. Enquanto que para as demais soluções a acurácia varia entre $92,84\%$ e
95,46\% e a quantidade de falsos negativos fica entre 9 e 26. Portanto,
tomando $C = 10$ temos, dentre as escolhas de $C$, o classificador
que fornece o melhor diagnóstico para os dados de câncer.

Analisando os experimentos numéricos realizados neste capítulo concluímos que a técnica SVM, tanto com margem rígida quanto com margem flexível, apresenta resultados satisfatórios na classificação de dados. A implementação da técnica para classificação do conjunto de dados Íris foi realizado com fins mais didáticos, através da qual foi possível compreender a atuação prática da técnica SVM e suas particularidades, além de visualizar as etapas (treinamento e teste) que são características das técnicas de aprendizagem supervisionada. Posteriormente, a aplicação da técnica SVM com margem flexível na classificação de dados de células de câncer de mama em tumor maligno e benigno consiste num problema prático. Neste experimento, percebemos que a eficiência do classificador encontrado depende da escolha do parâmetro de penalização $C$, escolha esta que nem sempre é trivial.

\newpage
\section{Conclusões}

Este projeto teve como principal objetivo apresentar uma análise teórica matemática, do ponto de vista da otimização, da técnica Máquinas de Vetores Suporte (SVM) aplicada à classificação binária de dados \cite{Evelin2017,Faisal2019}. 
%A SVM é uma técnica de aprendizado de máquina supervisionada e se destaca por apresentar uma boa fundamentação teórica \cite{Vladimir1992,Vladimir1995}.

%O No capítulo 1 apresentamos definições e resultados da teoria de otimização irrestrita e também com  restrições lineares, pois o principal problema proveniente da formulação da técnica SVM é um problema de otimização com restrições lineares. Assim, visando uma abordagem didática definimos os conceitos iniciais que permeiam um problema de otimização e demonstramos para cada caso as condições de otimalidade, dentre as quais as condições de Karush-Kuhn-Tucker.

%No capítulo 2 abordamos tópicos da teoria de otimização convexa, que fornece resultados importantes aos problemas de otimização, como a garantia, sob a hipótese de convexidade, que um minimizador local é global e condições necessárias tornam-se suficientes.

%No capítulo 3, utilizando o referencial teórico dos capítulos anteriores, desenvolvemos a formulação matemática da técnica SVM com margem rígida, aplicada quando os dados são linearmente separáveis, e SVM com margem flexível, caso em que os dados não são linearmente separáveis e a obtenção do classificador se dá promovendo um relaxamento nas restrições através de variáveis de folga $\xi_{i^}$. Com base nas demonstrações apresentadas por \textcite{Evelin2017}, garantimos que o problema formulado para a técnica SVM tem solução, e no caso em que os dados são linearmente separáveis esta solução é única.

Para tanto, vimos que a formulação matemática da técnica SVM para classificação se concentra em obter um hiperplano que melhor separa os dados em duas classes, de modo a possibilitar a máxima margem de separação. A partir disso, derivamos o problema de otimização com restrições lineares cuja solução fornece o hiperplano separador que atuará como classificador para novos dados. Esta abordagem é um dos fatores que torna a técnica SVM tão interessante, pois permite pensar a aprendizagem de máquina supervisionada baseada em argumentos geométricos, e não somente estatísticos como em outras técnicas.

Ademais, verificamos na \Cref{chap:maquinas_vetores_suporte} que o problema empregado pela técnica SVM na fase de treinamento para determinar o hiperplano separador é um problema de programação quadrática convexa com restrições lineares. Tendo isso em vista, na \Cref{chap:conceitos_de_otimizacao} discutimos alguns conceitos e resultados da teoria de otimização, apresentando o desenvolvimento das condições de otimalidade para problemas de otimização irrestrita e com restrições lineares, dentre as quais as condições de Karush-Kuhn-Tucker por exemplo. Na \Cref{chap:otimizacao_convexa}, por sua vez, tratamos de alguns elementos da teoria de otimização convexa, a qual fornece resultados importantes aos problemas de otimização, como a garantia de que minimizadores locais são globais. 

Na \Cref{chap:experimentos_numericos} implementamos a técnica SVM para classificação binária do conjunto de dados Flor Íris e de um conjunto de dados de células de câncer de mama \cite{Dua:2019}. Através destes experimentos numéricos concluímos que a técnica SVM apresenta, de fato, bons resultados na determinação de um classificador, de modo que foi possível atingir uma acurácia de 100\% nos testes realizados com o conjunto Íris, e uma acurácia de 96,18\% na classificação dos dados de câncer em tumor maligno ou benigno. Além disso, na implementação da técnica SVM para classificar os dados de câncer, que não são linearmente separáveis, constatamos que a eficiência do classificador está relacionada com a escolha adequada do parâmetro de penalização.

É importante pontuar que neste trabalho nos limitamos a formular somente o problema primal da técnica SVM, tanto de margem rígida quanto flexível. Assim, algumas sugestões para serem desenvolvidas em trabalhos futuros são realizar um estudo da teoria de dualidade para obter o dual dos problemas \eqref{eq:problemageralSVM} e \eqref{eq:problemaCSVM}, assim como, desenvolver a formulação matemática da técnica SVM não linear, que utiliza \emph{Kernels} não-lineares para classificar conjuntos de dados não-linearmente separáveis nos casos em que a técnica com margem flexível não fornece um bom classificador.

Por fim, o desenvolvimento deste projeto de Iniciação Científica trouxe inúmeros benefícios para a minha formação acadêmica. Através dele tive a oportunidade de ampliar meus conhecimentos com o estudo de conteúdos que não são abordados nas disciplinas usuais do curso de Licenciatura em Matemática, além de possibilitar um maior contato com a pesquisa em matemática, despertando meu interesse em cursar uma pós-graduação na área de Matemática e fornecendo ideias de projetos de pesquisa que poderiam ser desenvolvidos futuramente. 
\newpage

\printbibliography
\end{document} 